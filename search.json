[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "CEVE 543: Data Science for Climate Risk Assessment",
    "section": "",
    "text": "This page contains a schedule of the topics, content, and assignments for the semester. This schedule will be updated as necessary over the course of the semester to accommodate logistics and to adapt to student needs.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nModule\n\n\nCategory\n\n\n\n\n\n\nAug 21, 2023\n\n\nWelcome to CEVE 543!!\n\n\nModule 01\n\n\nLecture\n\n\n\n\nAug 23, 2023\n\n\nWhat drives uncertain climate hazard?\n\n\nModule 01\n\n\nLecture\n\n\n\n\nAug 25, 2023\n\n\nSetting up Julia, GitHub, and Quarto\n\n\nModule 01\n\n\nLecture\n\n\n\n\nAug 25, 2023\n\n\nLab 01\n\n\nModule 01\n\n\nLabs\n\n\n\n\nAug 28, 2023\n\n\nFundamentals of probability distributions and statistics\n\n\nModule 01\n\n\nLecture\n\n\n\n\nAug 30, 2023\n\n\nMarginal, conditional, and joint distributions\n\n\nModule 01\n\n\nLecture\n\n\n\n\nSep 1, 2023\n\n\nLab 02\n\n\nModule 01\n\n\nLabs\n\n\n\n\nSep 6, 2023\n\n\nLikelihood and maximum likelihood estimation\n\n\nModule 01\n\n\nLecture\n\n\n\n\nSep 8, 2023\n\n\nLab 03\n\n\nModule 01\n\n\nLabs\n\n\n\n\nSep 11, 2023\n\n\nParametric uncertainty and Monte Carlo\n\n\nModule 01\n\n\nLecture\n\n\n\n\nSep 13, 2023\n\n\nBayesian statistics and computation\n\n\nModule 01\n\n\nLecture\n\n\n\n\nSep 15, 2023\n\n\nLab 04\n\n\nModule 01\n\n\nLabs\n\n\n\n\nSep 25, 2023\n\n\nModule 2 motivation: downscaling\n\n\nModule 02\n\n\nLecture\n\n\n\n\nSep 29, 2023\n\n\nModule 1 practice problems\n\n\nModule 01\n\n\nLecture\n\n\n\n\nOct 2, 2023\n\n\nLab 05\n\n\nModule 02\n\n\nLabs\n\n\n\n\nOct 4, 2023\n\n\nGeneralized Linear Models\n\n\nModule 02\n\n\nLecture\n\n\n\n\nOct 6, 2023\n\n\nLoss Functions (Planned)\n\n\nModule 02\n\n\nLecture\n\n\n\n\nOct 11, 2023\n\n\nPrincipal Components Analysis (Planned)\n\n\nModule 02\n\n\nLecture\n\n\n\n\nOct 16, 2023\n\n\nRandom Forests (Planned)\n\n\nModule 02\n\n\nLecture\n\n\n\n\nOct 18, 2023\n\n\nHyperparameter tuning (Planned)\n\n\nModule 02\n\n\nLecture\n\n\n\n\nOct 23, 2023\n\n\nFrequency Analysis (Planned)\n\n\nModule 03\n\n\nLecture\n\n\n\n\nOct 25, 2023\n\n\nExtreme Value Theory (Planned)\n\n\nModule 03\n\n\nLecture\n\n\n\n\nOct 30, 2023\n\n\nGEV Estimators (Planned)\n\n\nModule 03\n\n\nLecture\n\n\n\n\nNov 1, 2023\n\n\nNonstationary GEV (Planned)\n\n\nModule 03\n\n\nLecture\n\n\n\n\nNov 6, 2023\n\n\nQuantitative and Graphical Model Selection (Planned)\n\n\nModule 03\n\n\nLecture\n\n\n\n\nNov 8, 2023\n\n\nRegionalization and spatial pooling (Planned)\n\n\nModule 03\n\n\nLecture\n\n\n\n\nNov 13, 2023\n\n\nPlaceholder TBD (Planned)\n\n\nModule 03\n\n\nLecture\n\n\n\n\nNov 13, 2023\n\n\nModule 3 Review (Planned)\n\n\nModule 03\n\n\nLecture\n\n\n\n\nNov 15, 2023\n\n\nPlaceholder TBD (Planned)\n\n\nModule 03\n\n\nLecture\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "recommended_reading.html",
    "href": "recommended_reading.html",
    "title": "Recommended Reading",
    "section": "",
    "text": "There are lots of great resources beyond this website. Here are some especially good ones."
  },
  {
    "objectID": "recommended_reading.html#programming",
    "href": "recommended_reading.html#programming",
    "title": "Recommended Reading",
    "section": "Programming",
    "text": "Programming\n\nNazarathy & Klok (2021) offers tutorials on using Julia for statistics and machine learning; reading good code is a great way to improve your coding\nLauwens & Downey (2019) teaches key concepts in programming, using Julia for all examples. It‚Äôs a great resource for beginners trying to understand code and for more experienced programmers who want to write better code."
  },
  {
    "objectID": "recommended_reading.html#fundamentals",
    "href": "recommended_reading.html#fundamentals",
    "title": "Recommended Reading",
    "section": "Fundamentals",
    "text": "Fundamentals\n\nBlitzstein & Hwang (2019) provides a thorough introduction to key concepts and ideas in probability. The book accompanies a free online course, Stat 110, which is a great resource for learning probability and statistics. Practice problems and solutions, handouts, and lecture videos are all available online.\nDowney (2021) offers an introduction to Bayesian statistics using computational methods. It‚Äôs not environment focused but provides code and a clear explanation of core concepts."
  },
  {
    "objectID": "recommended_reading.html#digging-deeper",
    "href": "recommended_reading.html#digging-deeper",
    "title": "Recommended Reading",
    "section": "Digging deeper",
    "text": "Digging deeper\n\nGelman et al. (2014) is a classical and detailed textbook on Bayesian inference.\nGelman (2021) is a less technical textbook with clear and well-worked examples (mostly not environmental).\nJames et al. (2013) is a classic introduction to machine learning, which complements the Bayesian perspective nicely\nCressie & Wikle (2011) provides a detailed exploration of hierarchical space-time models. There have been some computational advances since then that are worth keeping in mind before you apply these models directly, but it‚Äôs a clearly written and overview.\nThe documentation for the Turing, PyMC, and (especially) stan probabilistic programming languages offer outstanding tutorials on statistical modeling"
  },
  {
    "objectID": "recommended_reading.html#environmental-applications",
    "href": "recommended_reading.html#environmental-applications",
    "title": "Recommended Reading",
    "section": "Environmental applications",
    "text": "Environmental applications\n\nHelsel et al. (2020) is a textbook also produced by the USGS covering some general statistical methods with a water resources focus. This book is very focused on hydrological statistics, hypothesis testing, and frequency estimates, but there are still some valuable insights."
  },
  {
    "objectID": "recommended_reading.html#references",
    "href": "recommended_reading.html#references",
    "title": "Recommended Reading",
    "section": "References",
    "text": "References\n\n\nBlitzstein, J. K., & Hwang, J. (2019). Introduction to Probability, Second Edition (2nd Edition). Boca Raton: Chapman and Hall/CRC. Retrieved from http://probabilitybook.net\n\n\nCressie, N. A. C., & Wikle, C. K. (2011). Statistics for spatio-temporal data. Hoboken, N.J.: Wiley.\n\n\nDowney, A. B. (2021). Think Bayes. \"O‚ÄôReilly Media, Inc.\". Retrieved from https://allendowney.github.io/ThinkBayes2/\n\n\nGelman, A. (2021). Regression and other stories. Cambridge, United Kingdom ; Cambridge University Press.\n\n\nGelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (2014). Bayesian Data Analysis (3rd ed.). Chapman & Hall/CRC Boca Raton, FL, USA.\n\n\nHelsel, D. R., Hirsch, R. M., Ryberg, K. R., Archfield, S. A., & Gilroy, E. J. (2020). Statistical methods in water resources. Techniques and Methods. U.S. Geological Survey. https://doi.org/10.3133/tm4A3\n\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning (Vol. 103). New York, NY: Springer New York.\n\n\nLauwens, B., & Downey, A. B. (2019). Think Julia: How to think like a computer scientist. O‚ÄôReilly Media.\n\n\nNazarathy, Y., & Klok, H. (2021). Statistics with Julia: Fundamentals for Data Science, Machine Learning and Artificial Intelligence. Springer International Publishing. https://doi.org/10.1007/978-3-030-70901-3"
  },
  {
    "objectID": "slides/lecture06-downscaling.html#we-would-like-to-accurately-model-precipitation-at-high-spatial-and-temporal-resolution",
    "href": "slides/lecture06-downscaling.html#we-would-like-to-accurately-model-precipitation-at-high-spatial-and-temporal-resolution",
    "title": "Module 2 motivation: downscaling",
    "section": "We would like to accurately model precipitation at high spatial and temporal resolution",
    "text": "We would like to accurately model precipitation at high spatial and temporal resolution\n\nStormwater management (long-term desigm)\nWater resources management (subseasonal to multi-year planning)\nFire propagation (hourly to weekly)"
  },
  {
    "objectID": "slides/lecture06-downscaling.html#objectives",
    "href": "slides/lecture06-downscaling.html#objectives",
    "title": "Module 2 motivation: downscaling",
    "section": "Objectives",
    "text": "Objectives\n\nenhanced spatial detail\nmitigation of systematic ESM1 biases\ngeneration of variables not explicitly rendered by GCMs\n\n(Lanzante et al., 2018)\nEarth System Model"
  },
  {
    "objectID": "slides/lecture06-downscaling.html#earth-system-models",
    "href": "slides/lecture06-downscaling.html#earth-system-models",
    "title": "Module 2 motivation: downscaling",
    "section": "Earth System Models",
    "text": "Earth System Models"
  },
  {
    "objectID": "slides/lecture06-downscaling.html#dreary",
    "href": "slides/lecture06-downscaling.html#dreary",
    "title": "Module 2 motivation: downscaling",
    "section": "Dreary",
    "text": "Dreary"
  },
  {
    "objectID": "slides/lecture06-downscaling.html#drizzling",
    "href": "slides/lecture06-downscaling.html#drizzling",
    "title": "Module 2 motivation: downscaling",
    "section": "Drizzling",
    "text": "Drizzling"
  },
  {
    "objectID": "slides/lecture06-downscaling.html#challenges-summarized",
    "href": "slides/lecture06-downscaling.html#challenges-summarized",
    "title": "Module 2 motivation: downscaling",
    "section": "Challenges, summarized",
    "text": "Challenges, summarized\n\n\nESMs are tuned to get energy balance and large-scale circulation right, not local extremes\nESMs average over space and time\nLocal-scale precipitation can be tricky to model well"
  },
  {
    "objectID": "slides/lecture06-downscaling.html#supervised-downscaling",
    "href": "slides/lecture06-downscaling.html#supervised-downscaling",
    "title": "Module 2 motivation: downscaling",
    "section": "Supervised downscaling",
    "text": "Supervised downscaling\n\n\nInput: pairs \\((X_i, y_i)\\)\n\n\\(X_i\\): predictors (e.g., gridded rainfall)\n\\(y_i\\): predictand (e.g., gauge rainfall)\n\nGoal: learn a function \\(f\\) such that \\(f(X_i) \\approx y_i\\)\n\nMeasure quality of approximation through a loss function (more later)\n\nKey point: the \\(X_i\\) and \\(y_i\\) are observed at the same time\n\nExample: map satellite to radar data"
  },
  {
    "objectID": "slides/lecture06-downscaling.html#distributional-downscaling",
    "href": "slides/lecture06-downscaling.html#distributional-downscaling",
    "title": "Module 2 motivation: downscaling",
    "section": "Distributional downscaling",
    "text": "Distributional downscaling\n\n\nESMs simulate from the distribution of weather, given climate boundary conditions. For example:\n\nRun 100 ESM ensemble members over 20th century conditions\nStudy December 1, 1980 in all draws\nSome will be rainy, some will be dry; some cool, some warm\nStatistically meaningful, but not a forecast!\n\nNo pairs \\((X_i, y_i)\\). Instead we have samples \\(\\left\\{X_1, \\ldots, X_N \\right\\}\\) and \\(\\left\\{y_1, \\ldots, y_K \\right\\}\\)"
  },
  {
    "objectID": "slides/lecture06-downscaling.html#common-datasets",
    "href": "slides/lecture06-downscaling.html#common-datasets",
    "title": "Module 2 motivation: downscaling",
    "section": "Common datasets",
    "text": "Common datasets\n\n\nGauge data\nGridded observational products\n\nFor example: radar measurments are processed to produce gridded rainfall estimates\n\nReanalysis products\n\nUse assimilation to ‚Äúdigest‚Äù observations using a model\nGridded reconstructions of past weather\nState of the art is ERA5\n\nESM outputs\n\nHistorical runs\nCMIP: compare multiple models on standardized scenarios (e.g., RCP 2.6, 4.5, 8.5)\nSimulate from weather, conditional on boundary conditions"
  },
  {
    "objectID": "slides/lecture06-downscaling.html#bias-correction",
    "href": "slides/lecture06-downscaling.html#bias-correction",
    "title": "Module 2 motivation: downscaling",
    "section": "Bias correction",
    "text": "Bias correction\nSimplest form of downscaling. Usually \\(X\\) are samples from a climate model and \\(y\\) are observations. \\[\n\\begin{aligned}\n\\text{bias} &= \\mathbb{E}[X] - \\mathbb{E}[y] \\\\\n\\hat{y} &= X - \\text{bias}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nNote\n\n\nIs this a distributional or supervised method?"
  },
  {
    "objectID": "slides/lecture06-downscaling.html#quantile-quantile-mapping",
    "href": "slides/lecture06-downscaling.html#quantile-quantile-mapping",
    "title": "Module 2 motivation: downscaling",
    "section": "Quantile-quantile mapping",
    "text": "Quantile-quantile mapping"
  },
  {
    "objectID": "slides/lecture06-downscaling.html#correctorgan",
    "href": "slides/lecture06-downscaling.html#correctorgan",
    "title": "Module 2 motivation: downscaling",
    "section": "CorrectorGAN",
    "text": "CorrectorGAN\n\n\n (Price & Rasp, 2022)"
  },
  {
    "objectID": "slides/lecture06-downscaling.html#stationarity",
    "href": "slides/lecture06-downscaling.html#stationarity",
    "title": "Module 2 motivation: downscaling",
    "section": "Stationarity",
    "text": "Stationarity\nStationarity means the relationship between \\(X\\) and \\(y\\) does not change over time\n\nSupervised: \\(p(y | X)\\) or \\(y = f(X)\\) does not change over time\nDistributional: Corrections to the distribution do not change over time\n\nThis is never a perfect assumption"
  },
  {
    "objectID": "slides/lecture06-downscaling.html#your-task",
    "href": "slides/lecture06-downscaling.html#your-task",
    "title": "Module 2 motivation: downscaling",
    "section": "Your task",
    "text": "Your task\n\n\nGiven:\n\nHourly gridded rainfall data (small area)\nHourly large-scale pressure and temperature fields\nHourly gauge rainfall data at a single station\n\nDevelop a model to predict hourly gauge rainfall from the available datasets"
  },
  {
    "objectID": "slides/lecture06-downscaling.html#expectations",
    "href": "slides/lecture06-downscaling.html#expectations",
    "title": "Module 2 motivation: downscaling",
    "section": "Expectations",
    "text": "Expectations\n\nTry and compare at least two different approaaches\nQuantitative and qualitative evaluation of the models\nYou can use any methods you like:\n\nThose we cover in class\nThose you already know## References\n\n\n\n\n\n\n\n\nLanzante, J. R., Dixon, K. W., Nath, M. J., Whitlock, C. E., & Adams-Smith, D. (2018). Some Pitfalls in Statistical Downscaling of Future Climate. Bulletin of the American Meteorological Society, 99(4), 791‚Äì803. https://doi.org/10.1175/bams-d-17-0046.1\n\n\nPrice, I., & Rasp, S. (2022, March 23). Increasing the accuracy and resolution of precipitation forecasts using deep generative models. https://doi.org/10.48550/arXiv.2203.12297"
  },
  {
    "objectID": "slides/lecture07-loss-functions.html#tbd",
    "href": "slides/lecture07-loss-functions.html#tbd",
    "title": "Loss Functions (Planned)",
    "section": "TBD",
    "text": "TBD\nThis is a placeholder for a planned lecture. The schedule will be modified as needed."
  },
  {
    "objectID": "slides/lecture04-bayes2.html#last-time",
    "href": "slides/lecture04-bayes2.html#last-time",
    "title": "Bayesian statistics and computation",
    "section": "Last time",
    "text": "Last time\n\nParametric uncertainty matters for decision-making\nAs we collect more data, fewer combinations of parameters are consistent with observations"
  },
  {
    "objectID": "slides/lecture04-bayes2.html#logistics-preview",
    "href": "slides/lecture04-bayes2.html#logistics-preview",
    "title": "Bayesian statistics and computation",
    "section": "Logistics preview",
    "text": "Logistics preview\n\nNo class Monday\nToday: focus on the methods and ideas\nMonday, asynchronously, work through the code"
  },
  {
    "objectID": "slides/lecture04-bayes2.html#rare-disease",
    "href": "slides/lecture04-bayes2.html#rare-disease",
    "title": "Bayesian statistics and computation",
    "section": "Rare disease",
    "text": "Rare disease\n\n\nEveryone is tested for CEVE543acitis, a rare and deadly disease\nIt is known that 1 in 1,000 people have CEVE543acitis\nThe test is 99% accurate\nYour test comes back positive. What is the probability that you have CEVE543acitis?"
  },
  {
    "objectID": "slides/lecture04-bayes2.html#bayes-rule-discrete-event-version",
    "href": "slides/lecture04-bayes2.html#bayes-rule-discrete-event-version",
    "title": "Bayesian statistics and computation",
    "section": "Bayes‚Äô rule: discrete event version",
    "text": "Bayes‚Äô rule: discrete event version\n\\[\n\\Pr \\left\\{ \\theta | y \\right\\} = \\frac{\\Pr \\left\\{ y | \\theta \\right\\} \\Pr \\left\\{ \\theta \\right\\}}{\\Pr \\left\\{ y \\right\\}}\n\\]## Application: rare disease {.smaller .scrollable}\nDefine \\(y\\) is getting a positive test result and \\(\\theta\\) is having the underlying condition. Not that we do not observe \\(\\theta\\) directly! Here \\(y=1\\) and we want to know \\(\\Pr\\left\\{\\theta = 1 \\mid y=1 \\right\\}\\).\nLikelihood:\n\n\n\n\n\n\n\n\n\n\n\\(\\Pr\\left\\{y = 1 \\ldots\\right.\\)\n\\(\\Pr\\left\\{y = 0 |\\ldots \\right.\\)\n\n\n\n\n\\(\\left. ...\\theta=1 \\right\\}\\)\n0.99\n0.01\n\n\n\\(\\left. ...\\theta=0\\right\\}\\)\n0.01\n0.99\n\n\n\n\n. . . A naive application of maximum likelihood: \\(\\Pr\\left\\{y=1 \\mid \\theta=1 \\right\\} &gt; \\Pr\\left\\{y=1 \\mid \\theta=0 \\right\\}\\) so best estimate is \\(\\theta=1\\)"
  },
  {
    "objectID": "slides/lecture04-bayes2.html#solving",
    "href": "slides/lecture04-bayes2.html#solving",
    "title": "Bayesian statistics and computation",
    "section": "Solving",
    "text": "Solving\nWe are studying \\(\\Pr\\left\\{\\theta = 1 | y = 1 \\right\\}\\).\n\n\nStep 1: \\(\\Pr\\left\\{ y = 1 \\right\\}\\)\n\n\\(\\Pr\\left\\{y=1\\right\\} = \\Pr \\left\\{ y=1, \\theta=0 \\right\\} + \\Pr \\left\\{ y=1, \\theta=1 \\right\\}\\)\n\\(\\Pr\\left\\{y=1\\right\\} = \\Pr \\left\\{ \\theta=0 \\right\\} \\Pr \\left\\{ y = 1 | \\theta=0 \\right\\} + \\Pr \\left\\{ \\theta=1 \\right\\} \\Pr \\left\\{ y=1 | \\theta=0 \\right\\}\\)\n\\(\\underbrace{\\Pr\\left\\{y=1\\right\\}}_\\text{test +} = \\underbrace{0.999}_\\text{don't have it} \\times \\overbrace{0.01}^\\text{false +} + \\underbrace{0.001}_\\text{have it} \\times \\overbrace{0.99}^\\text{true +}\\)\n\nNow plug in to Bayes‚Äô rule\n\n\\(\\Pr\\left\\{ \\theta=1 | y=1 \\right\\} = \\frac{\\Pr\\left\\{y=1 | \\theta=1 \\right\\} \\Pr\\left\\{ \\theta=1 \\right\\}}{\\Pr\\left\\{y=1\\right\\}}\\)\n\\(\\Pr\\left\\{\\theta=1 | y=1 \\right\\} = \\frac{0.99 \\times 0.001}{\\Pr\\left\\{y=1\\right\\}}\\)"
  },
  {
    "objectID": "slides/lecture04-bayes2.html#implementation",
    "href": "slides/lecture04-bayes2.html#implementation",
    "title": "Bayesian statistics and computation",
    "section": "Implementation",
    "text": "Implementation\n\n\n0.01098\n\n\n0.09016"
  },
  {
    "objectID": "slides/lecture04-bayes2.html#key-idea",
    "href": "slides/lecture04-bayes2.html#key-idea",
    "title": "Bayesian statistics and computation",
    "section": "Key idea",
    "text": "Key idea\n\n\nParameters have probability distributions, not single point values\nStart with some prior distribution for parameters\nGoal: what is the distribution of the parameters given the data?"
  },
  {
    "objectID": "slides/lecture04-bayes2.html#bayes-rule-for-distributions",
    "href": "slides/lecture04-bayes2.html#bayes-rule-for-distributions",
    "title": "Bayesian statistics and computation",
    "section": "Bayes‚Äô rule for distributions",
    "text": "Bayes‚Äô rule for distributions\n\\[\np(\\theta \\mid y) = \\frac{p(y \\mid \\theta) p(\\theta)}{p(y)}\n\\]\n\nIf we are drawing samples from a distribution, we can calculate up to a constant of proportionality and ‚Äì since \\(p(y)\\) doesn‚Äôt depend on \\(\\theta\\) ‚Äì we can usually ignore it.\n\\[\n\\overbrace{p(\\theta \\mid y)}^\\rm{posterior} \\propto \\underbrace{p(y \\mid \\theta)}_\\rm{likelihood} \\overbrace{p(\\theta)}^\\rm{prior}\n\\]## Coin flipping\nWe flip a coin a few times. We want to estimate the probability of heads so that we can make well-calibrated bets on future coin tosses.\n\n\n\n\n8"
  },
  {
    "objectID": "slides/lecture04-bayes2.html#maximum-likelihood",
    "href": "slides/lecture04-bayes2.html#maximum-likelihood",
    "title": "Bayesian statistics and computation",
    "section": "Maximum likelihood",
    "text": "Maximum likelihood\nMaximum likelihood estimate (MLE) is the most likely value of \\(\\theta\\) given the data. As before, we can use our log-likelihood.\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis builds on what we did last time. A coin flip is represented by a Bernoulli process. In fact, we could use a Binomial distribution to model the number of heads in \\(N\\) flips.\nThe maximum likelihood estimate can in fact be shown to be exactly n_heads / length(coin_flips)"
  },
  {
    "objectID": "slides/lecture04-bayes2.html#prior",
    "href": "slides/lecture04-bayes2.html#prior",
    "title": "Bayesian statistics and computation",
    "section": "Prior",
    "text": "Prior\nWe should be suspicious of our analysis when it concludes that we will continue to see 8 out of 9 flips coming up heads forever.\n\nTo perform a Bayesian analysis, we‚Äôll need a prior. A Beta distribution is a natural choice for a prior on a probability, although we could use a Uniform distribution or even something silly like a truncated Gamma (don‚Äôt!)"
  },
  {
    "objectID": "slides/lecture04-bayes2.html#closed-form-solution",
    "href": "slides/lecture04-bayes2.html#closed-form-solution",
    "title": "Bayesian statistics and computation",
    "section": "Closed-form solution",
    "text": "Closed-form solution\nCool property: if you have a Beta prior and a Binomial likelihood, the posterior is also Beta distributed. Look up Beta-Binomial conjugacy for more! We will leverage this property to check our answers."
  },
  {
    "objectID": "slides/lecture04-bayes2.html#markov-chain-monte-carlo",
    "href": "slides/lecture04-bayes2.html#markov-chain-monte-carlo",
    "title": "Bayesian statistics and computation",
    "section": "Markov Chain Monte Carlo",
    "text": "Markov Chain Monte Carlo\n\nA class of methods for sampling from a probability distribution\nRandom walkers:\n\nStart at some value\nPropose a new value\nAccept or reject the new value based on some criteria\n\nRepeat to get a ‚Äúchain‚Äù of samples"
  },
  {
    "objectID": "slides/lecture04-bayes2.html#metropolis-hastings",
    "href": "slides/lecture04-bayes2.html#metropolis-hastings",
    "title": "Bayesian statistics and computation",
    "section": "Metropolis-Hastings",
    "text": "Metropolis-Hastings\nSee the very good Wikipedia article"
  },
  {
    "objectID": "slides/lecture04-bayes2.html#limitations",
    "href": "slides/lecture04-bayes2.html#limitations",
    "title": "Bayesian statistics and computation",
    "section": "Limitations",
    "text": "Limitations\n\nWorks great for a very simple problem\nComputation blows up in higher dimensions (p_accept gets very small)\nHave to code a new sampler for each problem\n\n\nModern samplers leverage gradients and clever tricks to draw better samples for harder problems. Let‚Äôs use them!"
  },
  {
    "objectID": "slides/lecture04-bayes2.html#turing-model-specification",
    "href": "slides/lecture04-bayes2.html#turing-model-specification",
    "title": "Bayesian statistics and computation",
    "section": "Turing model specification",
    "text": "Turing model specification\nWe can write down the full Bayesian model in Turing, which uses a syntax very close to our notation!"
  },
  {
    "objectID": "slides/lecture04-bayes2.html#turing-sampling",
    "href": "slides/lecture04-bayes2.html#turing-sampling",
    "title": "Bayesian statistics and computation",
    "section": "Turing sampling",
    "text": "Turing sampling\nWe can leverage sophisticated machinery for drawing samples from arbitrary posterior distributions. For now, we will trust that it is drawing samples from \\(p(y | \\theta)\\) and not worry about the details.\n\n\n\nSummary Statistics\n  parameters      mean       std      mcse    ess_bulk    ess_tail      rhat   ‚ãØ\n      Symbol   Float64   Float64   Float64     Float64     Float64   Float64   ‚ãØ\n           Œ∏    0.6832    0.1021    0.0016   4234.2533   5796.1544    1.0003   ‚ãØ\n                                                                1 column omitted"
  },
  {
    "objectID": "slides/lecture04-bayes2.html#visualize",
    "href": "slides/lecture04-bayes2.html#visualize",
    "title": "Bayesian statistics and computation",
    "section": "Visualize",
    "text": "Visualize\nWe can visualize our posterior\nhistogram( coin_chain[:Œ∏]; label=‚ÄúSamples‚Äù, normalize=:pdf, legend=:topleft, xlabel=L‚Äù\\(Œ∏\\)‚Äú, ylabel=L‚Äù\\(p(Œ∏ | y)\\)‚Äù ) plot!(closed_form; label=‚ÄúExact Posterior‚Äù, linewidth=3) plot!(prior_dist; label=‚ÄúPrior‚Äù, linewidth=3) vline!([Œ∏_mle]; label=‚ÄúMLE‚Äù, linewidth=3) ```## Compromise\nThe posterior is a compromise between the prior and the likelihood.\n\n\nBad priors lead to bad inferences\nThe choice of prior is subjective, which some people hate,\n\nWe will approach this in a principled manner (Gelman et al., 2020; Gelman & Shalizi, 2013)\nLots of other steps are also subjective (choice of likelihood model, which data to use, problem framing, etc)\nFalse sense of objectivity is dangerous anyways!"
  },
  {
    "objectID": "slides/lecture04-bayes2.html#read-data",
    "href": "slides/lecture04-bayes2.html#read-data",
    "title": "Bayesian statistics and computation",
    "section": "Read data",
    "text": "Read data"
  },
  {
    "objectID": "slides/lecture04-bayes2.html#model",
    "href": "slides/lecture04-bayes2.html#model",
    "title": "Bayesian statistics and computation",
    "section": "Model",
    "text": "Model\nDefine a LogNormal distribution with very diffuse (flat) priors"
  },
  {
    "objectID": "slides/lecture04-bayes2.html#sample",
    "href": "slides/lecture04-bayes2.html#sample",
    "title": "Bayesian statistics and computation",
    "section": "Sample",
    "text": "Sample"
  },
  {
    "objectID": "slides/lecture04-bayes2.html#posterior",
    "href": "slides/lecture04-bayes2.html#posterior",
    "title": "Bayesian statistics and computation",
    "section": "Posterior",
    "text": "Posterior\nWe leverage the histogram2d function to visualize the 2D posterior distribution."
  },
  {
    "objectID": "slides/lecture04-bayes2.html#return-period-with-uncertainty",
    "href": "slides/lecture04-bayes2.html#return-period-with-uncertainty",
    "title": "Bayesian statistics and computation",
    "section": "Return period with uncertainty",
    "text": "Return period with uncertainty\nEach draw from the posterior represents a plausible value of \\(\\mu\\) and \\(\\sigma\\). We can use these to explore the distribution of return periods."
  },
  {
    "objectID": "slides/lecture04-bayes2.html#trace-plot",
    "href": "slides/lecture04-bayes2.html#trace-plot",
    "title": "Bayesian statistics and computation",
    "section": "Trace plot",
    "text": "Trace plot\nVisualize the samples as a chain"
  },
  {
    "objectID": "slides/lecture04-bayes2.html#model-1",
    "href": "slides/lecture04-bayes2.html#model-1",
    "title": "Bayesian statistics and computation",
    "section": "Model",
    "text": "Model\nWe can treat the priors as parameters so that we don‚Äôt have to define a new @model each time we want to update our priors\n\nNo reason why we can‚Äôt pass distributions as functional arguments"
  },
  {
    "objectID": "slides/lecture04-bayes2.html#guess-and-prior-predictive-check",
    "href": "slides/lecture04-bayes2.html#guess-and-prior-predictive-check",
    "title": "Bayesian statistics and computation",
    "section": "Guess and prior predictive check",
    "text": "Guess and prior predictive check\nDefine priors\nDraw samples from the prior\nPlot the consequences of these samples"
  },
  {
    "objectID": "slides/lecture04-bayes2.html#revise",
    "href": "slides/lecture04-bayes2.html#revise",
    "title": "Bayesian statistics and computation",
    "section": "Revise",
    "text": "Revise\nIf we are getting return levels of \\(10^{12}\\) ft, we should probably revise our priors\n\nYes, I‚Äôm overwriting the old value\n\nWe can sample"
  },
  {
    "objectID": "slides/lecture04-bayes2.html#getting-closer",
    "href": "slides/lecture04-bayes2.html#getting-closer",
    "title": "Bayesian statistics and computation",
    "section": "Getting closer",
    "text": "Getting closer"
  },
  {
    "objectID": "slides/lecture04-bayes2.html#now-get-posterior",
    "href": "slides/lecture04-bayes2.html#now-get-posterior",
    "title": "Bayesian statistics and computation",
    "section": "Now get posterior",
    "text": "Now get posterior\nWe use the same model to get the posterior. Often we want to run multiple chains with different initial values to make sure we are getting good samples.\n\n\n\nSummary Statistics\n  parameters      mean       std      mcse     ess_bulk     ess_tail      rhat ‚ãØ\n      Symbol   Float64   Float64   Float64      Float64      Float64   Float64 ‚ãØ\n           Œº    1.3692    0.0194    0.0001   17256.1692   13201.9364    1.0003 ‚ãØ\n           œÉ    0.1861    0.0138    0.0001   17431.4216   13529.7674    1.0001 ‚ãØ\n                                                                1 column omitted"
  },
  {
    "objectID": "slides/lecture04-bayes2.html#traceplot-for-multiple-chains",
    "href": "slides/lecture04-bayes2.html#traceplot-for-multiple-chains",
    "title": "Bayesian statistics and computation",
    "section": "Traceplot for multiple chains",
    "text": "Traceplot for multiple chains"
  },
  {
    "objectID": "slides/lecture04-bayes2.html#visualize-1",
    "href": "slides/lecture04-bayes2.html#visualize-1",
    "title": "Bayesian statistics and computation",
    "section": "Visualize",
    "text": "Visualize\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nHere our likelihood is very informative, so it doesn‚Äôt much matter if we use excessively diffuse priors. This is nice, though not something we can count on in general."
  },
  {
    "objectID": "slides/lecture04-bayes2.html#return-period-with-uncertainty-1",
    "href": "slides/lecture04-bayes2.html#return-period-with-uncertainty-1",
    "title": "Bayesian statistics and computation",
    "section": "Return period with uncertainty",
    "text": "Return period with uncertainty\nAs before, we can visualize our posterior distribution in terms of return periods"
  },
  {
    "objectID": "slides/lecture04-bayes2.html#key-value-add-of-bayesian-inference",
    "href": "slides/lecture04-bayes2.html#key-value-add-of-bayesian-inference",
    "title": "Bayesian statistics and computation",
    "section": "Key value add of Bayesian inference",
    "text": "Key value add of Bayesian inference\n\n\nDraw samples from tricky posteriors to compute expectations \\(\\mathbb{E}[f(\\theta)]\\)\nQuantify parametric uncertainty\n\nIn practice, sometimes this is a big deal and sometimes model structure uncertainties matter more\n\nForce us to specify a data generating process\nComputational methods fail loudly"
  },
  {
    "objectID": "slides/lecture04-bayes2.html#learning-turing",
    "href": "slides/lecture04-bayes2.html#learning-turing",
    "title": "Bayesian statistics and computation",
    "section": "Learning Turing",
    "text": "Learning Turing\nThe official docs are great.\n\n\n\n\n\n\nWarning\n\n\nGoogle will often try to link you to the old site, https://turing.ml/. This is out of date! Use https://turinglang.org/stable/ instead."
  },
  {
    "objectID": "slides/lecture04-bayes2.html#another-word-on-generative-ai",
    "href": "slides/lecture04-bayes2.html#another-word-on-generative-ai",
    "title": "Bayesian statistics and computation",
    "section": "Another word on generative AI",
    "text": "Another word on generative AI\n\n\n\nGitHub Copilot\n\nFree for students: instructions\n\nGPT and related language models\n\n\nDo not just plug in the problem and paste the solution!\n\nLabs are just 10% of your grade\nYou will be resonsible for material on projects / tests\nYou won‚Äôt learn\n\nDo use it to for syntax help and code explanations"
  },
  {
    "objectID": "slides/lecture04-bayes2.html#logistics",
    "href": "slides/lecture04-bayes2.html#logistics",
    "title": "Bayesian statistics and computation",
    "section": "Logistics",
    "text": "Logistics\n\nFriday:\n\nLab 04\nLab 03 due\n\nMonday 9/18: no class. Work on lab 04 and work through these slides.\nTuesday 9/19: no office hours.\nWednesday 9/20: test I review\nFriday 9/22: test I\nYou may turn in lab 04 by 9/29 (two weeks)"
  },
  {
    "objectID": "slides/lecture04-bayes2.html#references",
    "href": "slides/lecture04-bayes2.html#references",
    "title": "Bayesian statistics and computation",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nGelman, A., & Shalizi, C. R. (2013). Philosophy and the practice of Bayesian statistics. British Journal of Mathematical and Statistical Psychology, 66(1), 8‚Äì38. https://doi.org/f4k2h4\n\n\nGelman, A., Vehtari, A., Simpson, D., Margossian, C. C., Carpenter, B., Yao, Y., et al. (2020, November 3). Bayesian workflow. https://doi.org/10.48550/arXiv.2011.01808"
  },
  {
    "objectID": "slides/lecture04-bayes1.html#recall-bayesian-decision-theory",
    "href": "slides/lecture04-bayes1.html#recall-bayesian-decision-theory",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Recall: Bayesian Decision Theory",
    "text": "Recall: Bayesian Decision Theory\nRecall: \\[\n\\mathbb{E}\\left[L(a, \\theta) \\right] = \\int_\\theta L(a, \\theta) p(\\theta) d\\theta\n\\] Where \\(\\theta\\) is a vector of parameters, \\(a\\) is some action or decision, and \\(L\\) is the loss function.\n\n\n\n\n\n\nNote\n\n\nWe previously called \\(\\theta\\) a ‚Äústate of the world‚Äù and \\(L\\) a ‚Äúreward function‚Äù."
  },
  {
    "objectID": "slides/lecture04-bayes1.html#problem-statement",
    "href": "slides/lecture04-bayes1.html#problem-statement",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Problem statement",
    "text": "Problem statement\nYou have been comissioned by a client to assess their exposure to flooding. Specifically, they want to know the probability distribution of annual flood damages at their property if they do not elevate or floodproof their building.\n\n\n\\(p(h)\\): probability distribution of annual maximum flood heights at their property\n\\(d(h)\\): flood damages as a deterministic function of flood height\n\\(p(d) = \\int_h d(h) p(h) \\, dh\\)\n\n\n\nWith this information, they can compute metrics like the expected annual damage, the 99th percentile annual damage, and the probability of any flood occurring that will help them make a decision."
  },
  {
    "objectID": "slides/lecture04-bayes1.html#data",
    "href": "slides/lecture04-bayes1.html#data",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Data",
    "text": "Data\nWe fold this long code block to save space.\n\n\n3√ó2 DataFrame\n\n\n\nRow\nyear\nlsl\n\n\n\nInt64\nQuantity‚Ä¶\n\n\n\n\n1\n1928\n5.05343 ft\n\n\n2\n1929\n3.90728 ft\n\n\n3\n1930\n4.60245 ft\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecall the *= syntax: x *= 2 is equivalent ot x = x * 2. We use .*= to work element-wise on the vector.\nThis adds the points to the plot\nnormalize=:pdf normalizes the histogram so that the area under the curve is 1.\nThe \\(y\\) axis of the histogram matches that of the line plot, so we remove any y ticks from the histogram.\nWe can define very flexible layouts for combining multiple plots. See the docs.\nsuptitle adds a title to the entire figure.\n\nThis data comes from the NOAA Tides and Currents database, specifically a gauge at Sewell‚Äôs Point, VA, with sea level rise removed."
  },
  {
    "objectID": "slides/lecture04-bayes1.html#flood-depths-model",
    "href": "slides/lecture04-bayes1.html#flood-depths-model",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Flood depths model",
    "text": "Flood depths model\nWe want a probability distribution for flood depths \\(p(h)\\). We can work with the log of the flood depths and treat them as normally distributed: \\[\n\\log h_i \\sim \\mathcal{N}(\\mu, \\sigma^2)\n\\]\n\nWe call this a lognormal distribution: \\[\nh_i \\sim \\text{LN}(\\mu, \\sigma)\n\\]"
  },
  {
    "objectID": "slides/lecture04-bayes1.html#distribution-and-histogram",
    "href": "slides/lecture04-bayes1.html#distribution-and-histogram",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Distribution and histogram",
    "text": "Distribution and histogram\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnormalizes the histogram so that the area under the curve is 1.\nThe fit_mle function requires a vector of numbers, without units. We use ustrip to convert the units to a number, using feet (u\"ft\") as the reference unit.\nThis accesses the :lsl_ft column we created.\n\n\n\n\n\n\n\n\nNote\n\n\nWe‚Äôll spend a whole module on extreme value distributions. They should do a better job of modeling annual maxima but require some subtelty."
  },
  {
    "objectID": "slides/lecture04-bayes1.html#return-periods-and-levels",
    "href": "slides/lecture04-bayes1.html#return-periods-and-levels",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Return periods and levels",
    "text": "Return periods and levels\nFlood probabilities are often plotted as return periods. This is just a visualization the CDF on a log (or log-log) scale.\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a trick to get the ticks on the axes right when we plot on a log scale.\n\n\n\n\n\n\n\nNote\n\n\nWork through this code and make sure you understand it"
  },
  {
    "objectID": "slides/lecture04-bayes1.html#plot-position",
    "href": "slides/lecture04-bayes1.html#plot-position",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Plot Position",
    "text": "Plot Position\nIt is common to add the data points as dots on the return period curve. This begs the question: what return period is assigned to each point? This is a subjective choice, but a common one is the Weibull plotting position:\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe are again using the vector that we converted to a scalar, in feet.\n\n\n\n\n\n\n\nü§î\n\n\nHow well does this fit the data?"
  },
  {
    "objectID": "slides/lecture04-bayes1.html#depth-damage-model",
    "href": "slides/lecture04-bayes1.html#depth-damage-model",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Depth-damage model",
    "text": "Depth-damage model\nA bounded logistic function provides a plausible depth-damage model for now: \\[\nd(h) = \\mathbb{I}\\left[x &gt; 0 \\right] \\frac{L}{1 + \\exp(-k(x - x_0))}\n\\] where \\(d\\) is damage as a percent of total value, \\(h\\) is water depth, \\(\\mathbb{I}\\) is the indicator function, \\(L\\) is the maximum loss, \\(k\\) is the slope, and \\(x_0\\) is the inflection point. We fix \\(L=1\\) (known upper bound: 100% damage) so"
  },
  {
    "objectID": "slides/lecture04-bayes1.html#plotting-fit",
    "href": "slides/lecture04-bayes1.html#plotting-fit",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Plotting fit",
    "text": "Plotting fit\nWe will use \\(x_0 = 4\\) and \\(k = 0.75\\).\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is another syntax for defining a function. It is equivalent to f(x) = blogistic(x, x0, k). It‚Äôs similar to lambda functions in python"
  },
  {
    "objectID": "slides/lecture04-bayes1.html#analytic-approach",
    "href": "slides/lecture04-bayes1.html#analytic-approach",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Analytic approach",
    "text": "Analytic approach\nPlugging in our bounded logistic model for \\(d(h)\\) and our lognormal model for \\(h\\): \\[\n\\begin{align}\np(d) &= \\int_h d(h) p(h) \\, dh \\\\\n&= \\int_{-\\infty}^\\infty \\mathbb{I}[h &gt; 0] \\text{logistic}(h) \\mathcal{N}(h | \\mu, \\sigma^2) \\, dh\\\\\n&= \\int_0^\\infty \\text{logistic}(h) \\mathcal{N}(\\mu, \\sigma^2) \\, dh\\\\\n&= \\int_0^\\infty \\frac{1}{1 + \\exp(-k * (x - x0))} \\frac {1}{\\sigma {\\sqrt {2\\pi }}} \\exp \\left\\{-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2} \\right\\} \\, dh\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/lecture04-bayes1.html#limitations-analytic-appraoch",
    "href": "slides/lecture04-bayes1.html#limitations-analytic-appraoch",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Limitations: analytic appraoch",
    "text": "Limitations: analytic appraoch\nWe might be able to solve this analytically (Wolfram Alpha can‚Äôt‚Ä¶). But‚Ä¶\n\n\nNumerous simplifying assumptions and approximations.\nWhat if we want to use a different distribution?\nA different damage model?"
  },
  {
    "objectID": "slides/lecture04-bayes1.html#what-is-monte-carlo",
    "href": "slides/lecture04-bayes1.html#what-is-monte-carlo",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "What is Monte Carlo?",
    "text": "What is Monte Carlo?\nMonte Carlo methods are a set of computational techniques for:\n\nGenerating samples from a target distribution\napproximating the expectations of some random quantities under this distribution."
  },
  {
    "objectID": "slides/lecture04-bayes1.html#monte-carlo-theory",
    "href": "slides/lecture04-bayes1.html#monte-carlo-theory",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Monte Carlo: Theory",
    "text": "Monte Carlo: Theory\nWe want to approximate the quantity \\[\n\\int_0^\\infty \\text{logistic}(h) \\mathcal{N}(\\mu, \\sigma^2) \\, dh\n\\]::::: {.incremental} :::: {.columns} ::: {.column width=‚Äú50%‚Äù} A deterministic strategy:\n\nsample \\(h^s = 0, \\Delta h, 2\\Delta h, \\ldots, (S-1)\\Delta h\\)\ncompute \\(\\text{logistic}(h^s) \\mathcal{N}(h^s | \\mu, \\sigma^2)\\) at each point and sum\ndrawbacks: we have to go to \\(\\infty\\) and select \\(\\Delta h\\).\n\n::: ::: {.column width=‚Äú50%‚Äù} A sampling strategy\n\nsample \\(h^1, h^2, \\ldots, h^S \\sim p(h)\\) ‚Äì which we can do because we have a model for \\(p(h)\\)\nfor each value: compute \\(\\mathbb{I}(h^s &gt; 0) \\text{logistic}(h^s)\\) and take the average\nthis converges to the correct expectation!\n\n::: :::: :::::"
  },
  {
    "objectID": "slides/lecture04-bayes1.html#more-formally",
    "href": "slides/lecture04-bayes1.html#more-formally",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "More formally",
    "text": "More formally\nIf \\(\\theta^s \\sim p(\\theta)\\), then \\[\n\\mathbb{E}\\left[ f(\\theta) \\right] = \\int_\\theta f(\\theta) p(\\theta) d\\theta \\approx \\frac{1}{S} \\sum_{s=1}^S f(\\theta^s)\n\\]\n\n\n\n\n\n\n\nReferences\n\n\nSee chapter 10 of Gelman et al. (2014) for more details or section 5 of Betancourt (2018) for a more precise mathematical treatment."
  },
  {
    "objectID": "slides/lecture04-bayes1.html#monte-carlo-implementation",
    "href": "slides/lecture04-bayes1.html#monte-carlo-implementation",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Monte Carlo: Implementation",
    "text": "Monte Carlo: Implementation\nA deceptively simple idea:"
  },
  {
    "objectID": "slides/lecture04-bayes1.html#monte-carlo-expectations",
    "href": "slides/lecture04-bayes1.html#monte-carlo-expectations",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Monte Carlo Expectations",
    "text": "Monte Carlo Expectations\nGiven these samples, we can compute expectations of any function of the samples. For example, we can compute the mean damage, the 99th percentile of damage, and the probability of any damage occurring\n\n\n3-element Vector{Float64}:\n 0.00235\n 0.07008\n 0.03486"
  },
  {
    "objectID": "slides/lecture04-bayes1.html#overview",
    "href": "slides/lecture04-bayes1.html#overview",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Overview",
    "text": "Overview\nWe have been working with a single probability distribution for flood depths, which we computed by maximum likelihood.\n\nThese values are not precise. What happens if we consider the lognormal distribution with slightly different, but still plausible, parameters?\n\n\nWhat about the depth-damage parameters \\(x_0\\) and \\(k\\)?"
  },
  {
    "objectID": "slides/lecture04-bayes1.html#parameter-uncertainty-flood-distribution",
    "href": "slides/lecture04-bayes1.html#parameter-uncertainty-flood-distribution",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Parameter uncertainty: flood distribution",
    "text": "Parameter uncertainty: flood distribution"
  },
  {
    "objectID": "slides/lecture04-bayes1.html#flood-distribution-damages",
    "href": "slides/lecture04-bayes1.html#flood-distribution-damages",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Flood distribution ‚û°Ô∏è damages",
    "text": "Flood distribution ‚û°Ô∏è damages\n\n\n3√ó3 DataFrame\n\n\n\nRow\nparams\nMLE\nalt_dist\n\n\n\nString\nFloat64\nFloat64\n\n\n\n\n1\nAverage Annual Loss\n0.00235\n0.00975\n\n\n2\nQ99 Annual Loss\n0.07008\n0.16531\n\n\n3\nProbability of Loss\n0.03486\n0.10076"
  },
  {
    "objectID": "slides/lecture04-bayes1.html#parameter-uncertainty-depth-damage-curve",
    "href": "slides/lecture04-bayes1.html#parameter-uncertainty-depth-damage-curve",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Parameter uncertainty: depth-damage curve",
    "text": "Parameter uncertainty: depth-damage curve"
  },
  {
    "objectID": "slides/lecture04-bayes1.html#depth-damage-curve-damages",
    "href": "slides/lecture04-bayes1.html#depth-damage-curve-damages",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Depth-damage curve ‚û°Ô∏è damages",
    "text": "Depth-damage curve ‚û°Ô∏è damages\n\n\n3√ó5 DataFrame\n\n\n\nRow\nparams\nMLE\nalt_dist\nalt_curve\nalt_both\n\n\n\nString\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nAverage Annual Loss\n0.00235\n0.00975\n0.00407\n0.01529\n\n\n2\nQ99 Annual Loss\n0.07008\n0.16531\n0.12715\n0.24093\n\n\n3\nProbability of Loss\n0.03486\n0.10076\n0.03308\n0.09746"
  },
  {
    "objectID": "slides/lecture04-bayes1.html#reflection",
    "href": "slides/lecture04-bayes1.html#reflection",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Reflection",
    "text": "Reflection\n\n\nCommon problem: \\(\\mathbb{E} \\left[ f(x) \\right]\\) where \\(x \\sim p(x)\\)\nSolution: \\(\\mathbb{E} \\left[ f(x) \\right] = \\int_x p(x) f(x) \\, dx\\)\nMonte Carlo approach:\n\nSample \\(x^1, x^2, \\ldots, x^S \\sim p(x)\\)\nCompute \\(\\frac{1}{S} \\sum_{s=1}^S f(x^s)\\)\n\nUncertainties in our model parameters propagate to uncertainties in the things we care about."
  },
  {
    "objectID": "slides/lecture04-bayes1.html#up-next",
    "href": "slides/lecture04-bayes1.html#up-next",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Up next",
    "text": "Up next\n\nTomrorow 9/12 at 10AM: Office hours (Ryon 215 or Zoom?)\nWednesday: intro to Bayesian inference\nFriday: Bayes lab"
  },
  {
    "objectID": "slides/lecture04-bayes1.html#references-1",
    "href": "slides/lecture04-bayes1.html#references-1",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nBetancourt, M. (2018, October). Probability Theory (For Scientists and Engineers). Retrieved from https://betanalpha.github.io/assets/case_studies/probability_theory.html#7_conclusion\n\n\nGelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (2014). Bayesian Data Analysis (3rd ed.). Chapman & Hall/CRC Boca Raton, FL, USA."
  },
  {
    "objectID": "slides/lecture01-welcome.html#about-me",
    "href": "slides/lecture01-welcome.html#about-me",
    "title": "Welcome to CEVE 543!!",
    "section": "About me",
    "text": "About me\n\n\n\n\nDr.¬†James Doss-Gollin\nAssistant professor in CEVE\nInterested in bridging Earth science, data science, and decision science to improve climate risk management and long-term infrastructure planning\nHometown: New Haven, CT (‚ù§Ô∏è for Houston, NYC, and Luque, Paraguay)\nDoss-Gollin lab\n\n\n\n\n\n\nJames Doss-Gollin\n\n\n\n\n\nOffice hours MWF TBD"
  },
  {
    "objectID": "slides/lecture01-welcome.html#ta",
    "href": "slides/lecture01-welcome.html#ta",
    "title": "Welcome to CEVE 543!!",
    "section": "TA",
    "text": "TA\n\n\n\n\n\nYuchen Lu\n\n\n\n\nYuchen Lu\nThird year Ph.D.¬†student in CEVE\nCurrently researching statistical methods to estimate the probability of extreme precipitation\nHometown: Wuhan, China (via Pittsburgh)\n\n\n\n\nYuchen will mainly help me with grading, but you can also reach out to her with questions"
  },
  {
    "objectID": "slides/lecture01-welcome.html#your-turn",
    "href": "slides/lecture01-welcome.html#your-turn",
    "title": "Welcome to CEVE 543!!",
    "section": "Your turn!",
    "text": "Your turn!\n\nYour name\nYour field and program of study\nYour hometown(s)"
  },
  {
    "objectID": "slides/lecture01-welcome.html#what-is-one-thing-you-hope-to-learn-this-semester",
    "href": "slides/lecture01-welcome.html#what-is-one-thing-you-hope-to-learn-this-semester",
    "title": "Welcome to CEVE 543!!",
    "section": "What is one thing you hope to learn this semester?",
    "text": "What is one thing you hope to learn this semester?\n\nTake a moment to think, write it down, and then we‚Äôll share.\n\n\n\nGood to get a sense of background and goals\nInvite students to share ‚Äì this will help me get a sense of how forthcoming they are"
  },
  {
    "objectID": "slides/lecture01-welcome.html#floods-in-paraguay-2015",
    "href": "slides/lecture01-welcome.html#floods-in-paraguay-2015",
    "title": "Welcome to CEVE 543!!",
    "section": "Floods in Paraguay, 2015",
    "text": "Floods in Paraguay, 2015\n\nFigure¬†1: Municipalidad de Asunci√≥n"
  },
  {
    "objectID": "slides/lecture01-welcome.html#tx-cold-snap-2021",
    "href": "slides/lecture01-welcome.html#tx-cold-snap-2021",
    "title": "Welcome to CEVE 543!!",
    "section": "TX Cold Snap, 2021",
    "text": "TX Cold Snap, 2021\n\n\n\n\n\n\n\n\n\n\nFigure¬†2: Go Nakamura for Getty Images"
  },
  {
    "objectID": "slides/lecture01-welcome.html#what-other-climate-hazards-do-you-know-about",
    "href": "slides/lecture01-welcome.html#what-other-climate-hazards-do-you-know-about",
    "title": "Welcome to CEVE 543!!",
    "section": "What other climate hazards do you know about?",
    "text": "What other climate hazards do you know about?\n\nTake a moment to think, write it down, and then we‚Äôll share."
  },
  {
    "objectID": "slides/lecture01-welcome.html#how-do-we-manage-climate-risks",
    "href": "slides/lecture01-welcome.html#how-do-we-manage-climate-risks",
    "title": "Welcome to CEVE 543!!",
    "section": "How do we manage climate risks?",
    "text": "How do we manage climate risks?\n\n\nReduce emissions to prevent future climate change (‚Äúmitigation‚Äù)\nReal-time monitoring and forecasting\nBuilding codes and design standards\nInsurance\nand much more!"
  },
  {
    "objectID": "slides/lecture01-welcome.html#bayesian-decision-theory",
    "href": "slides/lecture01-welcome.html#bayesian-decision-theory",
    "title": "Welcome to CEVE 543!!",
    "section": "Bayesian Decision Theory",
    "text": "Bayesian Decision Theory\nExpected reward \\(R\\) (equivalently utility, loss, etc.) for taking some decision \\(a \\in \\mathcal{A}\\): \\[\n\\mathbb{E}(R(a)) = \\int_{\\mathcal{S}} R(a, {\\bf{s}}) p({\\bf{s}}) d{\\bf{s}}\n\\] Crucial insight: \\[\n\\mathbb{E}(R(a)) \\neq R(a, \\mathbb{E}(\\bf{s}))\n\\]"
  },
  {
    "objectID": "slides/lecture01-welcome.html#implications",
    "href": "slides/lecture01-welcome.html#implications",
    "title": "Welcome to CEVE 543!!",
    "section": "Implications",
    "text": "Implications\n\n\nWe often care about extremes\nUncertainty (especially of extremes) matters\nWhat makes a ‚Äúgood‚Äù estimates of \\(p(\\bf{s})\\)?\n\nPhysically accurate / realistic\nHigh spatial and temporal resolution to quantify impacts on people and infrastructure\nLarge ensemble sizes to quantify uncertainty\nMultiple scenarios (of ‚Äúdeep‚Äù uncertainties)"
  },
  {
    "objectID": "slides/lecture01-welcome.html#fat-tails",
    "href": "slides/lecture01-welcome.html#fat-tails",
    "title": "Welcome to CEVE 543!!",
    "section": "Fat tails",
    "text": "Fat tails\n\nBonnafous et al. (2017)"
  },
  {
    "objectID": "slides/lecture01-welcome.html#quasi-periodic-oscillations",
    "href": "slides/lecture01-welcome.html#quasi-periodic-oscillations",
    "title": "Welcome to CEVE 543!!",
    "section": "Quasi-periodic oscillations",
    "text": "Quasi-periodic oscillations\n\nDoss-Gollin et al. (2019)"
  },
  {
    "objectID": "slides/lecture01-welcome.html#nonstationarity",
    "href": "slides/lecture01-welcome.html#nonstationarity",
    "title": "Welcome to CEVE 543!!",
    "section": "Nonstationarity",
    "text": "Nonstationarity\n\nFagnant et al. (2020)"
  },
  {
    "objectID": "slides/lecture01-welcome.html#vary-on-multiple-temporal-scales",
    "href": "slides/lecture01-welcome.html#vary-on-multiple-temporal-scales",
    "title": "Welcome to CEVE 543!!",
    "section": "Vary on multiple temporal scales",
    "text": "Vary on multiple temporal scales\n\nDoss-Gollin et al. (2019)"
  },
  {
    "objectID": "slides/lecture01-welcome.html#spatial-structure",
    "href": "slides/lecture01-welcome.html#spatial-structure",
    "title": "Welcome to CEVE 543!!",
    "section": "Spatial structure",
    "text": "Spatial structure\n\nFarnham et al. (2018)"
  },
  {
    "objectID": "slides/lecture01-welcome.html#emphasis-on-extremes",
    "href": "slides/lecture01-welcome.html#emphasis-on-extremes",
    "title": "Welcome to CEVE 543!!",
    "section": "Emphasis on extremes",
    "text": "Emphasis on extremes\n\nDoss-Gollin & Keller (2023)"
  },
  {
    "objectID": "slides/lecture01-welcome.html#deep-uncertainty",
    "href": "slides/lecture01-welcome.html#deep-uncertainty",
    "title": "Welcome to CEVE 543!!",
    "section": "Deep Uncertainty",
    "text": "Deep Uncertainty\n\nWalker et al. (2013)"
  },
  {
    "objectID": "slides/lecture01-welcome.html#syllabus",
    "href": "slides/lecture01-welcome.html#syllabus",
    "title": "Welcome to CEVE 543!!",
    "section": "Syllabus",
    "text": "Syllabus\nSyllabus"
  },
  {
    "objectID": "slides/lecture01-welcome.html#lectures",
    "href": "slides/lecture01-welcome.html#lectures",
    "title": "Welcome to CEVE 543!!",
    "section": "Lectures",
    "text": "Lectures\n\nTests will cover material from lectures and labs\n\nSlides will be posted ahead of time on course website (see instructions for printing to PDF)\n\nOccasional readings (assigned ahead of time on Canvas)\nI am not a mind reader! Ask questions."
  },
  {
    "objectID": "slides/lecture01-welcome.html#labs-10",
    "href": "slides/lecture01-welcome.html#labs-10",
    "title": "Welcome to CEVE 543!!",
    "section": "Labs (10%)",
    "text": "Labs (10%)\n\nBuild your hands-on computational skills\nMost weeks, generally Friday\nApply conecpts from lectures to simple problems\nGraded on a 3 point scale\nDue one week after the in-class lab; solutions will be posted and discussed so no late submissions (turn in what you have)"
  },
  {
    "objectID": "slides/lecture01-welcome.html#tests-40",
    "href": "slides/lecture01-welcome.html#tests-40",
    "title": "Welcome to CEVE 543!!",
    "section": "Tests (40%)",
    "text": "Tests (40%)\n\nMaterial from lecture, assigned readings, and labs\nAlways a review session"
  },
  {
    "objectID": "slides/lecture01-welcome.html#projects-40",
    "href": "slides/lecture01-welcome.html#projects-40",
    "title": "Welcome to CEVE 543!!",
    "section": "Projects (40%)",
    "text": "Projects (40%)\n\nApply concepts from class and lab to a more complex and open-ended problem\nEach module (except intro) centers on a project\n\nThree rainfall-focused projects planned:\n\nDownscaling\nFrequency analysis\nWeather typing"
  },
  {
    "objectID": "slides/lecture01-welcome.html#participation-10",
    "href": "slides/lecture01-welcome.html#participation-10",
    "title": "Welcome to CEVE 543!!",
    "section": "Participation (10%)",
    "text": "Participation (10%)\nSome ways to participate include:\n\nAttending every class\nAsking questions in class\nAnswering questions on Canvas\nComing to office hours\n\nWe will co-grade your participation for every module"
  },
  {
    "objectID": "slides/lecture01-welcome.html#job-opportunities",
    "href": "slides/lecture01-welcome.html#job-opportunities",
    "title": "Welcome to CEVE 543!!",
    "section": "Job opportunities",
    "text": "Job opportunities\nGrowing climate analytics opportunities in:\n\nInsurance\nFinance\nAgriculture\nEngineering\n\nAnd more! These require an understanding of climate, probability, statistics, coding, and communication."
  },
  {
    "objectID": "slides/lecture01-welcome.html#pre-requistes-linear-algebra",
    "href": "slides/lecture01-welcome.html#pre-requistes-linear-algebra",
    "title": "Welcome to CEVE 543!!",
    "section": "Pre-requistes: Linear Algebra",
    "text": "Pre-requistes: Linear Algebra\nYou need basic matrix notation and multiplication, but not much more. Let \\[\nA = \\left[ \\begin{matrix} a & b \\\\ c & d \\end{matrix} \\right], \\quad\nB = \\left[ \\begin{matrix} e & f \\\\ g & h \\end{matrix} \\right], \\quad\nx = \\left[ \\begin{matrix} k \\\\ \\ell \\end{matrix} \\right], \\quad\n\\]\nYou should be able to (with note-checking as needed!) figure out:\n\n\n\n\\(A_{2,1}\\)\n\\(A + B\\)\n\\(AB\\)\n\n\n\n\\(A x\\)\n\\(x^T x\\)\n\\(x x^T\\)"
  },
  {
    "objectID": "slides/lecture01-welcome.html#pre-requisites-probability-and-statistics",
    "href": "slides/lecture01-welcome.html#pre-requisites-probability-and-statistics",
    "title": "Welcome to CEVE 543!!",
    "section": "Pre-requisites: Probability and Statistics",
    "text": "Pre-requisites: Probability and Statistics\nYou should have a course in applied statistics. You should be able to:\n\nCompute summary statistics of a sample\nDefine joint, marginal, and conditional distributions\nUnderstand probability density functions, quantiles, and cumulative distribution functions\nExplain a few probability distributions and where they are appropriate\nPerform and interpret linear regressions"
  },
  {
    "objectID": "slides/lecture01-welcome.html#pre-requisites-coding",
    "href": "slides/lecture01-welcome.html#pre-requisites-coding",
    "title": "Welcome to CEVE 543!!",
    "section": "Pre-requisites: Coding",
    "text": "Pre-requisites: Coding\nWe will use the Julia programming language. I think you‚Äôll find it easy and fun to learn!\n\nNo experience in Julia is expected\nSome prior experience coding (R, Python, Matlab, C, etc.) is suggested\nIf you have no prior coding experience, you will need to put in extra effort to learn the basics"
  },
  {
    "objectID": "slides/lecture01-welcome.html#questions",
    "href": "slides/lecture01-welcome.html#questions",
    "title": "Welcome to CEVE 543!!",
    "section": "Questions?",
    "text": "Questions?\n\nWednesday: ‚ÄúWhat drives uncertain climate hazard?‚Äù\nFriday: ‚ÄúLab 01: Setting up Julia, GitHub, and Quarto‚Äù"
  },
  {
    "objectID": "slides/lecture01-welcome.html#references",
    "href": "slides/lecture01-welcome.html#references",
    "title": "Welcome to CEVE 543!!",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nBonnafous, L., Lall, U., & Siegel, J. (2017). A water risk index for portfolio exposure to climatic extremes: Conceptualization and an application to the mining industry. Hydrology and Earth System Sciences, 21(4), 2075‚Äì2106. https://doi.org/f96k67\n\n\nDoss-Gollin, J., & Keller, K. (2023). A subjective Bayesian framework for synthesizing deep uncertainties in climate risk management. Earth‚Äôs Future, 11(1). https://doi.org/10.1029/2022EF003044\n\n\nDoss-Gollin, J., Farnham, D. J., Steinschneider, S., & Lall, U. (2019). Robust adaptation to multiscale climate variability. Earth‚Äôs Future, 7(7), 734‚Äì747. https://doi.org/10.1029/2019ef001154\n\n\nFagnant, C., Gori, A., Sebastian, A., Bedient, P. B., & Ensor, K. B. (2020). Characterizing spatiotemporal trends in extreme precipitation in Southeast Texas. Natural Hazards, 104(2), 1597‚Äì1621. https://doi.org/10.1007/s11069-020-04235-x\n\n\nFarnham, D. J., Doss-Gollin, J., & Lall, U. (2018). Regional extreme precipitation events: Robust inference from credibly simulated GCM variables. Water Resources Research, 54(6). https://doi.org/10.1002/2017wr021318\n\n\nWalker, W. E., Lempert, R. J., & Kwakkel, J. H. (2013). Deep Uncertainty. In S. I. Gass & M. C. Fu (Eds.), Encyclopedia of Operations Research and Management Science (pp. 395‚Äì402). Boston, MA: Springer US. https://doi.org/10.1007/978-1-4419-1153-7_1140"
  },
  {
    "objectID": "slides/lecture09-hyperparameters.html#tbd",
    "href": "slides/lecture09-hyperparameters.html#tbd",
    "title": "Hyperparameter tuning (Planned)",
    "section": "TBD",
    "text": "TBD\nThis is a placeholder for a planned lecture. The schedule will be modified as needed."
  },
  {
    "objectID": "slides/lecture10-extreme-value-theory.html#tbd",
    "href": "slides/lecture10-extreme-value-theory.html#tbd",
    "title": "Extreme Value Theory (Planned)",
    "section": "TBD",
    "text": "TBD\nThis is a placeholder for a planned lecture. The schedule will be modified as needed."
  },
  {
    "objectID": "slides/lecture15-review.html#tbd",
    "href": "slides/lecture15-review.html#tbd",
    "title": "Module 3 Review (Planned)",
    "section": "TBD",
    "text": "TBD\nThis is a placeholder for a planned lecture. The schedule will be modified as needed."
  },
  {
    "objectID": "slides/lecture01-climate.html#sizing-a-stormwater-pipe",
    "href": "slides/lecture01-climate.html#sizing-a-stormwater-pipe",
    "title": "What drives uncertain climate hazard?",
    "section": "Sizing a stormwater pipe",
    "text": "Sizing a stormwater pipe\n\n\n\n\nRainfall-runoff model\n\ne.g., peak flow from rational method: \\(Q = CiA\\)\n\\(i\\) is rainfall intensity, \\(A\\) is area, and \\(C\\) is runoff coefficient\n\nDesign rainfall based on return period \\(T\\)\n\n\\(p(i &gt; i^*) = 1/T\\)\n\nSize your culvert to handle \\(Q^* = Ci^*A\\)\nRequires knowing \\(p(i)\\)!\n\n\n\n\n\n\nDrainage installation"
  },
  {
    "objectID": "slides/lecture01-climate.html#floodplain-mapping-in-a-riverine-system",
    "href": "slides/lecture01-climate.html#floodplain-mapping-in-a-riverine-system",
    "title": "What drives uncertain climate hazard?",
    "section": "Floodplain mapping in a riverine system",
    "text": "Floodplain mapping in a riverine system\n\n\n\nThis is a moderately simplified workflow\n\n\n\nAnalyze historical streamflow data at a gauge\nTake the 99th percentile (100 year return level) of annual maximum streamflows\nUse a hydraulic model to model where the water goes\n\n\n\n\n\n\nFloodplain in Selinsgrove, PA"
  },
  {
    "objectID": "slides/lecture01-climate.html#reservoir-sizing-simplest-version",
    "href": "slides/lecture01-climate.html#reservoir-sizing-simplest-version",
    "title": "What drives uncertain climate hazard?",
    "section": "Reservoir sizing (simplest version)",
    "text": "Reservoir sizing (simplest version)\n\n\nConsider \\(N\\) years of inflows (and releases, evaporation, etc)\nCount number of times reservoir is empty (‚Äúfailure‚Äù)\nRepeat experiment many different times with different inflows\nIf you are sampling this from \\(p(\\text{inflow})\\), you can estimate the reliability\n\nMonte Carlo method\nWhy not just use observed inflows?"
  },
  {
    "objectID": "slides/lecture01-climate.html#index-insurance-pricing",
    "href": "slides/lecture01-climate.html#index-insurance-pricing",
    "title": "What drives uncertain climate hazard?",
    "section": "Index insurance pricing",
    "text": "Index insurance pricing\n\n\nIndex insurance: if some index \\(I\\) is above a threshold \\(I^*\\), pay out \\(X\\)\n\nTotal rainfall in a season, area flooded, etc\n\nLet \\(p^* = p(I &gt; I^*)\\) is the probability of a payout\nNaive pricing: \\(R = p^* X\\)\nRisk premium: \\(R = X \\left( \\mathbb{E}[p^*] + \\lambda \\mathbb{V}^{1/2}[p^*] \\right)\\)"
  },
  {
    "objectID": "slides/lecture01-climate.html#other-examples",
    "href": "slides/lecture01-climate.html#other-examples",
    "title": "What drives uncertain climate hazard?",
    "section": "Other examples",
    "text": "Other examples\n\n\nSeasonal electricity resource adequacy (Doss-Gollin et al., 2021)\nLevee design (Garner & Keller, 2018)\nWater supply planning (Fletcher et al., 2019)\nMultihazard design (Bruneau et al., 2017)\netc‚Ä¶"
  },
  {
    "objectID": "slides/lecture01-climate.html#important-note",
    "href": "slides/lecture01-climate.html#important-note",
    "title": "What drives uncertain climate hazard?",
    "section": "Important note",
    "text": "Important note\nAll of these workflows are slightly simplified, but communicate the main idea. For each of these motivating problems, we need to know the probability distribution of some hazard ‚Äì \\(p(\\bf{s})\\) to use our notation from last class"
  },
  {
    "objectID": "slides/lecture01-climate.html#storms-on-the-north-sea",
    "href": "slides/lecture01-climate.html#storms-on-the-north-sea",
    "title": "What drives uncertain climate hazard?",
    "section": "Storms on the North Sea",
    "text": "Storms on the North Sea\n\n\n\n\n\nWaves\n\n\n\n\n\n\nSynoptic Forecast"
  },
  {
    "objectID": "slides/lecture01-climate.html#storm-surge-in-houston",
    "href": "slides/lecture01-climate.html#storm-surge-in-houston",
    "title": "What drives uncertain climate hazard?",
    "section": "Storm surge in Houston",
    "text": "Storm surge in Houston\n\n\n\n\n\nIke flood depths\n\n\n\n\n\n\nIke path"
  },
  {
    "objectID": "slides/lecture01-climate.html#problem-statement",
    "href": "slides/lecture01-climate.html#problem-statement",
    "title": "What drives uncertain climate hazard?",
    "section": "Problem statement:",
    "text": "Problem statement:\n\nYou are designing a storm surge barrier on Galveston Bay. What is the probability distribution of storm surge at your location?\n\nThis knowledge will help you trade off the cost of the barrier against the residual risk of flooding."
  },
  {
    "objectID": "slides/lecture01-climate.html#what-do-we-need-to-know",
    "href": "slides/lecture01-climate.html#what-do-we-need-to-know",
    "title": "What drives uncertain climate hazard?",
    "section": "What do we need to know?",
    "text": "What do we need to know?\n\nTake a moment, write, and then share"
  },
  {
    "objectID": "slides/lecture01-climate.html#historical-data",
    "href": "slides/lecture01-climate.html#historical-data",
    "title": "What drives uncertain climate hazard?",
    "section": "Historical data",
    "text": "Historical data\n\n\n\n\n\nMaximum water levels at Galveston\n\n\n\n\n\n\nReturn levels"
  },
  {
    "objectID": "slides/lecture01-climate.html#tropical-cyclone-tracks-and-characteristics",
    "href": "slides/lecture01-climate.html#tropical-cyclone-tracks-and-characteristics",
    "title": "What drives uncertain climate hazard?",
    "section": "Tropical cyclone tracks and characteristics",
    "text": "Tropical cyclone tracks and characteristics\nCan we use models to create a longer ‚Äúsynthetic‚Äù record?\n\nBloemendaal et al. (2020)"
  },
  {
    "objectID": "slides/lecture01-climate.html#wind-and-rainfall-fields",
    "href": "slides/lecture01-climate.html#wind-and-rainfall-fields",
    "title": "What drives uncertain climate hazard?",
    "section": "Wind and rainfall fields",
    "text": "Wind and rainfall fields\nIf we‚Äôre going to generate synthetic storms, we need to model the wind and rainfall fields (and other boundary conditions) in order to model the storm surge (using Adcirc, GeoClaw, SFINCS, etc)\n\nKleiber et al. (2023)"
  },
  {
    "objectID": "slides/lecture01-climate.html#sea-level",
    "href": "slides/lecture01-climate.html#sea-level",
    "title": "What drives uncertain climate hazard?",
    "section": "Sea level",
    "text": "Sea level\n\n\n\n\n\nGalveston Relative Sea Level and Projections\n\n\n\nWhat separates the scenarios? To first order:\n\nHow much \\(CO_2\\) we emit\nHow much the climate system warms in response to \\(CO_2\\) (climate sensitivity)\nIce sheet response to temperatures"
  },
  {
    "objectID": "slides/lecture01-climate.html#limitations",
    "href": "slides/lecture01-climate.html#limitations",
    "title": "What drives uncertain climate hazard?",
    "section": "Limitations",
    "text": "Limitations\nSobel et al. (2023):\n\nModels are incorrectly simulating the equatorial Pacific response to greenhouse gas warming. This implies that projections of regional tropical cyclone activity may be incorrect as well"
  },
  {
    "objectID": "slides/lecture01-climate.html#lessons-learned",
    "href": "slides/lecture01-climate.html#lessons-learned",
    "title": "What drives uncertain climate hazard?",
    "section": "Lessons learned?",
    "text": "Lessons learned?\n\n\nHistorical data\n\nMeasures what we care about\nSampling uncertainty\nDoesn‚Äôt account for future conditions\n\nModel simulations\n\nCan account for future conditions\nMay be biased or inaccurate\nModel structure uncertainty"
  },
  {
    "objectID": "slides/lecture01-climate.html#some-terms-to-know",
    "href": "slides/lecture01-climate.html#some-terms-to-know",
    "title": "What drives uncertain climate hazard?",
    "section": "Some terms to know üòâ",
    "text": "Some terms to know üòâ\n\nReturn period\nReturn level\nMonte Carlo\nSynthetic record\nClimate sensitivity\n\nYou should also be able to reason about the merits and limitations of different methodologies for estimating the probability distribution of a hazard (more practice incoming!)"
  },
  {
    "objectID": "slides/lecture01-climate.html#questions",
    "href": "slides/lecture01-climate.html#questions",
    "title": "What drives uncertain climate hazard?",
    "section": "Questions?",
    "text": "Questions?\nFriday:\n\nBring your laptop, if you have one.\nCreate an account at https://github.com/\n\nI will be absent on Friday (visiting Harris County Flood Control District). Yuchen will lead lab 01."
  },
  {
    "objectID": "slides/lecture01-climate.html#references",
    "href": "slides/lecture01-climate.html#references",
    "title": "What drives uncertain climate hazard?",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nBloemendaal, N., Haigh, I. D., de Moel, H., Muis, S., Haarsma, R. J., & Aerts, J. C. J. H. (2020). Generation of a global synthetic tropical cyclone hazard dataset using STORM. Scientific Data, 7(1, 1), 40. https://doi.org/10.1038/s41597-020-0381-2\n\n\nBruneau, M., Barbato, M., Padgett, J. E., Zaghi, A. E., Mitrani-Reiser, J., & Li, Y. (2017). State of the art of multihazard design. Journal of Structural Engineering, 143(10), 03117002. https://doi.org/10.1061/(ASCE)ST.1943-541X.0001893\n\n\nDoss-Gollin, J., Farnham, D. J., Lall, U., & Modi, V. (2021). How unprecedented was the February 2021 Texas cold snap? Environmental Research Letters. https://doi.org/10.1088/1748-9326/ac0278\n\n\nFletcher, S., Lickley, M., & Strzepek, K. (2019). Learning about climate change uncertainty enables flexible water infrastructure planning. Nature Communications, 10(1), 1782. https://doi.org/10.1038/s41467-019-09677-x\n\n\nGarner, G. G., & Keller, K. (2018). Using direct policy search to identify robust strategies in adapting to uncertain sea-level rise and storm surge. Environmental Modelling & Software, 107, 96‚Äì104. https://doi.org/10.1016/j.envsoft.2018.05.006\n\n\nKleiber, W., Sain, S., Madaus, L., & Harr, P. (2023). Stochastic tropical cyclone precipitation field generation. Environmetrics, 34(1), e2766. https://doi.org/10.1002/env.2766\n\n\nSobel, A. H., Lee, C.-Y., Bowen, S. G., Camargo, S. J., Cane, M. A., Clement, A., et al. (2023). Near-term tropical cyclone risk and coupled Earth system model biases. Proceedings of the National Academy of Sciences, 120(33), e2209631120. https://doi.org/10.1073/pnas.2209631120"
  },
  {
    "objectID": "slides/lecture12-regionalization.html#tbd",
    "href": "slides/lecture12-regionalization.html#tbd",
    "title": "Regionalization and spatial pooling (Planned)",
    "section": "TBD",
    "text": "TBD\nThis is a placeholder for a planned lecture. The schedule will be modified as needed."
  },
  {
    "objectID": "slides/lecture13-tbd2.html#tbd",
    "href": "slides/lecture13-tbd2.html#tbd",
    "title": "Placeholder TBD (Planned)",
    "section": "TBD",
    "text": "TBD\nThis is a placeholder for a planned lecture. The schedule will be modified as needed."
  },
  {
    "objectID": "slides/lecture06-module-1-wrapup.html#exam-1",
    "href": "slides/lecture06-module-1-wrapup.html#exam-1",
    "title": "Module 1 practice problems",
    "section": "Exam 1",
    "text": "Exam 1\n\nAverage: 47.5\nStandard deviation: 15.9\nMaximum: 70\nMedian: 49.5\nEasiest: maximum likelihood\nHardest: computation, conditional probability"
  },
  {
    "objectID": "slides/lecture06-module-1-wrapup.html#how-ill-support-your-learning",
    "href": "slides/lecture06-module-1-wrapup.html#how-ill-support-your-learning",
    "title": "Module 1 practice problems",
    "section": "How I‚Äôll support your learning",
    "text": "How I‚Äôll support your learning\n\nReorganized schedule ‚Äì two modules not three\nPractice problems at start of class\nMore links to suggested reading for each lecture"
  },
  {
    "objectID": "slides/lecture06-module-1-wrapup.html#recommended-reading",
    "href": "slides/lecture06-module-1-wrapup.html#recommended-reading",
    "title": "Module 1 practice problems",
    "section": "Recommended reading",
    "text": "Recommended reading\n\nThinkBayes: simple and conceptual online textbook on Bayesian statistics\nRethinking Statistics: a more serious text on Bayesian thinking and estimation\n\nCode examples on the website, including implementations in Turing\n\nAn Introduction to Statistical Learning: a machine learning perspective\n\nCode examples in R and Python\n\n\nThere are many other resources available online; use them critically."
  },
  {
    "objectID": "slides/lecture06-module-1-wrapup.html#how-you-need-to-support-your-learning",
    "href": "slides/lecture06-module-1-wrapup.html#how-you-need-to-support-your-learning",
    "title": "Module 1 practice problems",
    "section": "How you need to support your learning",
    "text": "How you need to support your learning\n\nCome to office hours\nReview lecture notes\n\nDon‚Äôt just read\nWork through examples\nUnderstand why\nAsk questions\n\n\nI have encouraged you to use tools (Copilot, GPT, etc) to help you with computing syntax so you can spend more time on understanding concepts."
  },
  {
    "objectID": "slides/lecture06-module-1-wrapup.html#revisions-1",
    "href": "slides/lecture06-module-1-wrapup.html#revisions-1",
    "title": "Module 1 practice problems",
    "section": "Revisions",
    "text": "Revisions\n\nExam I is graded\nFinal grades will be curved, not each exam\nRevisions will be allowed"
  },
  {
    "objectID": "slides/lecture06-module-1-wrapup.html#due-date",
    "href": "slides/lecture06-module-1-wrapup.html#due-date",
    "title": "Module 1 practice problems",
    "section": "Due Date",
    "text": "Due Date\n\nFriday, October 6th at 11:00AM.\nHand in to me in class\n\nLegible and clear handwritten work OR\nType up your work1\nHand in your original exam with your revisions\n\n\nIf you type, disable tools such as GitHub Copilot"
  },
  {
    "objectID": "slides/lecture06-module-1-wrapup.html#academic-integrity",
    "href": "slides/lecture06-module-1-wrapup.html#academic-integrity",
    "title": "Module 1 practice problems",
    "section": "Academic integrity",
    "text": "Academic integrity\n\nSubject to Rice Honor Code\nDO: consult class notes, textbooks, linked resources, or write your own code\nDO NOT: Consult with a classmate, search the internet, usie AI chat tools, or otherwise collaborate is not permitted\nAsk if you‚Äôre not sure\n\nIf you have questions about what is permitted, please ask."
  },
  {
    "objectID": "slides/lecture06-module-1-wrapup.html#instructions",
    "href": "slides/lecture06-module-1-wrapup.html#instructions",
    "title": "Module 1 practice problems",
    "section": "Instructions",
    "text": "Instructions\nFor each problem:\n\nState how many points you earned on the original exam.\nDerive the correct answer. Your answer should be clearly written and easy to follow.\nExplain why your original answer was incorrect and what confused you (no revisions are needed if your original answer was correct!)"
  },
  {
    "objectID": "slides/lecture06-module-1-wrapup.html#grading",
    "href": "slides/lecture06-module-1-wrapup.html#grading",
    "title": "Module 1 practice problems",
    "section": "Grading",
    "text": "Grading\n\nOn each problem, you will earn up to 60% of the points you missed\n\nGrading on the revision will be more strict than on the original exam\n\nAdditional 10% for clear and insightful explanation of your original mistake (where applicable)\nT/F questions: up to 50% for a clear and correct explanation of why the statement is true or false\nIf your revision is worse than the original, your score will not be lowered."
  },
  {
    "objectID": "slides/lecture06-module-1-wrapup.html#suggestions",
    "href": "slides/lecture06-module-1-wrapup.html#suggestions",
    "title": "Module 1 practice problems",
    "section": "Suggestions",
    "text": "Suggestions\n\n\nPrecipitation:\n\nCDF: \\(F(x) = P(X \\leq x)\\)\nCan assume that \\(F(0)=0.6\\)\n\nConditional probability: think hard about how to define \\(A\\) and \\(B\\)\nReturn period: read the wording carefully (‚Äúaccording to the distribution shown‚Ä¶‚Äù, ‚Äú‚Ä¶true return period‚Äù)\nRecording of review session"
  },
  {
    "objectID": "slides/lecture06-module-1-wrapup.html#bayes-rule",
    "href": "slides/lecture06-module-1-wrapup.html#bayes-rule",
    "title": "Module 1 practice problems",
    "section": "Bayes Rule",
    "text": "Bayes Rule\nA doctor is called to see a sick child. The doctor has prior information that 90% of sick children in that neighborhood have the flu, while the other 10% are sick with measles. Assume for simplicity that there are no other illnesses and that no children have both the flu and measles.\nA well-known symptom of measles is a rash. The probability of having a rash if one has measles is 0.95. However, occasionally children with flu also develop rash, and the probability of having a rash if one has flu is 0.08. Upon examining the child, the doctor finds a rash. What is the probability that the child has measles?"
  },
  {
    "objectID": "slides/lecture06-module-1-wrapup.html#analytic-posterior",
    "href": "slides/lecture06-module-1-wrapup.html#analytic-posterior",
    "title": "Module 1 practice problems",
    "section": "Analytic posterior",
    "text": "Analytic posterior\nWe collect some count data and model it using a Poisson likelihood. The Poisson likelihood is given by: \\[\np(y_i | \\lambda) = \\frac{\\lambda^{y_i} e^{-\\lambda}}{y_i!}\n\\] where \\(y_i\\) is the number of counts and \\(\\lambda\\) is the rate parameter of the Poisson distribution. We want to do inference on \\(\\lambda\\). We have a prior belief that \\(\\lambda\\) is distributed as an Exponential distribution with the modified parameterization: \\[\np(\\lambda | \\theta) = \\frac{1}{\\theta} e^{-\\frac{\\lambda}{\\theta}}\n\\] After collecting data \\(y_1, y_2, \\ldots, y_n\\), what is the posterior distribution of \\(\\lambda\\) given our prior parameter \\(\\theta\\)?## Next week:\n\nMonday: Gridded climate data lab\nWednesday: Generalized Linear Models\nFriday: Loss functions"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CEVE 543: Data Science for Climate Risk Assessment",
    "section": "",
    "text": "This is the course website for the Fall 2023 edition of CEVE 543, Data Science for Climate Risk Assessment, taught at Rice University by James Doss-Gollin."
  },
  {
    "objectID": "index.html#course-information",
    "href": "index.html#course-information",
    "title": "CEVE 543: Data Science for Climate Risk Assessment",
    "section": "Course Information",
    "text": "Course Information\n\nDetails on the class and course policies are provided in the syllabus.\nTopics, slides, assignments, and other materials can be found in the schedule."
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "CEVE 543: Data Science for Climate Risk Assessment",
    "section": "Instructor",
    "text": "Instructor\nDr.¬†James Doss-Gollin is an assistant professor of Civil and Environmental Engineering at Rice University. His research integrates Earth science, data science, and decision science to address challenges in climate risk management, water resources, and energy system resilience. He also teaches CEVE 421/521 (Climate Risk Management)."
  },
  {
    "objectID": "index.html#software-tools",
    "href": "index.html#software-tools",
    "title": "CEVE 543: Data Science for Climate Risk Assessment",
    "section": "Software Tools",
    "text": "Software Tools\n\nThis course will use the Julia programming language. Julia is a modern, free, open source language designed for scientific computing.\nNo prior knowledge of Julia (or other programming languages) is required. We will cover all required material in labs.\nAssignments will be distributed using GitHub Classroom."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "CEVE 543: Data Science for Climate Risk Assessment",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe layout for this site was inspired by and draws from Vivek Srikrishnan‚Äôs Environmental Systems Analysis course at Cornell, STA 210 at Duke University, and Andrew Heiss‚Äôs course materials at Georgia State."
  },
  {
    "objectID": "resources/julia-plots.html",
    "href": "resources/julia-plots.html",
    "title": "Tutorial: Making Plots with Julia",
    "section": "",
    "text": "This tutorial will give some examples of plotting and plotting features in Julia, as well as providing references to some relevant resources. The main plotting library is Plots.jl, but there are some others that provide useful features."
  },
  {
    "objectID": "resources/julia-plots.html#overview",
    "href": "resources/julia-plots.html#overview",
    "title": "Tutorial: Making Plots with Julia",
    "section": "",
    "text": "This tutorial will give some examples of plotting and plotting features in Julia, as well as providing references to some relevant resources. The main plotting library is Plots.jl, but there are some others that provide useful features."
  },
  {
    "objectID": "resources/julia-plots.html#some-resources",
    "href": "resources/julia-plots.html#some-resources",
    "title": "Tutorial: Making Plots with Julia",
    "section": "Some Resources",
    "text": "Some Resources\n\nPlots.jl useful tips\nPlots.jl examples\nPlot attributes\nAxis attributes\nColor names## Demos\n\n\nusing Plots\nusing Random\nRandom.seed!(1);\n\n\nLine Plots\nTo generate a basic line plot, use plot.\ny = rand(5) plot(y; label=‚Äúoriginal data‚Äù, legend=:topright) ```There‚Äôs a lot of customization here that can occur, a lot of which is discussed in the docs or can be found with some Googling.\n\n\nAdding Plot Elements\nNow we can add some other lines and point markers.\n\ny2 = rand(5)\ny3 = rand(5)\nplot!(y2; label=\"new data\")\nscatter!(y3; label=\"even more data\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemember that an exclamation mark (!) at the end of a function name means that function modifies an object in-place, so plot! and scatter! modify the current plotting object, they don‚Äôt create a new plot.\n\n\nRemoving Plot Elements\nSometimes we want to remove legends, axes, grid lines, and ticks.\n\nplot!(; legend=false, axis=false, grid=false, ticks=false)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\nAspect Ratio\nIf we want to have a square aspect ratio, use ratio = 1.\nv = rand(5) plot(v; ratio=1, legend=false) scatter!(v) ```### Heatmaps\nA heatmap is effectively a plotted matrix with colors chosen according to the values. Use clim to specify a fixed range for the color limits.\nA = rand(10, 10) heatmap(A; clim=(0, 1), ratio=1, legend=false, axis=false, ticks=false)\n\nM = [0 1 0; 0 0 0; 1 0 0]\nwhiteblack = [RGBA(1, 1, 1, 0), RGB(0, 0, 0)]\nheatmap(;\n    c=whiteblack,\n    M,\n    aspect_ratio=1,\n    ticks=0.5:3.5,\n    lims=(0.5, 3.5),\n    gridalpha=1,\n    legend=false,\n    axis=false,\n    ylabel=\"i\",\n    xlabel=\"j\",\n)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nCustom Colors\n\nusing Colors\n\nmycolors = [colorant\"lightslateblue\", colorant\"limegreen\", colorant\"red\"]\nA = [i for i in 50:300, j in 1:100]\nheatmap(A; c=mycolors, clim=(1, 300))\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\nPlotting Areas Under Curves\n\ny = rand(10)\nplot(y; fillrange=y .* 0 .+ 0.5, label=\"above/below 1/2\", legend=:top)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx = LinRange(0, 2, 100)\ny1 = exp.(x)\ny2 = exp.(1.3 .* x)\nplot(x, y1; fillrange=y2, fillalpha=0.35, c=1, label=\"Confidence band\", legend=:topleft)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx = -3:0.01:3\nareaplot(x, exp.(-x .^ 2 / 2) / ‚àö(2œÄ); alpha=0.25, legend=false)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nM = [1 2 3; 7 8 9; 4 5 6; 0 0.5 1.5]\nareaplot(1:3, M; seriescolor=[:red :green :blue], fillalpha=[0.2 0.3 0.4])\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nusing SpecialFunctions f = x -&gt; exp(-x^2 / 2) / ‚àö(2œÄ) Œ¥ = 0.01 plot() x = ‚àö2 .* erfinv.(2 .* ((Œ¥ / 2):Œ¥:1) .- 1) areaplot(x, f.(x); seriescolor=[:red, :blue], legend=false) plot!(x, f.(x); c=:black) ```### Plotting Shapes\n\nrectangle(w, h, x, y) = Shape(x .+ [0, w, w, 0], y .+ [0, 0, h, h])\ncircle(r, x, y) = (Œ∏ = LinRange(0, 2œÄ, 500);\n(x .+ r .* cos.(Œ∏), y .+ r .* sin.(Œ∏)))\nplot(circle(5, 0, 0); ratio=1, c=:red, fill=true)\nplot!(rectangle(5 * ‚àö2, 5 * ‚àö2, -2.5 * ‚àö2, -2.5 * ‚àö2); c=:white, fill=true, legend=false)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting Distributions\nThe StatsPlots.jl package is very useful for making various plots of probability distributions.\n\nusing Distributions, StatsPlots\nplot(Normal(2, 5))\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nscatter(LogNormal(0.8, 1.5))\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also use this functionality to plot distributions of data in tabular data structures like DataFrames.\n\nusing DataFrames\ndat = DataFrame(; a=1:10, b=10 .+ rand(10), c=10 .* rand(10))\n@df dat density([:b :c], color=[:black :red])\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEditing Plots Manually\n\npl = plot(1:4, [1, 4, 9, 16])\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npl.attr\n\nRecipesPipeline.DefaultsDict with 30 entries:\n  :dpi                      =&gt; 96\n  :background_color_outside =&gt; :match\n  :plot_titlefontvalign     =&gt; :vcenter\n  :warn_on_unsupported      =&gt; true\n  :background_color         =&gt; RGBA{Float64}(1.0,1.0,1.0,1.0)\n  :inset_subplots           =&gt; nothing\n  :size                     =&gt; (672, 480)\n  :display_type             =&gt; :auto\n  :overwrite_figure         =&gt; true\n  :html_output_format       =&gt; :svg\n  :plot_titlefontfamily     =&gt; :match\n  :plot_titleindex          =&gt; 0\n  :foreground_color         =&gt; RGB{N0f8}(0.0,0.0,0.0)\n  :window_title             =&gt; \"Plots.jl\"\n  :plot_titlefontrotation   =&gt; 0.0\n  :extra_plot_kwargs        =&gt; Dict{Any, Any}()\n  :pos                      =&gt; (0, 0)\n  :plot_titlefonthalign     =&gt; :hcenter\n  :tex_output_standalone    =&gt; false\n  :extra_kwargs             =&gt; :series\n  :thickness_scaling        =&gt; 1\n  :layout                   =&gt; 1\n  :plot_titlelocation       =&gt; :center\n  :plot_titlefontsize       =&gt; 16\n  :plot_title               =&gt; \"\"\n  ‚ãÆ                         =&gt; ‚ãÆ\n\n\n\npl.series_list[1]\n\nPlots.Series(RecipesPipeline.DefaultsDict(:plot_object =&gt; Plot{Plots.GRBackend() n=1}, :subplot =&gt; Subplot{1}, :label =&gt; \"y1\", :fillalpha =&gt; nothing, :linealpha =&gt; nothing, :linecolor =&gt; RGBA{Float64}(0.0,0.6056031611752245,0.9786801175696073,1.0), :x_extrema =&gt; (NaN, NaN), :series_index =&gt; 1, :markerstrokealpha =&gt; nothing, :markeralpha =&gt; nothing‚Ä¶))\n\n\n\npl[:size] = (300, 200)\n\n(300, 200)\n\n\n\npl\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog-Scaled Axes\n\nxx = 0.1:0.1:10\nplot(xx .^ 2; xaxis=:log, yaxis=:log)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot(exp.(x); yaxis=:log)"
  },
  {
    "objectID": "resources/julia-basics.html",
    "href": "resources/julia-basics.html",
    "title": "Tutorial: Julia Basics",
    "section": "",
    "text": "This tutorial will give some examples of basic Julia commands and syntax."
  },
  {
    "objectID": "resources/julia-basics.html#overview",
    "href": "resources/julia-basics.html#overview",
    "title": "Tutorial: Julia Basics",
    "section": "",
    "text": "This tutorial will give some examples of basic Julia commands and syntax."
  },
  {
    "objectID": "resources/julia-basics.html#getting-help",
    "href": "resources/julia-basics.html#getting-help",
    "title": "Tutorial: Julia Basics",
    "section": "Getting Help",
    "text": "Getting Help\n\nCheck out the official documentation for Julia: https://docs.julialang.org/en/v1/.\nStack Overflow is a commonly-used resource for programming assistance.\nAt a code prompt or in the REPL, you can always type ?functionname to get help."
  },
  {
    "objectID": "resources/julia-basics.html#further-resources",
    "href": "resources/julia-basics.html#further-resources",
    "title": "Tutorial: Julia Basics",
    "section": "Further Resources",
    "text": "Further Resources\nThere are lots of great resources on programming and Julia. Here is a curated list of some particularly helpful tools.\n\n\n\n\n\n\nNote\n\n\n\nSome of these tutorials provide their own instructions on how to install Julia. Please follow the instructions provided in this course\n\n\n\nJulia for Nervous Begineers: A free course on JuliaAcademy for people who are hesitant but curious about learning to write code in Julia.\nFastTrack to Julia cheatsheet\nPlotting cheatsheet\nIntroduction to Computational Thinking: a great Julia-based course at MIT covering applied mathematics and‚Ä¶ computational thinking\nComprehensive Julia Tutorials: YouTube playlist covering a variety of Julia topics, starting with an introduciton to the language.## Comments Comments hide statements from the interpreter or compiler. It‚Äôs a good idea to liberally comment your code so readers (including yourself!) know why your code is structured and written the way it is. Single-line comments in Julia are preceded with a #. Multi-line comments are preceded with #= and ended with =#"
  },
  {
    "objectID": "resources/julia-basics.html#suppressing-output",
    "href": "resources/julia-basics.html#suppressing-output",
    "title": "Tutorial: Julia Basics",
    "section": "Suppressing Output",
    "text": "Suppressing Output\nYou can suppress output using a semi-colon (;).\n\n4 + 8;\n\nThat didn‚Äôt show anything, as opposed to:\n\n4 + 8\n\n12"
  },
  {
    "objectID": "resources/julia-basics.html#variables",
    "href": "resources/julia-basics.html#variables",
    "title": "Tutorial: Julia Basics",
    "section": "Variables",
    "text": "Variables\nVariables are names which correspond to some type of object. These names are bound to objects (and hence their values) using the = operator.\n\nx = 5\n\n5\n\n\nVariables can be manipulated with standard arithmetic operators.\n\n4 + x\n\n9\n\n\nAnother advantage of Julia is the ability to use Greek letters (or other Unicode characters) as variable names. For example, type a backslash followed by the name of the Greek letter (i.e. \\alpha) followed by TAB.\n\nŒ± = 3\n\n3\n\n\nYou can also include subscripts or superscripts in variable names using \\_ and \\^, respectively, followed by TAB. If using a Greek letter followed by a sub- or super-script, make sure you TAB following the name of the letter before the sub- or super-script. Effectively, TAB after you finish typing the name of each \\character.\n\nŒ≤‚ÇÅ = 10 # The name of this variable was entered with \\beta + TAB + \\_1 + TAB\n\n10\n\n\nHowever, try not to overwrite predefined names! For example, you might not want to use œÄ as a variable name‚Ä¶\n\nœÄ\n\nœÄ = 3.1415926535897...\n\n\nIn the grand scheme of things, overwriting œÄ is not a huge deal unless you want to do some trigonometry. However, there are more important predefined functions and variables that you may want to be aware of. Always check that a variable or function name is not predefined!"
  },
  {
    "objectID": "resources/julia-basics.html#data-types",
    "href": "resources/julia-basics.html#data-types",
    "title": "Tutorial: Julia Basics",
    "section": "Data Types",
    "text": "Data Types\nEach datum (importantly, not the variable which is bound to it) has a data type. Julia types are similar to C types, in that they require not only the type of data (Int, Float, String, etc), but also the precision (which is related to the amount of memory allocated to the variable). Issues with precision won‚Äôt be a big deal in this class, though they matter when you‚Äôre concerned about performance vs.¬†decimal accuracy of code.\nYou can identify the type of a variable or expression with the typeof() function.\n\ntypeof(\"This is a string.\")\n\nString\n\n\n\ntypeof(x)\n\nInt64\n\n\n\nNumeric types\nA key distinction is between an integer type (or Int) and a floating-point number type (or float). Integers only hold whole numbers, while floating-point numbers correspond to numbers with fractional (or decimal) parts. For example, 9 is an integer, while 9.25 is a floating point number. The difference between the two has to do with the way the number is stored in memory. 9, an integer, is handled differently in memory than 9.0, which is a floating-point number, even though they‚Äôre mathematically the same value.\n\ntypeof(9)\n\nInt64\n\n\n\ntypeof(9.25)\n\nFloat64\n\n\nSometimes certain function specifications will require you to use a Float variable instead of an Int. One way to force an Int variable to be a Float is to add a decimal point at the end of the integer.\n\ntypeof(9.)\n\nFloat64\n\n\n\n\nStrings\nStrings hold characters, rather than numeric values. Even if a string contains what seems like a number, it is actually stored as the character representation of the digits. As a result, you cannot use arithmetic operators (for example) on this datum.\n\n\"5\" + 5\n\nLoadError: MethodError: no method matching +(::String, ::Int64)\n\n\u001b[0mClosest candidates are:\n\u001b[0m  +(::Any, ::Any, \u001b[91m::Any\u001b[39m, \u001b[91m::Any...\u001b[39m)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m\u001b[4moperators.jl:578\u001b[24m\u001b[39m\n\u001b[0m  +(\u001b[91m::T\u001b[39m, ::T) where T&lt;:Union{Int128, Int16, Int32, Int64, Int8, UInt128, UInt16, UInt32, UInt64, UInt8}\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m\u001b[4mint.jl:87\u001b[24m\u001b[39m\n\u001b[0m  +(\u001b[91m::Rational\u001b[39m, ::Integer)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m\u001b[4mrational.jl:327\u001b[24m\u001b[39m\n\u001b[0m  ...\n\n\nHowever, you can try to tell Julia to interpret a string encoding a numeric character as a numeric value using the parse() function. This can also be used to encode a numeric data as a string.\n\nparse(Int64, \"5\") + 5\n\n10\n\n\nTwo strings can be concatenated using *:\n\n\"Hello\" * \" \" * \"there\"\n\n\"Hello there\"\n\n\n\n\nBooleans\nBoolean variables (or Bools) are logical variables, that can have true or false as values.\n\nb = true\n\ntrue\n\n\nNumerical comparisons, such as ==, !=, or &lt;, return a Bool.\n\nc = 9 &gt; 11\n\nfalse\n\n\nBools are important for logical flows, such as if-then-else blocks or certain types of loops."
  },
  {
    "objectID": "resources/julia-basics.html#mathematical-operations",
    "href": "resources/julia-basics.html#mathematical-operations",
    "title": "Tutorial: Julia Basics",
    "section": "Mathematical operations",
    "text": "Mathematical operations\nAddition, subtraction, multiplication, and division work as you would expect. Just pay attention to types! The type of the output is influenced by the type of the inputs: adding or multiplying an Int by a Float will always result in a Float, even if the Float is mathematically an integer. Division is a little special: dividing an Int by another Int will still return a float, because Julia doesn‚Äôt know ahead of time if the denominator is a factor of the numerator.\n\n3 + 5\n\n8\n\n\n\n3 * 2\n\n6\n\n\n\n3 * 2.\n\n6.0\n\n\n\n6 - 2\n\n4\n\n\n\n9 / 3\n\n3.0\n\n\nRaising a base to an exponent uses ^, not **.\n\n3^2\n\n9\n\n\nJulia allows the use of updating operators to simplify updating a variable in place (in other words, using x += 5 instead of x = x + 5.\n\nBoolean algebra\nLogical operations can be used on variables of type Bool. Typical operators are && (and), || (or), and ! (not).\n\ntrue && true\n\ntrue\n\n\n\ntrue && false\n\nfalse\n\n\n\ntrue || false\n\ntrue\n\n\n\n!true\n\nfalse\n\n\nComparisons can be chained together.\n\n3 &lt; 4 || 8 == 12\n\ntrue\n\n\nWe didn‚Äôt do this above, since Julia doesn‚Äôt require it, but it‚Äôs easier to understand these types of compound expressions if you use parentheses to signal the order of operations. This helps with debugging!\n\n(3 &lt; 4) || (8 == 12)\n\ntrue"
  },
  {
    "objectID": "resources/julia-basics.html#data-structures",
    "href": "resources/julia-basics.html#data-structures",
    "title": "Tutorial: Julia Basics",
    "section": "Data Structures",
    "text": "Data Structures\nData structures are containers which hold multiple values in a convenient fashion. Julia has several built-in data structures, and there are many extensions provided in additional packages.\n\nTuples\nTuples are collections of values. Julia will pay attention to the types of these values, but they can be mixed. Tuples are also immutable: their values cannot be changed once they are defined.\nTuples can be defined by just separating values with commas.\n\ntest_tuple = 4, 5, 6\n\n(4, 5, 6)\n\n\nTo access a value, use square brackets and the desired index. Note: Julia indexing starts at 1, not 0!\n\ntest_tuple[1]\n\n4\n\n\nAs mentioned above, tuples are immutable. What happens if we try to change the value of the first element of test_tuple?\n\ntest_tuple[1] = 5\n\nLoadError: MethodError: no method matching setindex!(::Tuple{Int64, Int64, Int64}, ::Int64, ::Int64)\n\n\nTuples also do not have to hold the same types of values.\n\ntest_tuple_2 = 4, 5., 'h'\ntypeof(test_tuple_2)\n\nTuple{Int64, Float64, Char}\n\n\nTuples can also be defined by enclosing the values in parentheses.\ntest_tuple_3 = (4, 5., 'h')\ntypeof(test_tuple_3)\n\n\nArrays\nArrays also hold multiple values, which can be accessed based on their index position. Arrays are commonly defined using square brackets.\n\ntest_array = [1, 4, 7, 8]\ntest_array[2]\n\n4\n\n\nUnlike tuples, arrays are mutable, and their contained values can be changed later.\n\ntest_array[1] = 6\ntest_array\n\n4-element Vector{Int64}:\n 6\n 4\n 7\n 8\n\n\nArrays also can hold multiple types. Unlike tuples, this causes the array to no longer care about types at all.\n\ntest_array_2 = [6, 5., 'h']\ntypeof(test_array_2)\n\n\nVector{Any} (alias for Array{Any, 1})\n\n\n\nCompare this with test_array:\n\ntypeof(test_array)\n\n\nVector{Int64} (alias for Array{Int64, 1})\n\n\n\n\n\nDictionaries\nInstead of using integer indices based on position, dictionaries are indexed by keys. They are specified by passing key-value pairs to the Dict() method.\n\ntest_dict = Dict(\"A\"=&gt;1, \"B\"=&gt;2)\ntest_dict[\"B\"]\n\n2\n\n\n\n\nComprehensions\nCreating a data structure with more than a handful of elements can be tedious to do by hand. If your desired array follows a certain pattern, you can create structures using a comprehension. Comprehensions iterate over some other data structure (such as an array) implicitly and populate the new data structure based on the specified instructions.\n\n[i^2 for i in 0:1:5]\n\n6-element Vector{Int64}:\n  0\n  1\n  4\n  9\n 16\n 25\n\n\nFor dictionaries, make sure that you also specify the keys.\n\nDict(string(i) =&gt; i^2 for i in 0:1:5)\n\nDict{String, Int64} with 6 entries:\n  \"4\" =&gt; 16\n  \"1\" =&gt; 1\n  \"5\" =&gt; 25\n  \"0\" =&gt; 0\n  \"2\" =&gt; 4\n  \"3\" =&gt; 9"
  },
  {
    "objectID": "resources/julia-basics.html#functions",
    "href": "resources/julia-basics.html#functions",
    "title": "Tutorial: Julia Basics",
    "section": "Functions",
    "text": "Functions\nA function is an object which accepts a tuple of arguments and maps them to a return value. In Julia, functions are defined using the following syntax.\n\nfunction my_actual_function(x, y)\n    return x + y\nend\nmy_actual_function(3, 5)\n\n8\n\n\nFunctions in Julia do not require explicit use of a return statement. They will return the last expression evaluated in their definition. However, it‚Äôs good style to explicitly return function outputs. This improves readability and debugging, especially when functions can return multiple expressions based on logical control flows (if-then-else blocks).\nFunctions in Julia are objects, and can be treated like other objects. They can be assigned to new variables or passed as arguments to other functions.\n\ng = my_actual_function\ng(3, 5)\n\n8\n\n\n\nfunction function_of_functions(f, x, y)\n    return f(x, y)\nend\nfunction_of_functions(g, 3, 5)\n\n8\n\n\n\nShort and Anonymous Functions\nIn addition to the long form of the function definition shown above, simple functions can be specified in more compact forms when helpful.\nThis is the short form:\n\nh‚ÇÅ(x) = x^2 # make the subscript using \\_1 + &lt;TAB&gt;\nh‚ÇÅ(4)\n\n16\n\n\nThis is the anonymous form:\n\nx-&gt;sin(x)\n(x-&gt;sin(x))(œÄ/4)\n\n0.7071067811865475\n\n\n\n\nMutating Functions\nThe convention in Julia is that functions should not modify (or mutate) their input data. The reason for this is to ensure that the data is preserved. Mutating functions are mainly appropriate for applications where performance needs to be optimized, and making a copy of the input data would be too memory-intensive.\nIf you do write a mutating function in Julia, the convention is to add a ! to its name, like my_mutating_function!(x).\n\n\nOptional arguments\nThere are two extremes with regard to function parameters which do not always need to be changed. The first is to hard-code them into the function body, which has a clear downside: when you do want to change them, the function needs to be edited directly. The other extreme is to treat them as regular arguments, passing them every time the function is called. This has the downside of potentially creating bloated function calls, particularly when there is a standard default value that makes sense for most function evaluations.\nMost modern languages, including Julia, allow an alternate solution, which is to make these arguments optional. This involves setting a default value, which is used unless the argument is explicitly defined in a function call.\n\nfunction setting_optional_arguments(x, y, c=0.5)\n    return c * (x + y)\nend\n\nsetting_optional_arguments (generic function with 2 methods)\n\n\nIf we want to stick with the fixed value \\(c=0.5\\), all we have to do is call setting_optional_arguments with the x and y arguments.\n\nsetting_optional_arguments(3, 5)\n\n4.0\n\n\nOtherwise, we can pass a new value for c.\n\nsetting_optional_arguments(3, 5, 2)\n\n16\n\n\n\n\nPassing data structures as arguments\nInstead of passing variables individually, it may make sense to pass a data structure, such as an array or a tuple, and then unpacking within the function definition. This is straightforward in long form: access the appropriate elements using their index.\nIn short or anonymous form, there is a trick which allows the use of readable variables within the function definition.\n\nh‚ÇÇ((x,y)) = x*y # enclose the input arguments in parentheses to tell Julia to expect and unpack a tuple\n\nh‚ÇÇ (generic function with 1 method)\n\n\n\nh‚ÇÇ((2, 3)) # this works perfectly, as we passed in a tuple\n\n6\n\n\n\nh‚ÇÇ(2, 3) # this gives an error, as h‚ÇÇ expects a single tuple, not two different numeric values\n\nLoadError: MethodError: no method matching h‚ÇÇ(::Int64, ::Int64)\n\n\u001b[0mClosest candidates are:\n\u001b[0m  h‚ÇÇ(::Any)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[32mMain\u001b[39m \u001b[90m\u001b[4mIn[50]:1\u001b[24m\u001b[39m\n\n\n\nh‚ÇÇ([3, 10]) # this also works with arrays instead of tuples\n\n30\n\n\n\n\nVectorized operations\nJulia uses dot syntax to vectorize an operation and apply it element-wise across an array.\nFor example, to calculate the square root of 3:\n\nsqrt(3)\n\n1.7320508075688772\n\n\nTo calculate the square roots of every integer between 1 and 5:\n\nsqrt.([1, 2, 3, 4, 5])\n\n5-element Vector{Float64}:\n 1.0\n 1.4142135623730951\n 1.7320508075688772\n 2.0\n 2.23606797749979\n\n\nThe same dot syntax is used for arithmetic operations over arrays, since these operations are really functions.\n\n[1, 2, 3, 4] .* 2\n\n4-element Vector{Int64}:\n 2\n 4\n 6\n 8\n\n\nVectorization can be faster and is more concise to write and read than applying the same function to multiple variables or objects explicitly, so take advantage!\n\n\nReturning multiple values\nYou can return multiple values by separating them with a comma. This implicitly causes the function to return a tuple of values.\n\nfunction return_multiple_values(x, y)\n    return x + y, x * y\nend\nreturn_multiple_values(3, 5)\n\n(8, 15)\n\n\nThese values can be unpacked into multiple variables.\n\nn, ŒΩ = return_multiple_values(3, 5)\nn\n\n8\n\n\n\nŒΩ\n\n15\n\n\n\n\nReturning nothing\nSometimes you don‚Äôt want a function to return any values at all. For example, you might want a function that only prints a string to the console.\n\nfunction print_some_string(x)\n    println(\"x: $x\")\n    return nothing\nend\nprint_some_string(42)\n\nx: 42"
  },
  {
    "objectID": "resources/julia-basics.html#printing-text-output",
    "href": "resources/julia-basics.html#printing-text-output",
    "title": "Tutorial: Julia Basics",
    "section": "Printing Text Output",
    "text": "Printing Text Output\nThe Text() function returns its argument as a plain text string. Notice how this is different from evaluating a string!\n\nText(\"I'm printing a string.\")\n\nI'm printing a string.\n\n\nText() is used in this tutorial as it returns the string passed to it. To print directly to the console, use println().\n\nprintln(\"I'm writing a string to the console.\")\n\nI'm writing a string to the console.\n\n\n\nPrinting Variables In a String\nWhat if we want to include the value of a variable inside of a string? We do this using string interpolation, using $variablename inside of the string.\n\nbar = 42\nText(\"Now I'm printing a variable: $bar\")\n\nNow I'm printing a variable: 42"
  },
  {
    "objectID": "resources/julia-basics.html#control-flows",
    "href": "resources/julia-basics.html#control-flows",
    "title": "Tutorial: Julia Basics",
    "section": "Control Flows",
    "text": "Control Flows\nOne of the tricky things about learning a new programming language can be getting used to the specifics of control flow syntax. These types of flows include conditional if-then-else statements or loops.\n\nConditional Blocks\nConditional blocks allow different pieces of code to be evaluated depending on the value of a boolean expression or variable. For example, if we wanted to compute the absolute value of a number, rather than using abs():\n\nfunction our_abs(x)\n    if x &gt;= 0\n        return x\n    else\n        return -x\n    end\nend\n\nour_abs (generic function with 1 method)\n\n\n\nour_abs(4)\n\n4\n\n\n\nour_abs(-4)\n\n4\n\n\nTo nest conditional statements, use elseif.\n\nfunction test_sign(x)\n    if x &gt; 0\n        return Text(\"x is positive.\")\n    elseif x &lt; 0\n        return Text(\"x is negative.\")\n    else\n        return Text(\"x is zero.\")\n    end\nend\n\ntest_sign (generic function with 1 method)\n\n\n\ntest_sign(-5)\n\nx is negative.\n\n\n\ntest_sign(0)\n\nx is zero.\n\n\n\n\nLoops\nLoops allow expressions to be evaluated repeatedly until they are terminated. The two main types of loops are while loops and for loops.\n\nWhile loops\nwhile loops continue to evaluate an expression so long as a specified boolean condition is true. This is useful when you don‚Äôt know how many iterations it will take for the desired goal to be reached.\n\nfunction compute_factorial(x)\n    factorial = 1\n    while (x &gt; 1)\n        factorial *= x\n        x -= 1\n    end\n    return factorial\nend\ncompute_factorial(5)\n\n120\n\n\n\nWhile loops can easily turn into infinite loops if the condition is never meaningfully updated. Be careful, and look there if your programs are getting stuck. Also, If the expression in a while loop is false when the loop is reached, the loop will never be evaluated.\n\n\n\nFor loops\nfor loops run for a finite number of iterations, based on some defined index variable.\n\nfunction add_some_numbers(x)\n    total_sum = 0 # initialize at zero since we're adding\n    for i in 1:x # the counter i is updated every iteration\n        total_sum += i\n    end\n    return total_sum\nend\nadd_some_numbers(4)\n\n10\n\n\nfor loops can also iterate over explicitly passed containers, rather than iterating over an incrementally-updated index sequence. Use the in keyword when defining the loop.\n\nfunction add_passed_numbers(set)\n    total_sum = 0\n    for i in set # this is the syntax we use when we want i to correspond to different container values\n        total_sum += i\n    end\n    return total_sum\nend\nadd_passed_numbers([1, 3, 5])\n\n9"
  },
  {
    "objectID": "resources/julia-basics.html#linear-algebra",
    "href": "resources/julia-basics.html#linear-algebra",
    "title": "Tutorial: Julia Basics",
    "section": "Linear algebra",
    "text": "Linear algebra\nMatrices are defined in Julia as 2d arrays. Unlike basic arrays, matrices need to contain the same data type so Julia knows what operations are allowed. When defining a matrix, use semicolons to separate rows. Row elements should not be separated by commas.\n\ntest_matrix = [1 2 3; 4 5 6]\n\n2√ó3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n\n\nYou can also specify matrices using spaces and newlines.\n\ntest_matrix_2 = [\n    1 2 3\n    4 5 6\n]\n\n2√ó3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n\n\nFinally, matrices can be created using comprehensions by separating the inputs by a comma.\n\n[i*j for i in 1:1:5, j in 1:1:5]\n\n5√ó5 Matrix{Int64}:\n 1   2   3   4   5\n 2   4   6   8  10\n 3   6   9  12  15\n 4   8  12  16  20\n 5  10  15  20  25\n\n\nVectors are treated as 1d matrices.\n\ntest_row_vector = [1 2 3]\n\n1√ó3 Matrix{Int64}:\n 1  2  3\n\n\n\ntest_col_vector = [1; 2; 3]\n\n3-element Vector{Int64}:\n 1\n 2\n 3\n\n\nMany linear algebra operations on vectors and matrices can be loaded using the LinearAlgebra package."
  },
  {
    "objectID": "resources/julia-basics.html#package-management",
    "href": "resources/julia-basics.html#package-management",
    "title": "Tutorial: Julia Basics",
    "section": "Package management",
    "text": "Package management\nSometimes you might need functionality that does not exist in base Julia. Julia handles packages using the Pkg package manager. After finding a package which has the functions that you need, you have two options:\n\nUse the package management prompt in the Julia REPL (the standard Julia interface; what you get when you type julia in your terminal). Enter this by typing ] at the standard green Julia prompt julia&gt;. This will become a blue pkg&gt;. You can then add new packages using add packagename.\nFrom the standard prompt, enter import Pkg; Pkg.add(packagename). The packagename package can then be used by adding using packagename to the start of the script."
  },
  {
    "objectID": "labs/lab01/template.html",
    "href": "labs/lab01/template.html",
    "title": "Lab 01",
    "section": "",
    "text": "Please see the slides for more details."
  },
  {
    "objectID": "labs/lab01/template.html#overview",
    "href": "labs/lab01/template.html#overview",
    "title": "Lab 01",
    "section": "",
    "text": "Please see the slides for more details."
  },
  {
    "objectID": "labs/lab01/template.html#instructions",
    "href": "labs/lab01/template.html#instructions",
    "title": "Lab 01",
    "section": "Instructions",
    "text": "Instructions\n\nRead and go through the Software Installation Guide for instructions on setting up your computer for this course.\nFollow the link to lab 1 assignment from Canvas (it should start with classroom.github.com). It may take a few minutes for the site to configure your repository.\nYou will get a message saying ‚Äù Your assignment repository has been created: ‚Ä¶‚Äú. Click on the link to go to your repository.\nclone the repository for lab 01 (use the Github Classroom link from Canvas) to your computer. You can use VS Code functionality, GitHub Desktop, or your terminal.\nOpen the directory containing the repository in VS Code doing one of the following:\n\nFrom GitHub desktop: Repository &gt; Open in Visual Studio Code\nIn VS Code: File &gt; Open Folder...\n\nInstantiate the project environment as follows:\n\nOpen the command palette (Ctrl+Shift+P on Windows/Linux, Cmd+Shift+P on Mac)\nStart typing ‚ÄúJulia: Start REPL‚Äù. It will auto-complete; select the command as it appears.\nIn the Julia REPL, type ] to enter the package manager. It should now show something like (lab01) pkg&gt;.\nType instantiate and run it (Enter). This will install all the packages needed for this lab.\nType the backspace key to exit the package manager.\n\nEdit the solutions.qmd file to add your name and netID\nRender the document\n\nOpen the solutions.qmd file\nOpen the command palette and run ‚ÄúQuarto: Render‚Äù. After some activity, a preview of the rendered document should open in VS Code. If you see something like Browse at http://localhost:4200/labs/lab01/solution.html you can open that link in your web browser to see the rendered document.\nCheck the box on line 46 or 47 of the solutions.qmd file to indicate that you were able to render the document. If you were unable to render the document, check the other box and seek help. Make sure the box check renders correctly in the preview\n\nIf you‚Äôre still having trouble:\n\nTry running Pkg.build(\"IJulia\") in the Julia REPL\n\ncommit and push your changes"
  },
  {
    "objectID": "labs/lab05/template.html",
    "href": "labs/lab05/template.html",
    "title": "Lab 05",
    "section": "",
    "text": "Remember to git clone to your machine, then activate and instantiate the environment, as we have done in previous labs.\n\nusing CSV\nusing DataFrames\nusing Dates\nusing Plots\nusing NCDatasets\nusing Unitful\n\n\n\n\n\nNCDatasets.jl\nDataFramesMeta.jl"
  },
  {
    "objectID": "labs/lab05/template.html#setup",
    "href": "labs/lab05/template.html#setup",
    "title": "Lab 05",
    "section": "",
    "text": "Remember to git clone to your machine, then activate and instantiate the environment, as we have done in previous labs.\n\nusing CSV\nusing DataFrames\nusing Dates\nusing Plots\nusing NCDatasets\nusing Unitful"
  },
  {
    "objectID": "labs/lab05/template.html#resources",
    "href": "labs/lab05/template.html#resources",
    "title": "Lab 05",
    "section": "",
    "text": "NCDatasets.jl\nDataFramesMeta.jl"
  },
  {
    "objectID": "labs/lab03/template.html",
    "href": "labs/lab03/template.html",
    "title": "Lab 03",
    "section": "",
    "text": "In the wake of a severe flood, an insurance company has comissioned you to study flood damage in one of the most-affected neighborhoods. This neighborhood is perfectly flat, so we can assume that all houses experienced the same flood depth. However, they are elevated to different heights, use different materials, and are built to different standards, and as a result they experienced different amounts of damage. Specifically, the insurance company has provided you with a dataset of the fraction of the value of each house that was lost in the flood and asked you to model the distribution of losses.\n\n\nOur model for the loss fraction \\(y_i\\) for the \\(i\\)th house is \\[\ny_i | \\alpha, \\beta \\sim \\text{Beta}(\\alpha, \\beta)\n\\] where \\(\\alpha\\) and \\(\\beta\\) are the shape parameters of the Beta Distribution. Because we are not including any explanatory information in our model, we are assuming that the distribution of loss fractions is the same for all houses. This is reasonable for our neighborhood, but would not be applicable to another neighborhood or a different flood event.\nThe above notation is shorthand for \\[\np(y_i | \\alpha, \\beta) =\n\\frac{x^{\\alpha-1}(1-x)^{\\beta-1}} {\\mathrm{B}(\\alpha,\\beta)}\\!\n\\] where \\(\\mathrm {B} (\\alpha ,\\beta )={\\frac {\\Gamma (\\alpha )\\Gamma (\\beta )}{\\Gamma (\\alpha +\\beta )}}\\) and \\(\\Gamma\\) is the Gamma function. We will work directly with the pdf and logpdf functions in the Distributions package, so you don‚Äôt need to memorize this formula..\n\n\n\nAs for labs 1 and 2, make sure you follow the three standard initial setup steps:\n\nOpen the lab 3 folder in VS Code. Do NOT open a ‚Äúparent‚Äù directory containing lab 3. If you‚Äôre not sure what folder you‚Äôre in, open the Juila REPL and type pwd(). It should say something like /.../lab03-username....\nActivate the project environment: ] to enter package mode then activate .. Don‚Äôt forget the . at the end, it‚Äôs very important.\nInstall all required packages: ] to enter package mode, then instantiate.\n\nAt this point, check to make sure you can render the document. In VS Code, open the command palette (Windows: Ctrl+Shift+P, Mac: Cmd+Shift+P) and type Render: HTML. If this gives you trouble, try the following:\n\n] in the Julia REPL to enter Package mode\nbuild IJulia\n\nIf this still gives you trouble, ask for help on Canvas or in-person.\n\n\n\nAs usual we start by specifying the packages we are using\n\nusing Distributions # probability distributions\nusing DelimitedFiles # read data\nusing LaTeXStrings # LaTeX plot labels\nusing Optim # optimization\nusing Plots # plotting\nusing StatsPlots # plot distributions\n\n[ Info: Precompiling StatsPlots [f3b207a7-027a-5e70-b257-86293d7955fd]\n\n\n\n\n\nWe can learn something about the Beta distribution defined above by plotting it for some different values:\n\n1p = plot(; xlabel=L\"y\", ylabel=L\"p(y | \\alpha, \\beta)\", legend=:top)\n2for (Œ±, color) in zip([1.0, 5.0, 25.0], [:orange, :purple, :black])\n3    for (Œ≤, linestyle) in zip([1.0, 5.0, 25.0], [:solid, :dash, :dot])\n4        plot!(\n            p,\n5            Beta(Œ±, Œ≤);\n            label=\"Œ± = $Œ±, Œ≤ = $Œ≤\",\n            color=color,\n            linestyle=linestyle,\n            linewidth=2\n        )\n    end\nend\np\n\n\n1\n\nThis defines a blank plot for us to add to\n\n2\n\nWhen we loop through our values of Œ±, we zip it with a vector of colors so that we can plot each value of Œ± in a different color.\n\n3\n\nSimilarly, we can attach each value of Œ≤ to a different linestyle.\n\n4\n\nplot!(p, ...) will add more elements to the plot p that we defined above.\n\n5\n\nUsing the StatsPlots package, we can plot a distribution by passing it to plot! with no additional arguments."
  },
  {
    "objectID": "labs/lab03/template.html#mathematical-model",
    "href": "labs/lab03/template.html#mathematical-model",
    "title": "Lab 03",
    "section": "",
    "text": "Our model for the loss fraction \\(y_i\\) for the \\(i\\)th house is \\[\ny_i | \\alpha, \\beta \\sim \\text{Beta}(\\alpha, \\beta)\n\\] where \\(\\alpha\\) and \\(\\beta\\) are the shape parameters of the Beta Distribution. Because we are not including any explanatory information in our model, we are assuming that the distribution of loss fractions is the same for all houses. This is reasonable for our neighborhood, but would not be applicable to another neighborhood or a different flood event.\nThe above notation is shorthand for \\[\np(y_i | \\alpha, \\beta) =\n\\frac{x^{\\alpha-1}(1-x)^{\\beta-1}} {\\mathrm{B}(\\alpha,\\beta)}\\!\n\\] where \\(\\mathrm {B} (\\alpha ,\\beta )={\\frac {\\Gamma (\\alpha )\\Gamma (\\beta )}{\\Gamma (\\alpha +\\beta )}}\\) and \\(\\Gamma\\) is the Gamma function. We will work directly with the pdf and logpdf functions in the Distributions package, so you don‚Äôt need to memorize this formula.."
  },
  {
    "objectID": "labs/lab03/template.html#setup",
    "href": "labs/lab03/template.html#setup",
    "title": "Lab 03",
    "section": "",
    "text": "As for labs 1 and 2, make sure you follow the three standard initial setup steps:\n\nOpen the lab 3 folder in VS Code. Do NOT open a ‚Äúparent‚Äù directory containing lab 3. If you‚Äôre not sure what folder you‚Äôre in, open the Juila REPL and type pwd(). It should say something like /.../lab03-username....\nActivate the project environment: ] to enter package mode then activate .. Don‚Äôt forget the . at the end, it‚Äôs very important.\nInstall all required packages: ] to enter package mode, then instantiate.\n\nAt this point, check to make sure you can render the document. In VS Code, open the command palette (Windows: Ctrl+Shift+P, Mac: Cmd+Shift+P) and type Render: HTML. If this gives you trouble, try the following:\n\n] in the Julia REPL to enter Package mode\nbuild IJulia\n\nIf this still gives you trouble, ask for help on Canvas or in-person."
  },
  {
    "objectID": "labs/lab03/template.html#package-imports",
    "href": "labs/lab03/template.html#package-imports",
    "title": "Lab 03",
    "section": "",
    "text": "As usual we start by specifying the packages we are using\n\nusing Distributions # probability distributions\nusing DelimitedFiles # read data\nusing LaTeXStrings # LaTeX plot labels\nusing Optim # optimization\nusing Plots # plotting\nusing StatsPlots # plot distributions\n\n[ Info: Precompiling StatsPlots [f3b207a7-027a-5e70-b257-86293d7955fd]"
  },
  {
    "objectID": "labs/lab03/template.html#about-the-beta-distribution",
    "href": "labs/lab03/template.html#about-the-beta-distribution",
    "title": "Lab 03",
    "section": "",
    "text": "We can learn something about the Beta distribution defined above by plotting it for some different values:\n\n1p = plot(; xlabel=L\"y\", ylabel=L\"p(y | \\alpha, \\beta)\", legend=:top)\n2for (Œ±, color) in zip([1.0, 5.0, 25.0], [:orange, :purple, :black])\n3    for (Œ≤, linestyle) in zip([1.0, 5.0, 25.0], [:solid, :dash, :dot])\n4        plot!(\n            p,\n5            Beta(Œ±, Œ≤);\n            label=\"Œ± = $Œ±, Œ≤ = $Œ≤\",\n            color=color,\n            linestyle=linestyle,\n            linewidth=2\n        )\n    end\nend\np\n\n\n1\n\nThis defines a blank plot for us to add to\n\n2\n\nWhen we loop through our values of Œ±, we zip it with a vector of colors so that we can plot each value of Œ± in a different color.\n\n3\n\nSimilarly, we can attach each value of Œ≤ to a different linestyle.\n\n4\n\nplot!(p, ...) will add more elements to the plot p that we defined above.\n\n5\n\nUsing the StatsPlots package, we can plot a distribution by passing it to plot! with no additional arguments."
  },
  {
    "objectID": "labs/lab03/template.html#likelihood-model",
    "href": "labs/lab03/template.html#likelihood-model",
    "title": "Lab 03",
    "section": "2.1 Likelihood model",
    "text": "2.1 Likelihood model\nfunction log_lik(y::Vector{T}, Œ±::T, Œ≤::T) where {T&lt;:Real}\n    # fill in here\n    # refer to lecture slides for syntax and function names\n    # don't forget your `return` statement!\nend\n\n\n\n\n\n\nFill in the log_lik function where y is a vector of data, and Œ± and Œ≤ are the parameters of the Beta distribution. Then, convert it to live code by adding curly brackets, like you did in lab 02. Hint: define a function function log_lik(y::T, Œ±::T, Œ≤::T) where {T&lt;:Real}... that takes in a single point. Then this function that takes in a vector y can calculate the log-likelihood for each data point individually and combine them. This is not the only way to solve this problem"
  },
  {
    "objectID": "labs/lab03/template.html#check-if-its-right",
    "href": "labs/lab03/template.html#check-if-its-right",
    "title": "Lab 03",
    "section": "2.2 Check if it‚Äôs right",
    "text": "2.2 Check if it‚Äôs right\nWe can check your log_lik function by comparing what you calculate using it to a known, correct value.\nyour_val = log_lik([0.2, 0.4, 0.6, 0.8], 5., 5.) # calls your implementation\ntrue_value = -0.2947032775653282 # I calculated this\n@assert isapprox(your_val, true_value) # checks if they're close\nprintln(\"üòÅ\")\n\nConvert this to a live code block by adding curly brackets, like you did in lab 02, and run. You should see a üòÑ."
  },
  {
    "objectID": "labs/lab03/template.html#plot",
    "href": "labs/lab03/template.html#plot",
    "title": "Lab 03",
    "section": "2.3 Plot",
    "text": "2.3 Plot\nNow that we have the log_likelihood, we‚Äôre going to plot it for many values of \\(\\alpha\\) and \\(\\beta\\).\nFirst, let‚Äôs define a grid of values for \\(\\alpha\\) and \\(\\beta\\) to plot:\n\nŒ±_plot = range(0.01, 50; length=500)\nŒ≤_plot = range(0.01, 50; length=501)\n\n\nDefine them to be different lengths so that if we make an indexing mistake, we should get an error.\n\nNext, we can calculate the log likelihood at each point on the grid of Œ±_plot and Œ≤_plot as:\nlog_lik_fax = [log_lik(fax, Œ±i, Œ≤i) for Œ±i in Œ±_plot, Œ≤i in Œ≤_plot]\n\n# ensure that the dimensions are correct\n@assert size(log_lik_fax) == (length(Œ±_plot), length(Œ≤_plot))\ndisplay(size(log_lik_fax))\nNow, we‚Äôre ready to plot. You can use the following code, which provides some helpful keyword arguments to make your plot look nice. Feel free to play around with them.\np1 = plot(\n    Œ±_plot,\n    Œ≤_plot,\n    log_lik_fax';\n    st=:heatmap,\n    xlabel=L\"$\\alpha$\",\n    ylabel=L\"$\\beta$\",\n    legend=:topright,\n    colorbar_title=L\"$\\log p(y | \\alpha, \\beta)$\"\n)\np1\n\n\n\n\n\n\nSize and plotting\n\n\n\nNote that we plot log_lik_fax' instead of log_lik_fax. That is equivalent to transpose(log_lik_fax). This is a quirk of plots.jl syntax (see here). We avoid confusion by checking the size of log_lik_fax above.\n\n\n\n\n\n\n\n\nNote that we have assigned our plot a variable name, p1. This will let us add elements to it later.\n\n\n\n\n\n\n\n\n\nAdd curly brackets to the code blocks in this section once your log_lik function is working."
  },
  {
    "objectID": "labs/lab03/template.html#best-estimates",
    "href": "labs/lab03/template.html#best-estimates",
    "title": "Lab 03",
    "section": "2.4 Best estimates",
    "text": "2.4 Best estimates\nWhat values of Œ±_plot and Œ≤_plot that maximize the log likelihood? We can find out in several different ways.\n\n2.4.1 Optimization on a grid\nThe easiest thing to do is to find the maximum value of the log likelihood on our grid. We can do that as follows:\nidx_fax = argmax(log_lik_fax) # the index that maximizes the log likelihood\nŒ±_fax_best = Œ±_plot[idx_fax[1]]\nŒ≤_fax_best = Œ≤_plot[idx_fax[2]]\n\n\n\n\n\n\nAdd curly brackets to the code blocks in this section once your log_lik function is working.\n\n\n\n\n\n2.4.2 Optim.jl\nWe can also use the optimize function from Optim.jl to find the maximum value of the log likelihood.\n# define the function to be optimized\nloss_fax(Œ∏) = # define the negative log-likelihood here, using your `log_lik` function and passing in `fax` as the `y` argument.\nlower = [0.001, 0.001] # lower bound -- don't need to change\nupper = [Inf, Inf] # upper bound -- don't need to edit\nguess = [1.0, 1.0] # initial guess -- leave as-is\n\nres = optimize(loss_fax, lower, upper, guess) # run the optimization\nŒ∏_fax = Optim.minimizer(res) # get the best parameters\n\n\n\n\n\n\nAdd curly brackets to the code blocks in this section once you have implemented everything. Follow the commented instructions.\n\n\n\n\n\n2.4.3 Distributions.jl\nAs an alternative to Optim.jl, we can use the fit_mle function from Distributions.jl for most distributions. We can use this to check our work:\nDistributions.fit_mle(Beta, fax)\n\n\n\n\n\n\nAdd curly brackets to the code blocks in this section once you have implemented everything above. Compare the fit_mle result to your result using optimize."
  },
  {
    "objectID": "labs/lab03/template.html#update-the-plot",
    "href": "labs/lab03/template.html#update-the-plot",
    "title": "Lab 03",
    "section": "2.5 Update the plot",
    "text": "2.5 Update the plot\nThese best estimates should show up as points on our plot. We can add them as follows:\nscatter!(p1, [Œ±_fax_best], [Œ≤_fax_best]; label=\"Grid Search\")\nscatter!(p1, [Œ∏_fax[1]], [Œ∏_fax[2]]; label=\"MLE\")\np1\n\n\nWhen we add a single point, we have to wrap it in brackets [] to make it a vector. The x and y inputs to plot (or scatter!) need to be vectors.\n\n\n\n\n\n\nAdd curly brackets to the code blocks in this section once you have implemented everything above."
  },
  {
    "objectID": "labs/lab04/template.html",
    "href": "labs/lab04/template.html",
    "title": "Lab 04",
    "section": "",
    "text": "Use linear regression to relate the mean annual runoff of several streams (runoff) with nine other variables:\n\nthe precipitation falling at the gage (precip),\nthe drainage area of the basin (area),\nthe average slope of the basin (slope)\nthe length of the drainage basin (length)\nthe perimeter of the basin (perim)\nthe diameter of the largest circle which could be inscribed within the drainage basin (diameter)\nthe shape factor of the basin (shape_factor)\nthe stream frequency ‚Äì the ratio of the number of streams in the basin to the basin area (stream_freq)\nthe relief ratio for the basin (relief_ratio)\n\n\n\n\n\n\n\nCredit\n\n\n\nThis is a light modification of Example 11.3 from\nHelsel, D. R., Hirsch, R. M., Ryberg, K. R., Archfield, S. A., & Gilroy, E. J. (2020). Statistical methods in water resources. Techniques and Methods. U.S. Geological Survey. https://doi.org/10.3133/tm4A3"
  },
  {
    "objectID": "labs/lab04/template.html#data",
    "href": "labs/lab04/template.html#data",
    "title": "Lab 04",
    "section": "2.1 Data",
    "text": "2.1 Data\nNext, we need to load in the data.\n\nrunoff_df = CSV.read(\"data/runoff.csv\", DataFrame)\ndisplay(size(runoff_df))\nfirst(runoff_df, 3)\n\n(13, 10)\n\n\n3√ó10 DataFrame\n\n\n\nRow\nrunoff\nprecip\narea\nslope\nlength\nperim\ndiameter\nshape_factor\nstream_freq\nrelief_ratio\n\n\n\nFloat64\nFloat64\nFloat64\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nInt64\n\n\n\n\n1\n17.38\n44.37\n2.21\n50\n2.38\n7.93\n0.91\n0.38\n1.36\n332\n\n\n2\n14.62\n44.09\n2.53\n7\n2.55\n7.65\n1.23\n0.48\n2.37\n55\n\n\n3\n15.48\n41.25\n5.63\n19\n3.11\n11.61\n2.11\n0.57\n2.31\n77"
  },
  {
    "objectID": "labs/lab04/template.html#model-specification",
    "href": "labs/lab04/template.html#model-specification",
    "title": "Lab 04",
    "section": "3.1 Model specification",
    "text": "3.1 Model specification\nWe are going to fit models of the form \\[\ny_i \\sim \\mathcal{N} \\left( \\beta X_i , \\sigma \\right).\n\\] We only have 13 observations of 10 variables, so we‚Äôll need to be careful with our data. Before diving into the model, let‚Äôs look at the data."
  },
  {
    "objectID": "labs/lab04/template.html#pairwise-correlations",
    "href": "labs/lab04/template.html#pairwise-correlations",
    "title": "Lab 04",
    "section": "3.2 Pairwise correlations",
    "text": "3.2 Pairwise correlations\nOne simple way to look at the data is to look at the pairwise correlations between the variables. The following function plot_correlation will plot a heatmap of the pairwise correlations.\n\n\nCode\nfunction plot_correlation(df::AbstractDataFrame)\n    varnames = names(df)\n1    correlations = cor(Matrix(df))\n    P = size(correlations, 1)\n\n    p = heatmap(\n        varnames, # x labels\n        varnames, # y labels\n        correlations; # variable\n        aspect_ratio=1, # 1:1 aspect ratio\n        xrotation=30, # rotate x ticks\n        size=(600, 600), # specify the size of the plot\n        title=\"Pairwise Correlations\",\n        clims=(-1, 1),\n        cmap=:PuOr, # select a diverging color map\n    )\n\n    # add the exact text\n    ann = [\n        (i - 0.5, j - 0.5, text(round(correlations[i, j]; digits=2), 8, :white, :center))\n        for i in 1:P for j in 1:P\n    ]\n    annotate!(p, ann; linecolor=:white)\n    return p\nend\n\n\n\n1\n\ncor expects a Matrix input and will throw an error if we give it a DataFrame. Matrix(df) just extracts numeric values for our DataFrame. This won‚Äôt work well if we have non-numeric columns, but we don‚Äôt here.\n\n\n\n\n\nplot_correlation(runoff_df)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nDescribe what you learn from this plot in the specific context of modeling the relationship between runoff and the other variables. What can you not learn from this plot?"
  },
  {
    "objectID": "labs/lab04/template.html#define-model",
    "href": "labs/lab04/template.html#define-model",
    "title": "Lab 04",
    "section": "4.1 Define model",
    "text": "4.1 Define model\nWe can implement this in Turing as follows:\n#| output: false\n@model function linear_reg(X::AbstractVector, y::AbstractVector)\n\n    # First, define all parameters\n    # they need priors so we use a ~\n    Œ± # the intercept\n    Œ≤ # the slope\n    œÉ # the standard deviation\n\n    # We can calculate intermediate quantities in our function\n    # Turing knows this isn't a parameter because we use = not ~\n    Œº = Œ± .+ Œ≤ .* X\n\n    # likelihood model\n    y ...\nend\n::: {.callout-important} ## Task\nImplement the model above so that it is consistent with the mathematical model written above. You will iterate between this and the next step a few times. You only need to keep your final model, but briefly describe what you did and why. :::## Prior predictive check\nBefore we fit the model, we run a prior predictive check. This lets us check whether the prior assumptions we have encoded into the model are consisten with our beliefs. First we sample from the prior:\ny = vec(runoff_df[!, :runoff])\nX = vec(runoff_df[!, :YOUR_VAR_HERE]) # which variable did you choose?\nppc_linear = let\n    model = linear_reg(X, y) # call the model\n    sampler = Prior() # we want to sample from the prior\n    nsamples = 10_000\n    sample(model, sampler, nsamples; drop_warmup=true)\nend\nWe can use the lm_predict function to generate predictions from the prior.\n\n\nCode\nfunction lm_predict(chn::Turing.MCMCChains.Chains, X::AbstractVecOrMat, N::Int)\n1    p = get_params(chn)\n2    Œ± = vec(p[:Œ±])\n3    Œ≤ = if isa(p[:Œ≤], Tuple)\n        Œ≤_matrix = hcat(vec.(p[:Œ≤])...)\n        [Œ≤_matrix[i, :] for i in 1:size(Œ≤_matrix, 1)]\n    else\n        vec(p[:Œ≤])\n    end\n4    idx = rand(1:length(Œ±), N)\n5    ≈∑ = [\n        Œ±[i] .+ X * Œ≤[i] for i in idx\n    ]\n6    if N == 1\n        ≈∑ = ≈∑[1]\n    end\n    return ≈∑\nend\n\n\n\n1\n\nThis function extracts the parameters from the chain\n\n2\n\nIf we use multiple chains, then p[:Œ±] is a Matrix. We convert it to a vector so that it‚Äôs ‚Äúflat‚Äù.\n\n3\n\nIf we have multiple predictors (multivariate linear regression) then p[:Œ≤] is a tuple of vectors. We convert it into a vector of vectors so that Œ≤[i] is a vector of coefficients for the \\(i\\)th sample.\n\n4\n\nWe randomly select N samples from the posterior.\n\n5\n\nWe calculate the predictions for each of the N samples.\n\n6\n\nIf N is 1, then we return a vector, not a vector of vectors.\n\n\n\n\nWe leverage this function to generate predictions from the prior and plot them.\nX_pred = ... # specify the range of values for\np1 = plot(xlabel=\"Precip\", ylabel=\"Runoff\", title=\"Prior predictive check\")\nfor i in 1:250\n    y_pred = lm_predict(ppc_linear, X_pred, 1)\n    plot!(p1, X_pred, y_pred; color=:black, alpha=0.1, label=false)\nend\np1\n\n\n\n\n\n\nTask\n\n\n\nFill in the missing lines in the above blocks and run."
  },
  {
    "objectID": "labs/lab04/template.html#inference",
    "href": "labs/lab04/template.html#inference",
    "title": "Lab 04",
    "section": "4.2 Inference",
    "text": "4.2 Inference\nOnce we have a prior that we‚Äôre happy with, we can use our model to generate samples from the posterior distribution. We draw 4 chains of 5000 samples. If you get a weird error here, it may be because of threading. Try the mapreduce based approach here rather than waste time debugging.\nchn_linear = let\n    model = ... # FILL IN\n    sampler = externalsampler(DynamicHMC.NUTS())\n    n_per_chain = 5000\n    nchains = 4\n    sample(model, sampler, MCMCThreads(), n_per_chain, nchains; drop_warmup=true)\nend\nsummarystats(chn_linear)\nBased on \\(\\hat{R}\\) values close to 1, we have some evidence that our chains have converged. We can also check the trace plots:\n... # Fill in to generate the trace plot\n\n\n\n\n\n\nTask\n\n\n\nFill in the missing lines in the above blocks. Describe what you learn from the trace plot."
  },
  {
    "objectID": "labs/lab04/template.html#posterior-predictive-checks",
    "href": "labs/lab04/template.html#posterior-predictive-checks",
    "title": "Lab 04",
    "section": "4.3 Posterior predictive checks",
    "text": "4.3 Posterior predictive checks\nWe can again plot our predictions, this time using the posterior samples. We add the data to the plot as well.\np2 = plot(xlabel=\"Precip\", ylabel=\"Runoff\", title=\"Posterior\") # blank plot\npred_linear = lm_predict(chn_linear, X_pred, 500) # generate predictions\nfor y_pred in pred_linear\n    plot!(p2, X_pred, y_pred; color=:black, alpha=0.1, label=false) # add each prediction to a plot\nend\n# add the observations with scatter!\np2\n\n\n\n\n\n\nTask\n\n\n\nAdd the observations to the plot above with scatter!"
  },
  {
    "objectID": "labs/lab04/template.html#prior-predictive-check",
    "href": "labs/lab04/template.html#prior-predictive-check",
    "title": "Lab 04",
    "section": "5.1 Prior predictive check",
    "text": "5.1 Prior predictive check\nX = Matrix(runoff_df[:, Not(:runoff)]) # all columns except runoff\ny = vec(runoff_df[!, :runoff]) # runoff \n\nppc_multi = let\n    ... # FILL IN\nend\nHow should we visualize the prior predictive check? A simple way is to visualize the implied distribution of \\(y\\) given observed \\(X\\):\nhistogram(\n    vcat(lm_predict(ppc_multi, X, 500)...),\n    xlabel=L\"$y$\",\n    ylabel=L\"Implied $p(y)$\",\n    label=false,\n    normalize=:pdf,\n    yticks=false,\n    title=\"Prior predictive check\",\n)\n\n\n\n\n\n\nTask\n\n\n\nModify the above code to draw samples from the prior and save as ppc_multi."
  },
  {
    "objectID": "labs/lab04/template.html#inference-1",
    "href": "labs/lab04/template.html#inference-1",
    "title": "Lab 04",
    "section": "5.2 Inference",
    "text": "5.2 Inference\nNow that our priors seem plausible (we could a;ways do better, but that‚Äôs not the point of this lab), we can fit the model.julia chn_multi = let     ... end # summary stats\n\n\n\n\n\n\nTask\n\n\n\nDraw the samples from the posterior and save as chn_multi. Display the summary statistics. What do you learn?"
  },
  {
    "objectID": "labs/lab04/template.html#posterior-predictive-check",
    "href": "labs/lab04/template.html#posterior-predictive-check",
    "title": "Lab 04",
    "section": "5.3 Posterior predictive check",
    "text": "5.3 Posterior predictive check\nHow should we visualize these inferences? It‚Äôs hard to plot a single \\(x\\) vs \\(y\\) when we have multiple \\(x\\). There are many possible plots, but a good one is to plot the predicted vs actual values of \\(y\\). To make the plot more readable, we‚Äôll add some jitter to the data. Here‚Äôs a function to do that:\n\n\nCode\nfunction jitter(x, y; œµx=0.1, œµy=0.1)\n    x_jit = x .+ rand(Normal(0, œµx), size(x))\n    y_jit = y .+ rand(Normal(0, œµy), size(y))\n    return x_jit, y_jit\nend\n\n\nNow we can create our scatter plot.\np4 = plot(; xlabel=\"Predicted Runoff\", ylabel=\"Actual Runoff\", aspect_ratio=1) # blank plot\nfor i in 1:500 # loop through\n    pred = lm_predict(chn_multi, X, 1)\n    xplt, yplt = jitter(pred, y; œµx=0.1, œµy=0.1)\n    scatter!(xplt, yplt; color=:black, alpha=0.5, label=false, markersize=0.5)\nend\nPlots.abline!(p4, 1, 0; color=:red, label=\"1:1 line\")\np4\n\n\n\n\n\n\nTask\n\n\n\nThe above block shouldn‚Äôt need to be modified. Add the curly brackets when you are ready. What do you learn from this plot?"
  },
  {
    "objectID": "labs/lab02/template.html",
    "href": "labs/lab02/template.html",
    "title": "Lab 02",
    "section": "",
    "text": "In this lab we will learn how to work with tabular data in Julia. Specifically, you will get some experience using:\n\nDataFrames.jl to store tabular data as a DataFrame\nCSV.jl to read CSV files and convert them to DataFrames\nDataFramesMeta.jl to manipulate DataFrames\nPlots.jl and StatsPlots.jl to create visualizations\n\n\n\nTechnically, what we are doing today is called exploratory modeling or exploratory data analysis. The latter is more common, but the former makes clear that all visualizations include some (usually implicit) conceptual model of the data. You will find more resources if you search for ‚Äúexploratory data analysis‚Äù, however.\n\n\n\nI have provided you with a lot of template code.\nAny code block that starts with\n\n\n```{julia}\n\n\nis a ‚Äúlive‚Äù code block and will execute (run).\n\nThese blocks will automatically run when you render the document\nYou can click ‚ÄúRun Cell‚Äù to run the cell\nMake sure you understand what the code does and ask questions (in-person or on Canvas) if you don‚Äôt\nDo not change these blocks unless instructed\n\nAny code block that starts with\n\n\n```julia\n\n\nis source code and will not run.\n\nYou should change these blocks to complete the lab, filling in blanks and adding code as needed.\nTo make it a cell that you can run, add the brackets\n\nChanges you are expected to make are marked as follows:\n\n\n\n\n\n\nThis is an instruction.\n\n\n\n\n\n\nYou may find the following Quarto documentation pages helpful:\n\nUsing Julia\nMarkdown Basic\n\nAsk questions in class or post them on the Lab02 discussion on Canvas. The sections of this lab are numbered, so refer to the number when asking questions on Canvas to make it easier for us to help you."
  },
  {
    "objectID": "labs/lab02/template.html#exploratory-modeling",
    "href": "labs/lab02/template.html#exploratory-modeling",
    "title": "Lab 02",
    "section": "",
    "text": "Technically, what we are doing today is called exploratory modeling or exploratory data analysis. The latter is more common, but the former makes clear that all visualizations include some (usually implicit) conceptual model of the data. You will find more resources if you search for ‚Äúexploratory data analysis‚Äù, however."
  },
  {
    "objectID": "labs/lab02/template.html#how-to-use-this-template",
    "href": "labs/lab02/template.html#how-to-use-this-template",
    "title": "Lab 02",
    "section": "",
    "text": "I have provided you with a lot of template code.\nAny code block that starts with\n\n\n```{julia}\n\n\nis a ‚Äúlive‚Äù code block and will execute (run).\n\nThese blocks will automatically run when you render the document\nYou can click ‚ÄúRun Cell‚Äù to run the cell\nMake sure you understand what the code does and ask questions (in-person or on Canvas) if you don‚Äôt\nDo not change these blocks unless instructed\n\nAny code block that starts with\n\n\n```julia\n\n\nis source code and will not run.\n\nYou should change these blocks to complete the lab, filling in blanks and adding code as needed.\nTo make it a cell that you can run, add the brackets\n\nChanges you are expected to make are marked as follows:\n\n\n\n\n\n\nThis is an instruction."
  },
  {
    "objectID": "labs/lab02/template.html#getting-help",
    "href": "labs/lab02/template.html#getting-help",
    "title": "Lab 02",
    "section": "",
    "text": "You may find the following Quarto documentation pages helpful:\n\nUsing Julia\nMarkdown Basic\n\nAsk questions in class or post them on the Lab02 discussion on Canvas. The sections of this lab are numbered, so refer to the number when asking questions on Canvas to make it easier for us to help you."
  },
  {
    "objectID": "labs/lab02/template.html#clone-the-respository",
    "href": "labs/lab02/template.html#clone-the-respository",
    "title": "Lab 02",
    "section": "2.1 Clone the respository",
    "text": "2.1 Clone the respository\nFirst, you‚Äôll need to clone this repository to your computer. As with Lab 01, I recommend to use GitHub Desktop or the built-in Git support in VS Code. Remember to use the link from Canvas (classroom.github.com/...)."
  },
  {
    "objectID": "labs/lab02/template.html#install-required-packages",
    "href": "labs/lab02/template.html#install-required-packages",
    "title": "Lab 02",
    "section": "2.2 Install required packages",
    "text": "2.2 Install required packages\n\nIn VS Code, open the command palette (Windows: Ctrl+Shift+P, Mac: Cmd+Shift+P) and select Julia: Start REPL.\nIn the Julia REPL, type ] to enter the package manager.\nType activate . to activate the project environment.\nType instantiate to install the required packages. This may take a moment.1"
  },
  {
    "objectID": "labs/lab02/template.html#using-statement",
    "href": "labs/lab02/template.html#using-statement",
    "title": "Lab 02",
    "section": "2.3 Using statement",
    "text": "2.3 Using statement\nIn Julia we say using to import a package. Typically we want to do this as early as possible in a script or notebook.\n\nusing CSV\nusing DataFrames\nusing DataFramesMeta\nusing Dates\nusing Plots\nusing StatsBase: mean\nusing StatsPlots\nusing Unitful\n\n\n\nThat this is a code block code and will run. If you have not yet installed the packages, you will see an error message. Don‚Äôt change this block ‚Äì see instructions to install and instantiate."
  },
  {
    "objectID": "labs/lab02/template.html#check",
    "href": "labs/lab02/template.html#check",
    "title": "Lab 02",
    "section": "2.4 Check",
    "text": "2.4 Check\nTo make sure everything is working, you should open the command palette and select Quarto: Render HTML. This will generate a HTML file from this notebook. This is a good way to check that everything is working before you start editing code."
  },
  {
    "objectID": "labs/lab02/template.html#dates",
    "href": "labs/lab02/template.html#dates",
    "title": "Lab 02",
    "section": "3.1 Dates",
    "text": "3.1 Dates\nWe can see that our DataFrame has five columns, the first of which is ‚ÄúDate Time‚Äù. However, the ‚ÄúDate Time‚Äù column is being parsed as a string. We want it to be a DateTime object from the Dates package. To do that, we need to tell Julia how the dates are formatted. We could then manually convert, but CSV.read has a kewyord argument that we can use\n\n1date_format = \"yyyy-mm-dd HH:MM\"\n2df = CSV.read(fname, DataFrame; dateformat=date_format)\nfirst(df, 3)\n\n\n1\n\nThis is a string that tells Julia how the dates are formatted. For example, 1928-01-01 00:00. See the documentation for more information.\n\n2\n\ndateformat is a keyword argument while date_format is a variable whose value is \"yyyy-mm-dd HH:MM\". We could equivalently write dateformat=\"yyyy-mm-dd HH:MM\".\n\n\n\n\n3√ó5 DataFrame\n\n\n\nRow\nDate Time\nWater Level\nSigma\nI\nL\n\n\n\nDateTime\nFloat64\nFloat64\nInt64\nInt64\n\n\n\n\n1\n1928-01-01T00:00:00\n-0.547\n0.0\n0\n0\n\n\n2\n1928-01-01T01:00:00\n-0.699\n0.0\n0\n0\n\n\n3\n1928-01-01T02:00:00\n-0.73\n0.0\n0\n0"
  },
  {
    "objectID": "labs/lab02/template.html#water-levels",
    "href": "labs/lab02/template.html#water-levels",
    "title": "Lab 02",
    "section": "3.2 Water levels",
    "text": "3.2 Water levels\nThe next column is ‚ÄúWater Level‚Äù, which is the height of the water above the reference point (NAVD) in meters. We can see that this is being parsed as a float, which is what we want üëç. However, you have to know that the data is in meters rather than inches or feet or something else. To explicitly add information about the units, we can use the Unitful package.\n\ndf[!, \" Water Level\"] .*= 1u\"m\"\nfirst(df, 3)\n\n3√ó5 DataFrame\n\n\n\nRow\nDate Time\nWater Level\nSigma\nI\nL\n\n\n\nDateTime\nQuantity‚Ä¶\nFloat64\nInt64\nInt64\n\n\n\n\n1\n1928-01-01T00:00:00\n-0.547 m\n0.0\n0\n0\n\n\n2\n1928-01-01T01:00:00\n-0.699 m\n0.0\n0\n0\n\n\n3\n1928-01-01T02:00:00\n-0.73 m\n0.0\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n\nCode explanation\n\n\n\n\nWe select the column with water levels using its name. The ! means ‚Äúall rows‚Äù. Thus, df[!, \" Water Level\"] is a vector of all the water levels stored.\n*= means to multiply in place. For example, if x=2 then x *= 2 is equivalent to x = x * 2. .*= is a vector syntax, meaning do the multiplication to each element of the vector individually.\n1u\"m\" is a Unitful object that represents 1 meter. We multiply the water levels by this to convert them to meters."
  },
  {
    "objectID": "labs/lab02/template.html#subsetting-and-renaming",
    "href": "labs/lab02/template.html#subsetting-and-renaming",
    "title": "Lab 02",
    "section": "3.3 Subsetting and renaming",
    "text": "3.3 Subsetting and renaming\nWe want to only keep the first two (for more on the other three, see here). We can also rename the columns to make them easier to work with (spaces in variable names are annoying). To do this, we use the @rename function:\n\n1df = @rename(df, :datetime = $\"Date Time\", :lsl = $\" Water Level\");\n\n\n1\n\nThe $ is needed here because the right hand side is a string, not a symbol.\n\n\n\n\nThen, we can use the @select function to do select the columns we want. Notice how the first argument to select is the DataFrame and the subsequent arguments are column names. Notice also that our column names were strings (\"Date Time\"), but we can also use symbols (:datetime).\n\ndf = @select(df, :datetime, :lsl)\nfirst(df, 3)\n\n3√ó2 DataFrame\n\n\n\nRow\ndatetime\nlsl\n\n\n\nDateTime\nQuantity‚Ä¶\n\n\n\n\n1\n1928-01-01T00:00:00\n-0.547 m\n\n\n2\n1928-01-01T01:00:00\n-0.699 m\n\n\n3\n1928-01-01T02:00:00\n-0.73 m\n\n\n\n\n\n\nFor more on what DataFramesMeta can do, see this Tweet."
  },
  {
    "objectID": "labs/lab02/template.html#writing-a-function",
    "href": "labs/lab02/template.html#writing-a-function",
    "title": "Lab 02",
    "section": "3.4 Writing a function",
    "text": "3.4 Writing a function\nWe have just done a lot of work to read in our data. However, this just gives us data for the year 1928. In fact, we have a CSV file for each year 1928-2021. To make sure we can read them each in excatly the same way, we want to write a function.\nfunction read_tides(year::Int)\n    fname = \"data/tidesandcurrents-8638610-$(year)-NAVD-GMT-metric.csv\" # don't change this\n    date_format = \"yyyy-mm-dd HH:MM\" # don't change this\n    # your code here\n    # 1. read in the CSV file and save as a dataframe\n    # 2. convert the \"Date Time\" column to a DateTime object\n    # 3. convert the \" Water Level\" column to meters\n    # 4. rename the columns to \"datetime\" and \"lsl\"\n    # 5. select the \"datetime\" and \"lsl\" columns\n    # 6. return the dataframe\nend\n\n# print out the first 10 rows of the 1928 data\nfirst(read_tides(1928), 10) \n\n\n\n\n\n\nInstructions\n\n\n\nFill out this function. Your function should implement the six steps indicated in the instructions. When it‚Äôs done, convert it to a live code block with: ```{julia}. When you run this code, it should print out the first 10 rows of the 1928 data. Make sure they look right!"
  },
  {
    "objectID": "labs/lab02/template.html#combining-files",
    "href": "labs/lab02/template.html#combining-files",
    "title": "Lab 02",
    "section": "3.5 Combining files",
    "text": "3.5 Combining files\nNow that we have the ability to read in the data corresponding to any year, we can read them all in and combine into a single DataFrame. First, let‚Äôs read in all the data.\nyears = 1928:2021 # all the years of data\nannual_data = # 1. call the read_tides function on each year\ntypeof(annual_data) # should be a vector of DataFrames\nNext, we‚Äôll use the vcat function to combine all the data into a single DataFrame.\ndf = vcat(annual_data...) # don't change this\nfirst(df, 5)\nlast(df, 5) # check the last 5 years\n\n\n\n\n\n\nInstructions\n\n\n\n\nCall the read_tides function on each year\nTurn the two code blocks in this section into live code blocks\nRun the code and make sure the first 5 rows and last five rows look right"
  },
  {
    "objectID": "labs/lab02/template.html#time-series-plot",
    "href": "labs/lab02/template.html#time-series-plot",
    "title": "Lab 02",
    "section": "4.1 Time series plot",
    "text": "4.1 Time series plot\nLet‚Äôs start with a simple time series plot of the water levels. Our data is collected hourly, so we have a lot of data points! Still, we can plot them all.\n\nplot(\n    df.datetime,\n    df.lsl;\n    title=\"Water levels at Sewells Point, VA\",\n1    ylabel=\"Water level\",\n2    label=false,\n)\n\n\n1\n\nBecause we are using the Unitful pacakge, the y-axis label will automatically include the units!!!\n\n2\n\nWe are only plotting one ‚Äúseries‚Äù (data set), so we don‚Äôt need a legend."
  },
  {
    "objectID": "labs/lab02/template.html#zooming-in",
    "href": "labs/lab02/template.html#zooming-in",
    "title": "Lab 02",
    "section": "4.2 Zooming in",
    "text": "4.2 Zooming in\nFocusing on the entire time series means we can‚Äôt dig into the details. Let‚Äôs zoom in on a single month (October 1928) using the @subset function.\n\nt_start = Dates.DateTime(1928, 10, 1, 0)\nt_end = Dates.DateTime(1928, 10, 31, 23)\ndf_month = @subset(df, t_start .&lt;= :datetime .&lt;= t_end)\nfirst(df_month, 3)\n\n3√ó2 DataFrame\n\n\n\nRow\ndatetime\nlsl\n\n\n\nDateTime\nQuantity‚Ä¶\n\n\n\n\n1\n1928-10-01T00:00:00\n0.215 m\n\n\n2\n1928-10-01T01:00:00\n0.429 m\n\n\n3\n1928-10-01T02:00:00\n0.581 m\n\n\n\n\n\n\nNow we can plot it as above.\n\nplot(\n    df_month.datetime,\n    df_month.lsl;\n    title=\"Water levels at Sewells Point, VA\",\n    ylabel=\"Water level\",\n    label=false,\n)"
  },
  {
    "objectID": "labs/lab02/template.html#instructions-2",
    "href": "labs/lab02/template.html#instructions-2",
    "title": "Lab 02",
    "section": "4.3 Instructions",
    "text": "4.3 Instructions\nChange the start and end dates to plot March 2020. What do you notice? :::"
  },
  {
    "objectID": "labs/lab02/template.html#groupby",
    "href": "labs/lab02/template.html#groupby",
    "title": "Lab 02",
    "section": "4.4 Groupby",
    "text": "4.4 Groupby\nAn essential idea in working with tabular data (and other data formats) is ‚Äúsplit-apply-combine‚Äù. Essentially: split the data into groups, apply some function to each group, and then combine the results.\nWe can use this workflow to answer an interesting question: what is the average water level for each month?2\n\n1df[!, :month] = Dates.month.(df.datetime)\n2dropmissing!(df, :lsl)\n3df_bymonth = groupby(df, :month)\n4df_climatology = combine(df_bymonth, :lsl =&gt; mean =&gt; :lsl_avg);\n\n\n1\n\nThis creates a new column called :month that is the month of each observation.\n\n2\n\nThis will discard any rows in df that have a missing value of :lsl. This is necessary because the mean function will return missing if any of the values are missing.\n\n3\n\nThis creates a GroupedDataFrame object that contains all the data grouped by month.\n\n4\n\nThis takes the grouped data and calculates the mean of the :lsl column for each month. The general syntax is combine(grouped_df, :column =&gt; function).\n\n\n\n\nWe can now plot the climatology.\n\nplot(\n1    df_climatology.month,\n    df_climatology.lsl_avg;\n2    xticks=1:12,\n    xlabel=\"Month\",\n    ylabel=\"Average Water level\",\n3    linewidth=3,\n    label=false,\n)\n\n\n1\n\nWe can use df.colname instead of df[!, :colname]. The latter is more robust but the former is easier to type.\n\n2\n\nSetting xticks will set the x-axis ticks to the values in the vector. We can use this to make sure the x-axis ticks are labeled with the months.\n\n3\n\nWe can set the line width to make the plot easier to read.\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstructions\n\n\n\n\nUse the full dataset to plot the climatology using data from all years\nNext, create a new Markdown header (## Groupby Day of Year) plot the average water level for each day of the year (Dates.dayofyear from 1 to 366).\nWhat do you notice?"
  },
  {
    "objectID": "labs/lab02/template.html#boxplot",
    "href": "labs/lab02/template.html#boxplot",
    "title": "Lab 02",
    "section": "4.5 Boxplot",
    "text": "4.5 Boxplot\nBoxplots are ways to visualize the distribution of data. They show the median (the line in the middle of the box), the interquartile range (the box), and the range of the data (the whiskers). Outliers are shown as dots. We can use the boxplot function from the StatsPlots.jl package:\n\nboxplot(\n1    df[!, :month],\n    df[!, :lsl];\n    xticks=1:12,\n    xlabel=\"Month\",\n    ylabel=\"Water level\",\n    label=false,\n2    title=\"Climatology\",\n)\n\n\n1\n\nWe are back to df[!, :colname] syntax. Both work!\n\n2\n\nWe can set the title using the title keyword argument.\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstructions\n\n\n\nRepeat this analysis grouping by year rather than month. What do you notice from the boxplots?"
  },
  {
    "objectID": "labs/lab02/template.html#submission",
    "href": "labs/lab02/template.html#submission",
    "title": "Lab 02",
    "section": "4.6 Submission",
    "text": "4.6 Submission\n\nRemove all of the instructions blocks (from the first ::: {.callout-important} to the following :::)\nRemove all of the text I have written (including this block) so that all the text is your own. This makes it much easier to grade and to follow what is yours.\n\nDo not delete any of the headers\n\nThus, the beginning of the document should have the headers for the overview, exploratory modeling, instructions, and getting help, but these should not have any text\nThis will ensure all documents have the same numbering to make it easier to troubleshoot any issues\n\nDo not delete any of the code\n\nMake sure your code runs (click the ‚ÄúRun All‚Äù button in the command palette)\nRender your notebook as HTML (optional)\n\nOpen comand palette\nSelect Quarto: Render HTML\nThis will create a website that you can view in your browser. The address will be in your terminal like\n\nWatching files for changes\nBrowse at http://localhost:4200/labs/lab02/instructions.html\n\nMake sure your notebook looks right in the browser.\n\nRender your hnotebook as DOCX (required)\n\nOpen comand palette\nSelect Quarto: Render DOCX\nIt may give you a link to click on in order to download the file. Click it and it will be downloaded to your computer (probably in your Downloads folder)\n\nSubmit the .docx file to Canvas.\n\nProofread before you submit!"
  },
  {
    "objectID": "labs/lab02/template.html#footnotes",
    "href": "labs/lab02/template.html#footnotes",
    "title": "Lab 02",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nJulia precompiles packages when they are installed, and (to a lesser extent) when they are first used. The first time you use a package it may take a moment to load. This is normal, nothing to worry about, and rapidly improving.‚Ü©Ô∏é\nTo do a better job, we should separate out the long-term trend from the seasonal cycle. This is called de-trending and is a common technique in climate science. We can worry more about this later.‚Ü©Ô∏é"
  },
  {
    "objectID": "labs/lab01/solution.html",
    "href": "labs/lab01/solution.html",
    "title": "Lab 01",
    "section": "",
    "text": "Do not modify this section!"
  },
  {
    "objectID": "labs/lab01/solution.html#setup",
    "href": "labs/lab01/solution.html#setup",
    "title": "Lab 01",
    "section": "",
    "text": "Do not modify this section!"
  },
  {
    "objectID": "labs/lab01/solution.html#running-code",
    "href": "labs/lab01/solution.html#running-code",
    "title": "Lab 01",
    "section": "Running Code",
    "text": "Running Code\nWe can use Quarto to run Julia code in-line\n\nprintln(\"I'm using Julia!\")\n\nI'm using Julia!"
  },
  {
    "objectID": "labs/lab01/solution.html#rendering-the-document",
    "href": "labs/lab01/solution.html#rendering-the-document",
    "title": "Lab 01",
    "section": "Rendering the Document",
    "text": "Rendering the Document\nNow, verify that you can render the document in HTML:\n\nOpen the command palette (Cmd+Shift+P on macOS, Ctrl+Shift+P on Windows/Linux)\nType ‚ÄúQuarto: Render Document‚Äù\nA web browser should open with the rendered document\n\nCheck the box (replace the space with an x) to confirm that this worked\n\nthis worked for me\nthis did not work for me (specify the error below)"
  },
  {
    "objectID": "resources/github.html",
    "href": "resources/github.html",
    "title": "GitHub Resources",
    "section": "",
    "text": "Git Basics from The Odin Project.\nLearn Git Branching: An interactive, visual tutorial to how git works.\nVersion Control from MIT‚Äôs ‚ÄúCS: Your Missing Semester‚Äù course.\nGit and GitHub for Poets: YouTube playlist covering the basics of git and GitHub."
  },
  {
    "objectID": "resources/github.html#helpful-resources",
    "href": "resources/github.html#helpful-resources",
    "title": "GitHub Resources",
    "section": "",
    "text": "Git Basics from The Odin Project.\nLearn Git Branching: An interactive, visual tutorial to how git works.\nVersion Control from MIT‚Äôs ‚ÄúCS: Your Missing Semester‚Äù course.\nGit and GitHub for Poets: YouTube playlist covering the basics of git and GitHub."
  },
  {
    "objectID": "resources/quarto.html",
    "href": "resources/quarto.html",
    "title": "Quarto",
    "section": "",
    "text": "In this tutorial, you will learn how to typeset mathematics and equations in Jupyter notebooks using LaTeX."
  },
  {
    "objectID": "resources/quarto.html#overview",
    "href": "resources/quarto.html#overview",
    "title": "Quarto",
    "section": "",
    "text": "In this tutorial, you will learn how to typeset mathematics and equations in Jupyter notebooks using LaTeX."
  },
  {
    "objectID": "resources/quarto.html#further-resourcesw",
    "href": "resources/quarto.html#further-resourcesw",
    "title": "Quarto",
    "section": "Further Resourcesw",
    "text": "Further Resourcesw\n\nMarkdown Cheatsheet\nLaTeX Cheatsheet\nMathpix Snpi allows you to convert images of equations to LaTeX code (there is a free tier)\nDetexify lets you draw a symbol and suggests the LaTeX code for the corresponding symbol\n\nThis tutorial was inspired and draws from Justin Bois‚Äô tutorial."
  },
  {
    "objectID": "resources/quarto.html#inline-mathematics",
    "href": "resources/quarto.html#inline-mathematics",
    "title": "Quarto",
    "section": "Inline Mathematics",
    "text": "Inline Mathematics\nTo include mathematical notation within text, enclose the LaTeX within dollar signs $. For example, to obtain the output\n\nthe objective function is \\(4x + 7x\\),\n\nyou would enter\n\nthe objective function is $4x + 7x$.\n\nYou can enter subscripts and superscripts with _ and ^, respectively; to get\n\nthe function is \\(f(x_i) = x_i^2\\),\n\ntype\n\nthe function is $f(x_i) = x_i^2$.\n\nIf you want multiple characters to be enclosed in a subscript or superscript, enclose them in braces {}:\n\n\\(e^{i \\pi} - 1 = 0\\) is produced by\n\n\n$e^{i \\pi} - 1 = 0$.\n\nTo get special characters like \\(\\pi\\) (or other Greek letters), precede their name (or sometimes a code) with a backslash: $\\pi$. There are a number of special characters like this, which you can find in cheatsheets like this one.\nBold characters, which you might use to denote vectors, can be rendered using \\mathbf:\n\n\\(\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^n a_i \\times b_i\\)\n\n\n$\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^n a_i \\times b_i$\n\nFractions can be displayed using \\frac{}{}, where the first bracket encloses the numerator and the second the denominator, as in\n\n\\(\\frac{1}{2}\\)\n\n\n$\\frac{1}{2}$"
  },
  {
    "objectID": "resources/quarto.html#displaying-equations",
    "href": "resources/quarto.html#displaying-equations",
    "title": "Quarto",
    "section": "Displaying Equations",
    "text": "Displaying Equations\nTo place equations or other mathematics on their own line(s), enclose the entire block in two dollar signs $$. For example, the prior dot-product definition could be displayed as \\[\n\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^n a_i \\times b_i\n\\] using\n$$\n\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^n a_i \\times b_i\n$$\nDisplaying equations on their own line(s) can improve the spacing of symbols like sums (as above) or fractions: compare the inline\n\n\\(x &lt; \\frac{1}{2}\\)\n\nto \\[\nx &lt; \\frac{1}{2}.\n\\]\nTo display multiple related lines in a single block, there are two environments of note. The first will center all of the equations, and is obtained by enclosing the equations in \\begin{gather} and \\end{gather}. Each line should be separated with \\\\:\n\\[\n\\begin{gather}\nx_1 + x_2 \\leq 5 \\\\\ny \\leq \\frac{1}{2}.\n\\end{gather}\n\\]\n$$\n\\begin{gather}\nx_1 + x_2 \\leq 5 \\\\\ny \\leq \\frac{1}{2}.\n\\end{gather}\n$$\nThe second environment will let you align the equations as you wish instead of automatically centering them, and is used by enclosing the equations with \\begin{align} and \\end{align}, with an ampersand & in front of the characters which will be used on each line to align the equations:\n\\[\n\\begin{align}\nx_1 + x_2 &\\leq 5 \\\\\ny &\\leq \\frac{1}{2}.\n\\end{align}\n\\]\n$$\n\\begin{align}\nx_1 + x_2 &\\leq 5 \\\\\ny &\\leq \\frac{1}{2}.\n\\end{align}\n$$"
  },
  {
    "objectID": "resources/quarto.html#sizing-parentheses-or-brackets",
    "href": "resources/quarto.html#sizing-parentheses-or-brackets",
    "title": "Quarto",
    "section": "Sizing Parentheses or Brackets",
    "text": "Sizing Parentheses or Brackets\nBy default, parentheses and brackets are sized for simple characters, but will look bad when used to surround fractions or sums, particularly when they are not used in-line: \\[\nx_n = (\\frac{1}{2})^n.\n\\] To make this look better, use \\left and \\right around the left and right parentheses or brackets: \\[\nx_n = \\left(\\frac{1}{2}\\right)^n\n\\]\n$$\nx_n = \\left(\\frac{1}{2}\\right)^n.\n$$\nThis is totally optional, but helps!"
  },
  {
    "objectID": "resources/quarto.html#using-latex-in-figures",
    "href": "resources/quarto.html#using-latex-in-figures",
    "title": "Quarto",
    "section": "Using LaTeX in Figures",
    "text": "Using LaTeX in Figures\nYou may want to use LaTeX in figures, for example if your \\(x\\)-axis should have a title like \\(x\\). To do this, load the LaTeXStrings package and precede the relevant LaTeX-formatted string (within in-line dollar signs $) with L, as in:\n\nusing Plots\nusing LaTeXStrings\n\nx = (-2œÄ):0.01:(2œÄ)\nplot(x, exp.(sin.(x)); xlabel=L\"$x$\", ylabel=L\"$e^{\\sin(x)}$\", legend=false)"
  },
  {
    "objectID": "resources/llm.html",
    "href": "resources/llm.html",
    "title": "Tutorial: Responsible use of large language models",
    "section": "",
    "text": "Large language models (LLMs), like GPT, are powerful tools for generating text that can be used for coding and doing data analysis. This is at once empowering (LLMs are powerful and can save you time) and risky (LLMs can make mistakes that are hard to detect).\nOur general view is that LLMs are powerful tools that you will encounter and use when you leave this classroom, so it‚Äôs important to learn how to use them responsibly and effectively. As described in the syllabus, you are generally permitted to use LLMs in this course, but ultimately, you are responsible for guaranteeing, understanding, and interpreting your results, and you can‚Äôt do this if you don‚Äôt understand the code that you are running (this isn‚Äôt exclusive to code generated by LLMs ‚Äì it also applies to code that you copy from the internet!).\nThe use of LLMs for coding is a new and rapidly evolving area. Rather than provide a lesson plan for you, this page will provide some resources for self-learning."
  },
  {
    "objectID": "resources/llm.html#links-and-resources",
    "href": "resources/llm.html#links-and-resources",
    "title": "Tutorial: Responsible use of large language models",
    "section": "Links and resources",
    "text": "Links and resources\n\nBlog: ‚ÄúBob Carpenter thinks GPT-4 is awesome‚Äù: this post highlights how GPT-4 is able to write a program in Stan, a statistical programming language, and also the mistakes that it makes. Finding and correcting these mistakes requires knowing the Stan language and having a deep understanding of the statistical model, but someone with this expertise could potentially use GPT-4 to accelerate their coding workflow. The comments are also interesting and insightful.\nGitHub Copilot is an extension for VS Code that can provide suggestions for code completion and editing. It is free for students and educators.\nAI Snake Oil is a blog that seeks to dispel hype, remove misconceptions, and clarify the limits of AI. The authors are in the Princeton University Department of Computer Science."
  },
  {
    "objectID": "slides/lecture02-marginal-joint-conditional.html#pdf-and-cdf",
    "href": "slides/lecture02-marginal-joint-conditional.html#pdf-and-cdf",
    "title": "Marginal, conditional, and joint distributions",
    "section": "PDF and CDF",
    "text": "PDF and CDF\nIf \\(F_X\\) is the cumulative distribution function (CDF) of \\(X\\) and \\(f_X\\) is the probability density function (PDF) of \\(X\\), then: \\[\nF_X ( x ) = \\int_{-\\infty}^x f_X(u) \\, du,\n\\] and (if \\(f_X\\) is continuous at \\(x\\) which it typically will be) \\[\nf_{X}(x)={\\frac {d}{dx}}F_{X}(x).\n\\] A useful property is \\[\n\\Pr[a\\leq X\\leq b]=\\int _{a}^{b}f_{X}(x)\\,dx\n\\]\n\n\n\n\n\n\nImportant\n\n\nWe can only talk about the probability that \\(y\\) is in some interval \\([a, b]\\), which is given by the integral of the PDF over that interval. The probability that \\(y\\) takes on the value \\(y^*\\), written \\(p(y=y^*)\\), is zero."
  },
  {
    "objectID": "slides/lecture02-marginal-joint-conditional.html#pdf-example",
    "href": "slides/lecture02-marginal-joint-conditional.html#pdf-example",
    "title": "Marginal, conditional, and joint distributions",
    "section": "PDF example",
    "text": "PDF example\nSimple example to illustrate that \\[\nF_X(2) = \\int_{-\\infty}^2 f_X(u) \\, du\n\\]\nWe will use a standard Normal distribution as an example\n\n\n(0.9771562639858903, 0.9772498680518208)\n\n\n\nMean 0 and standard deviation 1 by default\npdf(d, x) tells us the probability density function of distribution d evaluated at x\nquad_trap is a trapezoidal approximation of the integral with arguments: function, lower bound, upper bound, and number of points"
  },
  {
    "objectID": "slides/lecture02-marginal-joint-conditional.html#pmfs",
    "href": "slides/lecture02-marginal-joint-conditional.html#pmfs",
    "title": "Marginal, conditional, and joint distributions",
    "section": "PMFs",
    "text": "PMFs\n\nDiscrete distributions (like the Poisson) have a probability mass function (PMF) instead of a PDF\nFor PMFs, \\(p(y=y^*)\\) is the probability that \\(y\\) takes on the value \\(y^*\\), and is defined\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the Distributions package, both PDFs and PMFs are called pdf"
  },
  {
    "objectID": "slides/lecture02-marginal-joint-conditional.html#bayes-rule",
    "href": "slides/lecture02-marginal-joint-conditional.html#bayes-rule",
    "title": "Marginal, conditional, and joint distributions",
    "section": "Bayes‚Äô Rule",
    "text": "Bayes‚Äô Rule\n\\[\np(\\theta, y) = p(\\theta) p(y | \\theta)\n\\] and thus \\[\np(\\theta | y) = \\frac{p(\\theta, y)}{p(y)} = \\frac{p(\\theta) p(y | \\theta)}{p(y)}\n\\] generally: \\[\np(\\theta | y) \\propto p(\\theta) p(y | \\theta)\n\\]"
  },
  {
    "objectID": "slides/lecture02-marginal-joint-conditional.html#marginal-probability",
    "href": "slides/lecture02-marginal-joint-conditional.html#marginal-probability",
    "title": "Marginal, conditional, and joint distributions",
    "section": "Marginal probability",
    "text": "Marginal probability\nProbability of event \\(A\\): \\(\\Pr(A)\\)\n\nWe will write the marginal probability density function as \\[\np(\\theta) \\quad \\text{or} \\quad p(y)\n\\]"
  },
  {
    "objectID": "slides/lecture02-marginal-joint-conditional.html#joint-probability",
    "href": "slides/lecture02-marginal-joint-conditional.html#joint-probability",
    "title": "Marginal, conditional, and joint distributions",
    "section": "Joint probability",
    "text": "Joint probability\nProbability of events \\(A\\) and \\(B\\): \\(\\Pr(A \\& B)\\)\n\n\\[\np(\\theta, y)\n\\]"
  },
  {
    "objectID": "slides/lecture02-marginal-joint-conditional.html#conditional-probability",
    "href": "slides/lecture02-marginal-joint-conditional.html#conditional-probability",
    "title": "Marginal, conditional, and joint distributions",
    "section": "Conditional probability",
    "text": "Conditional probability\nProbability of event \\(A\\) given event \\(B\\): \\(\\Pr(A | B)\\)\n\n\\[\np(\\theta | y) \\quad \\text{or} \\quad p(y | \\theta)\n\\]"
  },
  {
    "objectID": "slides/lecture02-marginal-joint-conditional.html#example-two-dice-wager",
    "href": "slides/lecture02-marginal-joint-conditional.html#example-two-dice-wager",
    "title": "Marginal, conditional, and joint distributions",
    "section": "Example: two-dice wager",
    "text": "Example: two-dice wager\n\nA gambler presents you with an even-money wager. You will roll two dice, and if the highest number showing is one, two, three or four, then you win. If the highest number on either die is five or six, then she wins. Should you take the bet?"
  },
  {
    "objectID": "slides/lecture02-marginal-joint-conditional.html#overview",
    "href": "slides/lecture02-marginal-joint-conditional.html#overview",
    "title": "Marginal, conditional, and joint distributions",
    "section": "Overview",
    "text": "Overview\nStandard linear regression model, let‚Äôs assume \\(x \\in \\mathbb{R}\\) for simplicity (1 predictor): \\[\ny_i = ax_i + b + \\epsilon_i\n\\] where \\(\\epsilon_i \\sim N(0, \\sigma^2)\\)."
  },
  {
    "objectID": "slides/lecture02-marginal-joint-conditional.html#conditional-distribution-of-y_i",
    "href": "slides/lecture02-marginal-joint-conditional.html#conditional-distribution-of-y_i",
    "title": "Marginal, conditional, and joint distributions",
    "section": "Conditional distribution of \\(y_i\\)",
    "text": "Conditional distribution of \\(y_i\\)\nThe conditional probability density of \\(y_i\\) given \\(x_i\\) is \\[\np(y_i | x_i, a, b, \\sigma) = N(ax_i + b, \\sigma^2)\n\\] which is a shorthand for writing out the full equation for the Normal PDF. We can (and often will) write this as \\[\ny_i \\sim \\mathcal{N}(ax_i + b, \\sigma^2)\n\\] Finally, we will sometimes write \\(p(y_i | x_i)\\) as a shorthand for \\(p(y_i | x_i, a, b, \\sigma)\\). While fine in many circumstances, we should take care to make sure we are extremely clear about what parameters we are conditioning on."
  },
  {
    "objectID": "slides/lecture02-marginal-joint-conditional.html#marginal-distribution-of-y_i",
    "href": "slides/lecture02-marginal-joint-conditional.html#marginal-distribution-of-y_i",
    "title": "Marginal, conditional, and joint distributions",
    "section": "Marginal distribution of \\(y_i\\)",
    "text": "Marginal distribution of \\(y_i\\)\nThe marginal probability density of \\(y_i\\) is \\[\np(y_i | a, b, \\sigma) = \\int p(y_i | x_i, a, b, \\sigma) p(x_i) \\, dx_i\n\\] where \\(p(x_i)\\) is the probability density of \\(x_i\\)."
  },
  {
    "objectID": "slides/lecture02-marginal-joint-conditional.html#joint-distribution-of-y_i-and-x_i",
    "href": "slides/lecture02-marginal-joint-conditional.html#joint-distribution-of-y_i-and-x_i",
    "title": "Marginal, conditional, and joint distributions",
    "section": "Joint distribution of \\(y_i\\) and \\(x_i\\)",
    "text": "Joint distribution of \\(y_i\\) and \\(x_i\\)\nThe joint probability density of \\(y_i\\) and \\(x_i\\) is \\[\np(y_i, x_i | a, b, \\sigma) = p(y_i | x_i, a, b, \\sigma) p(x_i)\n\\] where \\(p(x_i)\\) is the probability density of \\(x_i\\)."
  },
  {
    "objectID": "slides/lecture02-marginal-joint-conditional.html#simulation",
    "href": "slides/lecture02-marginal-joint-conditional.html#simulation",
    "title": "Marginal, conditional, and joint distributions",
    "section": "Simulation",
    "text": "Simulation\n\nIf \\(x=2\\), we can simulate from the conditional distribution of \\(y\\):\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf \\(x \\sim N(0, 1)\\), then we can simulate from the joint distribution of \\(x\\) and \\(y\\):\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA list comprehension here is less elegant than writing rand.(Normal.(m .* x .+ b, œÉ)) but it is easy to read. The results are the same.\n\n\n\nFinally, assuming the same distribution, we can simulate from the marginal distribution of \\(y\\):"
  },
  {
    "objectID": "slides/lecture02-marginal-joint-conditional.html#overview-1",
    "href": "slides/lecture02-marginal-joint-conditional.html#overview-1",
    "title": "Marginal, conditional, and joint distributions",
    "section": "Overview",
    "text": "Overview\nThe Negative Binomial distribution (see last lecture) can be interpreted as a Gamma-Poisson mixture:\n\\[\n\\begin{align}\ny &\\sim \\textrm{Poisson}(\\lambda) \\\\\n\\lambda &\\sim \\textrm{Gamma}\\left(r, \\frac{p}{1-p} \\right)\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/lecture02-marginal-joint-conditional.html#mathematical-derivation",
    "href": "slides/lecture02-marginal-joint-conditional.html#mathematical-derivation",
    "title": "Marginal, conditional, and joint distributions",
    "section": "Mathematical derivation",
    "text": "Mathematical derivation\nWe can show mathematically that if \\(y ~ \\textrm{Negative Binomial}(r, p)\\), that is equivalent to the mixture model \\(y ~ \\textrm{Poisson}(\\lambda)\\) and \\(\\lambda ~ \\textrm{Gamma}(r, p / (1 - p))\\). \\[\n\\begin{align}\n& \\int_0^{\\infty} f_{\\text {Poisson }(\\lambda)}(y) \\times f_{\\operatorname{Gamma}\\left(r, \\frac{p}{1-p}\\right)}(\\lambda) \\mathrm{d} \\lambda \\\\\n& = \\int_0^{\\infty} \\frac{\\lambda^y}{y !} e^{-\\lambda} \\times \\frac{1}{\\Gamma(r)}\\left(\\frac{p}{1-p} \\lambda\\right)^{r-1} e^{-\\frac{p}{1-p} \\lambda}\\left(\\frac{p}{1-p} \\mathrm{~d} \\lambda\\right) \\\\\n\\ldots \\\\\n&= f_{\\text {Negative Binomial }(r, p)}(y)\n\\end{align}\n\\] For all the steps see Wikipedia."
  },
  {
    "objectID": "slides/lecture02-marginal-joint-conditional.html#simulation-example",
    "href": "slides/lecture02-marginal-joint-conditional.html#simulation-example",
    "title": "Marginal, conditional, and joint distributions",
    "section": "Simulation example",
    "text": "Simulation example\nWe can see this with simulation. First we define a function to simulate from the Gamma-Poisson mixture:\n\n\ngamma_poisson (generic function with 1 method)\n\n\n\nThen we can simulate from the mixture and compare to the Negative Binomial distribution:"
  },
  {
    "objectID": "slides/lecture02-marginal-joint-conditional.html#so-what",
    "href": "slides/lecture02-marginal-joint-conditional.html#so-what",
    "title": "Marginal, conditional, and joint distributions",
    "section": "So what?",
    "text": "So what?\nI don‚Äôt need you to know all the details of this particular mixture model. What I do want you to understand is:\n\n\nWe can model data using combinations of simpler distributions\nWe can use simple simulation approaches to approximate more complex relationships\n\nFor example, if we wanted to know \\(\\Pr(y &gt; 10)\\) when \\(y \\sim \\text{Negative Binomial}(r, p)\\) but we didn‚Äôt have a Negative Binomial distribution in our software package we could estimate our quantity of interest\nThis isn‚Äôt very interesting for this model (there is an analytic solution!) but lots of models we might want to write down don‚Äôt have analytic solutions"
  },
  {
    "objectID": "slides/lecture02-marginal-joint-conditional.html#key-ideas",
    "href": "slides/lecture02-marginal-joint-conditional.html#key-ideas",
    "title": "Marginal, conditional, and joint distributions",
    "section": "Key ideas",
    "text": "Key ideas\n\nConditional probability\nJoint probability\nMarginal probability\nBayes‚Äô Rule\nLikelihood\nPosterior\nSimulation methods"
  },
  {
    "objectID": "slides/lecture03-maximum-likelihood.html#motivation",
    "href": "slides/lecture03-maximum-likelihood.html#motivation",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Motivation",
    "text": "Motivation\nWe have some parametric statistical model with unknown parameters. We want to evaluate how consistent the data are with different values of the parameters, and to find the values of the parameters that are most consistent with the data."
  },
  {
    "objectID": "slides/lecture03-maximum-likelihood.html#today",
    "href": "slides/lecture03-maximum-likelihood.html#today",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Today",
    "text": "Today\nToday‚Äôs lecture contains a few key ideas and a lot of slides.\n\nLots of examples\nWe will move quickly through examples\nReview the examples yourself and ask questions on Canvas\nLab will build on the examples (use them as a reference!)"
  },
  {
    "objectID": "slides/lecture03-maximum-likelihood.html#definition",
    "href": "slides/lecture03-maximum-likelihood.html#definition",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Definition",
    "text": "Definition\nThe likelihood is the probability of the data given some parameters: \\[\np(y | \\theta)\n\\]\n\nOften, we want to study how the likelihood changes for different values of \\(\\theta\\), holding \\(y\\) fixed. This is just \\(p(y | \\theta)\\) for a range of \\(\\theta\\).\n\n\n\n\n\n\n\n\nNote\n\n\nYou will sometimes see this referred to as \\(\\mathcal{L}(\\theta)\\), which is a confusing notation‚Ä¶"
  },
  {
    "objectID": "slides/lecture03-maximum-likelihood.html#likelihood-example",
    "href": "slides/lecture03-maximum-likelihood.html#likelihood-example",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Likelihood example",
    "text": "Likelihood example\nWe can plot \\(p(y | \\theta)\\) for different values of \\(\\theta\\). To do that, we need a function for \\(p(y | \\theta)\\). We will consider \\(y \\sim \\mathcal{N}(\\mu, \\sigma)\\) so \\(\\theta = \\left\\{ \\mu, \\sigma \\right\\}\\).\n\nThe ::T are called ‚Äútype annotations‚Äù and specify the type of variable that each argument can take. In this case, any Real (float or integer) will work. Read more in the docs.\nThis specifies the likelihood using the pdf function\n\n\nNext we plug in some values for \\(\\mu\\) and plot the likelihood for each. This is essentially plotting the likelihood as a function of \\(\\theta\\) for fixed \\(y\\).\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe vector notation lik. means to apply the function lik to each element of Œº_try. [lik(xi, 1, 2) for xi in x] would do the same thing.\nNotice the likelihood function is maximized at \\(\\mu = y\\)."
  },
  {
    "objectID": "slides/lecture03-maximum-likelihood.html#iid-assumption",
    "href": "slides/lecture03-maximum-likelihood.html#iid-assumption",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "IID assumption",
    "text": "IID assumption\nIndependent and identically distributed (i.i.d.) assumption: \\[\n\\begin{align}\np(y_1, y_2, \\ldots, y_n) &= p(y_1) p(y_2) \\times \\ldots \\times p(y_n)\\\\\n&= \\prod_{i=1}^n p(y_i)\n\\end{align}\n\\]\n\nUsually we have more than one data point. Say we measure \\(y = y_1, y_2, \\ldots, y_n\\): \\[\np(y | \\theta) = \\prod_{i=1}^n p(y_i | \\theta)\n\\]"
  },
  {
    "objectID": "slides/lecture03-maximum-likelihood.html#log-trick",
    "href": "slides/lecture03-maximum-likelihood.html#log-trick",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Log trick",
    "text": "Log trick\nRecall: \\(\\log(AB) = \\log(A) + \\log(B)\\) or, more generally, \\[\n\\log \\left( \\prod_{i=1}^n f_i \\right) = \\sum_{i=1}^n \\log(f_i)\n\\]\n\nThus, we can work with the ‚Äúlog likelihood‚Äù: \\[\n\\log p(y | \\theta) =  \\log \\left( \\prod_{i=1}^n p(y_i | \\theta) \\right) = \\sum_{i=1}^n \\log \\left( p(y_i | \\theta) \\right)\n\\]\n\n\nAdding small numbers is more numerically stable than multiplying them"
  },
  {
    "objectID": "slides/lecture03-maximum-likelihood.html#numerical-example-multiple-data-points",
    "href": "slides/lecture03-maximum-likelihood.html#numerical-example-multiple-data-points",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Numerical example: multiple data points",
    "text": "Numerical example: multiple data points\nWe can extend our previous example to multiple data points. As before, we need a likelihood function.\n\nVector{&lt;:Real} means a vector of any subtype of Real. Julia uses ‚Äúmultiple dispatch‚Äù which means that we can have multiple functions with the same name but that do different things depending on what the type of the arguments is.\nlogpdf function (from Distributions) is the log of the pdf. Here log_liks will be a vector with the same length as y.\nAdd up all the log likelihoods then take the exponent ‚Äì equvalent to the product of the likelihoods.\n\n\nAs before, we can plot the likelihood as a function of \\(\\mu\\) for fixed \\(y\\) and \\(\\sigma\\).\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this case both \\(\\mu\\) and \\(y\\) are vectors, with different lengths, so using the dot notation lik. won‚Äôt work ‚Äì it doesn‚Äôt know which variable to vectorize over."
  },
  {
    "objectID": "slides/lecture03-maximum-likelihood.html#setup",
    "href": "slides/lecture03-maximum-likelihood.html#setup",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Setup",
    "text": "Setup\nWe collect \\(y_1, \\ldots, y_n\\) which are the number of tropical cyclones that make landfall in the continental United States in a given year. We decide to model them as a Poisson distribution with unknown rate \\(\\lambda\\): \\[\np(y_i | \\lambda) = \\frac{\\lambda^{y_i} e^{-\\lambda}}{y_i!}\n\\]## Total log likelihood\n\n\nTake the log \\[\n\\log p(y_i | \\lambda) = y_i \\log(\\lambda) - \\lambda - \\log(y_i!)\n\\]\nFor multiple data points \\[\n\\log p(y | \\lambda) = \\sum_{i=1}^n y_i \\log(\\lambda) - n \\lambda - \\sum_{i=1}^n \\log(y_i!)\n\\]\n\n\n\n\nI am deliberately showing different ways to implement the same thing in different examples. Here we use a list comprehension. We could instead use, for example, vector notation: logpdf.(Poisson(Œª)). Performance differences are usually negligible, focus on readability."
  },
  {
    "objectID": "slides/lecture03-maximum-likelihood.html#plot",
    "href": "slides/lecture03-maximum-likelihood.html#plot",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Plot",
    "text": "Plot"
  },
  {
    "objectID": "slides/lecture03-maximum-likelihood.html#setup-1",
    "href": "slides/lecture03-maximum-likelihood.html#setup-1",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Setup",
    "text": "Setup\nLet‚Äôs say we don‚Äôt know for sure that \\(\\sigma = 1\\). In that case our mathmatical model for \\(p(y | \\mu, \\sigma)\\) is unchanged from the single-variable case.\nLet‚Äôs write a function for the log likelihood"
  },
  {
    "objectID": "slides/lecture03-maximum-likelihood.html#plotting",
    "href": "slides/lecture03-maximum-likelihood.html#plotting",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Plotting",
    "text": "Plotting\nWith two parameters, we need to plot a surface\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis syntax: z = [f(x, y) for xi in x, yi in y] will produce a matrix with z[i, j] = f(x[i], y[j]).\nDue to a quirk of syntax, we need to transpose the matrix lik_plot to get the correct orientation. Here, lik_plot' is equivalent to transpose(lik_plot).\nst=:heatmap tells Plots to plot a heatmap. We could also try :surface or :contourf.\n\nNotice that there is a very small region for which the likelihood is [relatively] high."
  },
  {
    "objectID": "slides/lecture03-maximum-likelihood.html#logic",
    "href": "slides/lecture03-maximum-likelihood.html#logic",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Logic",
    "text": "Logic\nCan we find the parameters \\(\\theta^*\\) that maximize the likelihood \\(p(y | \\theta)\\)?"
  },
  {
    "objectID": "slides/lecture03-maximum-likelihood.html#log-likelihood",
    "href": "slides/lecture03-maximum-likelihood.html#log-likelihood",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Log likelihood",
    "text": "Log likelihood\nWe can use the log likelihood \\(\\log p(y | \\theta)\\) instead of the likelihood \\(p(y | \\theta)\\).\nThe log likelihood is monotonic with the likelihood, so \\[\n\\arg \\max \\log p(y | \\theta) = \\arg \\max p(y | \\theta)\n\\]"
  },
  {
    "objectID": "slides/lecture03-maximum-likelihood.html#analytic-solution",
    "href": "slides/lecture03-maximum-likelihood.html#analytic-solution",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Analytic solution",
    "text": "Analytic solution\nSolving things analytically takes time up-front, but can be much faster to run because you can avoid the optimization step. Consider the (potentially multivariate) Gaussian example with known covariance matrix \\(\\Sigma\\). We want to maximize the likelihood \\[\n\\sum_{i=1}^n p(y_i | \\mu, \\Sigma)\n\\]\n\nTo maximize, we set its derivative with respect to \\(\\mu\\), which we‚Äôll denote with \\(\\nabla_\\mu\\), to zero: \\[\n\\sum_{i=1}^n \\nabla_\\mu \\log p(y_i | \\mu, \\Sigma) = 0\n\\]\n\n\nSubstituting in the multivariate Gaussian likelihood we get: \\[\n\\begin{aligned}\n0 & =\\sum_{i=1}^n \\nabla_\\mu \\log \\frac{1}{\\sqrt{(2 \\pi)^d|\\Sigma|}} \\exp \\left(-\\frac{1}{2}\\left(x_i-\\mu\\right)^{\\top} \\Sigma^{-1}\\left(x_i-\\mu\\right)\\right) \\\\\n& =\\sum_{i=1}^n \\nabla_\\mu\\left(\\log \\left(\\frac{1}{\\sqrt{(2 \\pi)^d|\\Sigma|}}\\right)\\right)+\\log \\left(\\exp \\left(-\\frac{1}{2}\\left(x_i-\\mu\\right)^{\\top} \\Sigma^{-1}\\left(x_i-\\mu\\right)\\right)\\right) \\\\\n& =\\sum_{i=1}^n \\nabla_\\mu\\left(-\\frac{1}{2}\\left(x_i-\\mu\\right)^{\\top} \\Sigma^{-1}\\left(x_i-\\mu\\right)\\right)\\\\\n&=\\sum_{i=1}^n \\Sigma^{-1}\\left(x_i-\\mu\\right) \\\\\n0 &= \\sum_{i=1}^n (x_i - \\mu) \\\\\n\\mu &= \\frac{1}{n} \\sum_{i=1}^n x_i\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\nNote\n\n\nYou are not expected to remember the above equations and I won‚Äôt ask you to do this derivation in a time-constrained exam. You should understand the general procedure:\n\nwrite down log likelihood for all data points\n\nwrite down likelihood for one data point\nwrite down log likelihood for one data point\nsum over all data points\n\ntake \\(\\frac{d}{d\\theta}\\) and set equal to zero to maximize\nsolve for \\(\\theta^*\\)."
  },
  {
    "objectID": "slides/lecture03-maximum-likelihood.html#numerical-approach-i",
    "href": "slides/lecture03-maximum-likelihood.html#numerical-approach-i",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Numerical approach I",
    "text": "Numerical approach I\nWe can use the optimize function from the Optim.jl package to find the maximum likelihood estimate. First, we need to define the function to be optimized. optimize will minimize the function, so we need to define the negative log likelihood. We‚Äôll call this the ‚Äúloss‚Äù function.\n\nNote that this function takes in a single argument which is a vector of parameters. We‚Äôll call this vector Œ∏ but it doesn‚Äôt matter what we call it."
  },
  {
    "objectID": "slides/lecture03-maximum-likelihood.html#numerical-approach-ii",
    "href": "slides/lecture03-maximum-likelihood.html#numerical-approach-ii",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Numerical approach II",
    "text": "Numerical approach II\nNow we can run the optimization. Since \\(\\sigma &gt; 0\\) always, we will pass along bounds. We could alternatively do something clever like work with \\(\\log \\sigma\\) instead of \\(\\sigma\\).\n\n\n2-element Vector{Float64}:\n 1.97000000014951\n 0.7590125165166877\n\n\n\nThe lower bound is actually zero, but we just set it to a ‚Äúpretty small‚Äù number.\nThe upper bound is infinity, we can pass in Inf\nWe need to pass in a guess for the parameters. We‚Äôll just use \\(\\mu = \\sigma = 1\\).\nThis will actually run the optimization\nThis will extract the parameters that minimize the loss function.\n\nWe could convert this to a Distributions object as\n\n\nNormal{Float64}(Œº=1.97000000014951, œÉ=0.7590125165166877)"
  },
  {
    "objectID": "slides/lecture03-maximum-likelihood.html#overview",
    "href": "slides/lecture03-maximum-likelihood.html#overview",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Overview",
    "text": "Overview\nLet‚Äôs consider the generic regression probelem where we have paired observations \\(\\left\\{x_i, y_i\\right\\}_{i=1}^n\\). In general, we can write this regression as \\[\ny_i | \\alpha, \\beta, \\epsilon \\sim \\mathcal{N}(\\alpha + x_i \\beta, \\sigma^2)\n\\] where \\(x_i\\) and \\(\\beta\\) may be vectors.\n\nWe can create some raw data (click to ‚Äúunfold‚Äù the code)"
  },
  {
    "objectID": "slides/lecture03-maximum-likelihood.html#anlytic-approach",
    "href": "slides/lecture03-maximum-likelihood.html#anlytic-approach",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Anlytic approach",
    "text": "Anlytic approach\nWe can use the same approach to derive the maximum likelihood estimate for linear regression:\n\nWrite the likelihood for one data point\nWrite the log likelihood for one data point\nWrite the log likelihood for all data points\nTake \\(\\frac{d}{d\\theta}\\) and set equal to zero to maximize\n\nIf you want a walkthrough, see Ryan Adams‚Äôs lecture notes starting at about equation 11."
  },
  {
    "objectID": "slides/lecture03-maximum-likelihood.html#numerical-optimization-i",
    "href": "slides/lecture03-maximum-likelihood.html#numerical-optimization-i",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Numerical optimization I",
    "text": "Numerical optimization I\nAs before, we need to write down a (log) likelihood function"
  },
  {
    "objectID": "slides/lecture03-maximum-likelihood.html#numerical-optimization-ii",
    "href": "slides/lecture03-maximum-likelihood.html#numerical-optimization-ii",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Numerical optimization II",
    "text": "Numerical optimization II\n\n\n3-element Vector{Float64}:\n -3.147\n  2.392\n  1.562\n\n\n\nThis is the loss function, which is the negative log likelihood. The first value of \\(\\theta\\) is the intercept, the second is the slope, and the third is the standard deviation.\nWe need to pass in bounds for the parameters. The standard deviation is always positive, so we set the lower bound to a small number.\nThis will extract the parameters that minimize the loss function and round to show three decimal places."
  },
  {
    "objectID": "slides/lecture03-maximum-likelihood.html#parallel-least-squares",
    "href": "slides/lecture03-maximum-likelihood.html#parallel-least-squares",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Parallel: least squares",
    "text": "Parallel: least squares\nIf we work through the math, we can show that the log likelihood for the linear regression problem is \\[\n\\log p(y | X, \\beta, \\sigma) = \\frac{N}{2} \\log (2 \\sigma^2 \\pi)  - \\frac{1}{2 \\sigma^2} \\left( X \\beta - y \\right)^T \\left( X \\beta - y \\right)\n\\]\n\n\n\n\n\n\nLinear algebra notation\n\n\nThere is no intercept here! This is a common notation and assumes that the first column of \\(X\\) is all ones. That is equivalent to writing down an intercept, but lets us use linear algebra notation and keep track of fewer variable names\n\n\n\n\nFrom this, we can show that terms drop out and \\[\n\\beta^\\text{MLE} = \\arg \\min_\\beta \\left( X \\beta - y \\right)^T \\left( X \\beta - y \\right)\n\\] which is exactly the least squares problem (minimize squared error): \\[\n\\min_{\\theta} \\sum_{i=1}^n (y_i - y_i^\\text{pred})^2\n\\]\n\n\n\n\n\n\n\n\nKey point\n\n\n‚ÄúLeast squares can be interpreted as assuming Gaussian noise, and particular choices of likelihood can be interpreted directly as (usually exponentiated) loss functions‚Äù ‚ÄìAdams\n\n\n\nIf we then want to estimate \\(\\sigma\\), we can estimate the standard deviation of the residuals."
  },
  {
    "objectID": "slides/lecture03-maximum-likelihood.html#dont-get-it-twisted",
    "href": "slides/lecture03-maximum-likelihood.html#dont-get-it-twisted",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Don‚Äôt get it twisted",
    "text": "Don‚Äôt get it twisted\n\nMany people get this backwards!\n\n\n\nThe likelihood is the probability of the data given the parameters: \\(p(y | \\theta)\\).\nWe often plot the likelihood for many different \\(\\theta\\)\n\n\\(p(y | \\theta)\\) for many different \\(\\theta\\)\n\nDon‚Äôt confuse this with the posterior, which is the probability of the parameters given the data: \\(p(\\theta | y)\\)"
  },
  {
    "objectID": "slides/lecture03-maximum-likelihood.html#logistics",
    "href": "slides/lecture03-maximum-likelihood.html#logistics",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Logistics",
    "text": "Logistics\n\nFriday:\n\nLab 03 in class ‚Äì look for GH Classroom link on Canvas\nLab 02 due\n\nNext week:\n\nBayesian inference"
  },
  {
    "objectID": "slides/lecture03-maximum-likelihood.html#references",
    "href": "slides/lecture03-maximum-likelihood.html#references",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "slides/lecture01-lab.html#overview",
    "href": "slides/lecture01-lab.html#overview",
    "title": "Setting up Julia, GitHub, and Quarto",
    "section": "Overview",
    "text": "Overview\nLabs are in-class exercises intended to get practice with coding or analysis workflows.\n\nInstructions available on website\nDownload ahead of time by using link from Canvas\nYou will have your own repository (more in a minute)\nTry to finish in class, but due in 1 week"
  },
  {
    "objectID": "slides/lecture01-lab.html#tool-overview",
    "href": "slides/lecture01-lab.html#tool-overview",
    "title": "Setting up Julia, GitHub, and Quarto",
    "section": "Tool overview",
    "text": "Tool overview\nIn this class, we will use\n\n\nJulia\nGitHub\nQuarto\nVS Code (suggested)"
  },
  {
    "objectID": "slides/lecture01-lab.html#why-julia",
    "href": "slides/lecture01-lab.html#why-julia",
    "title": "Setting up Julia, GitHub, and Quarto",
    "section": "Why Julia?",
    "text": "Why Julia?\n\n\n\n\nSyntax\n\nReadable to computers and humans\nClosely parallels math notation\n\nDesigned for numerical and scientific computing\n\n\n\nFast!\n\n‚ÄúTwo language problem‚Äù\nAll you need is Julia\n\nOpen source"
  },
  {
    "objectID": "slides/lecture01-lab.html#julia-example",
    "href": "slides/lecture01-lab.html#julia-example",
    "title": "Setting up Julia, GitHub, and Quarto",
    "section": "Julia example",
    "text": "Julia example\nA (naive) implementation of the Fibonacci sequence:\n\nfunction fib(n)\n    if n &lt; 2\n        return n\n    else\n        return fib(n - 1) + fib(n - 2)\n    end\nend\nfib(10)\n\n55"
  },
  {
    "objectID": "slides/lecture01-lab.html#github",
    "href": "slides/lecture01-lab.html#github",
    "title": "Setting up Julia, GitHub, and Quarto",
    "section": "GitHub",
    "text": "GitHub\n\n\nYou need a GitHub account\nCode is stored in ‚Äúrepositories‚Äù\nclone a repository to your computer\nMake changes and commit them\npush your changes to GitHub\nUsing GitHub classroom, instructors can view your code"
  },
  {
    "objectID": "slides/lecture01-lab.html#quarto",
    "href": "slides/lecture01-lab.html#quarto",
    "title": "Setting up Julia, GitHub, and Quarto",
    "section": "Quarto",
    "text": "Quarto\nQuarto is a tool that allows you to combine text and code and create many types of output\n\nThis website is made with Quarto\nYou will use Quarto to create reports for labs\n\nEverything in one place\nNo running code, save a figure to Downloads, copy into Word, then update your code and try to remember where to paste it\n\nReproducible"
  },
  {
    "objectID": "slides/lecture01-lab.html#vs-code",
    "href": "slides/lecture01-lab.html#vs-code",
    "title": "Setting up Julia, GitHub, and Quarto",
    "section": "VS Code",
    "text": "VS Code\n\nVS Code is a text editor\n\nIf you are an advanced user of another text editor, you can use that instead, but I recommend VS Code\n\nVS Code can work as a Julia IDE"
  },
  {
    "objectID": "slides/lecture01-lab.html#detailed-instructions",
    "href": "slides/lecture01-lab.html#detailed-instructions",
    "title": "Setting up Julia, GitHub, and Quarto",
    "section": "Detailed instructions",
    "text": "Detailed instructions\nSee Setup"
  },
  {
    "objectID": "slides/lecture01-lab.html#lab-01-instructions",
    "href": "slides/lecture01-lab.html#lab-01-instructions",
    "title": "Setting up Julia, GitHub, and Quarto",
    "section": "Lab 01 Instructions",
    "text": "Lab 01 Instructions\n\nInstall software up following instructions on course website\nclone the repository for lab 01 (use the Github Classroom link from Canvas)\nEdit the solutions.qmd file to add your name and netID\ncommit and push your changes"
  },
  {
    "objectID": "slides/lecture11-gev-nonstationary.html#tbd",
    "href": "slides/lecture11-gev-nonstationary.html#tbd",
    "title": "Nonstationary GEV (Planned)",
    "section": "TBD",
    "text": "TBD\nThis is a placeholder for a planned lecture. The schedule will be modified as needed."
  },
  {
    "objectID": "slides/lecture12-model-selection.html#tbd",
    "href": "slides/lecture12-model-selection.html#tbd",
    "title": "Quantitative and Graphical Model Selection (Planned)",
    "section": "TBD",
    "text": "TBD\nThis is a placeholder for a planned lecture. The schedule will be modified as needed."
  },
  {
    "objectID": "slides/lecture10-frequency-analysis.html#tbd",
    "href": "slides/lecture10-frequency-analysis.html#tbd",
    "title": "Frequency Analysis (Planned)",
    "section": "TBD",
    "text": "TBD\nThis is a placeholder for a planned lecture. The schedule will be modified as needed."
  },
  {
    "objectID": "slides/lecture08-pca.html#tbd",
    "href": "slides/lecture08-pca.html#tbd",
    "title": "Principal Components Analysis (Planned)",
    "section": "TBD",
    "text": "TBD\nThis is a placeholder for a planned lecture. The schedule will be modified as needed."
  },
  {
    "objectID": "slides/lecture11-gev-estimators.html#tbd",
    "href": "slides/lecture11-gev-estimators.html#tbd",
    "title": "GEV Estimators (Planned)",
    "section": "TBD",
    "text": "TBD\nThis is a placeholder for a planned lecture. The schedule will be modified as needed."
  },
  {
    "objectID": "slides/lecture09-random-forest.html#tbd",
    "href": "slides/lecture09-random-forest.html#tbd",
    "title": "Random Forests (Planned)",
    "section": "TBD",
    "text": "TBD\nThis is a placeholder for a planned lecture. The schedule will be modified as needed."
  },
  {
    "objectID": "slides/lecture13-tbd.html#tbd",
    "href": "slides/lecture13-tbd.html#tbd",
    "title": "Placeholder TBD (Planned)",
    "section": "TBD",
    "text": "TBD\nThis is a placeholder for a planned lecture. The schedule will be modified as needed."
  },
  {
    "objectID": "slides/lecture07-glm.html#practice-problem-regression",
    "href": "slides/lecture07-glm.html#practice-problem-regression",
    "title": "Generalized Linear Models",
    "section": "Practice problem: regression",
    "text": "Practice problem: regression\nYou are given pairs of data \\((x_i, y_i)\\) where \\(x_i\\) is the number of vehicles on the road and \\(y_i\\) is the Air Quality Index (AQI). We model their relationship as \\[\n\\begin{align}\ny_i &\\sim N(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha_i + \\beta x_i\n\\end{align}\n\\] where \\(\\alpha_i\\) and \\(\\beta\\) are parameters to be estimated.\n\nWrite down the log likelihood for a single data point \\(\\log p(y_i | x_i)\\)\nWrite down the log likelihood for the entire dataset \\(\\log p(\\mathbf{y} | \\mathbf{x})\\)\n\nFor reference, the Normal PDF is \\[\nf(x | \\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2 \\sigma^2}\\right)\n\\]"
  },
  {
    "objectID": "slides/lecture07-glm.html#linear-regression",
    "href": "slides/lecture07-glm.html#linear-regression",
    "title": "Generalized Linear Models",
    "section": "Linear regression",
    "text": "Linear regression\nWe have recently seen models that look like \\[\n\\begin{align}\ny_i &\\sim N(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha_i + \\beta x_i\n\\end{align}\n\\] Or (using another notation) \\[\n\\begin{align}\ny_i &= \\alpha_i + \\beta x_i + \\epsilon_i \\\\\n\\epsilon_i &\\sim N(0, \\sigma)\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/lecture07-glm.html#why-linear",
    "href": "slides/lecture07-glm.html#why-linear",
    "title": "Generalized Linear Models",
    "section": "Why linear?",
    "text": "Why linear?\n\n\n\\(y = ax + b\\) is a strong assumption, not always physically justifiable, though often useful.\nAnother nice way to think about linear models is that they are Taylor series representations of functions. Michael Betancourt has an excellent and thorough case study."
  },
  {
    "objectID": "slides/lecture07-glm.html#motivation-depth-damage",
    "href": "slides/lecture07-glm.html#motivation-depth-damage",
    "title": "Generalized Linear Models",
    "section": "Motivation: depth-damage",
    "text": "Motivation: depth-damage\nIn lab 3, we studied the distribution of flood losses in a neighborhood.\n\n\nWhat if we wanted to condition this distribution on variables describing flood characteristics and/or household risk management practices (as in R√∂zer et al., 2019)?\nRegression lets us condition estimates on covariates\nBut we can‚Äôt use linear regression here, because the response variable has support \\((0, 1)\\)"
  },
  {
    "objectID": "slides/lecture07-glm.html#today",
    "href": "slides/lecture07-glm.html#today",
    "title": "Generalized Linear Models",
    "section": "Today",
    "text": "Today\nGeneralized Linear Models extend the concept of regression to other distributions ‚Äì specifically when the conditional likelihood is not Normal."
  },
  {
    "objectID": "slides/lecture07-glm.html#motivation",
    "href": "slides/lecture07-glm.html#motivation",
    "title": "Generalized Linear Models",
    "section": "Motivation",
    "text": "Motivation\nConsider a forest patch where we have recorded the occurrence of forest fires over several years. For each year, we have also noted the average summertime temperature. We want to investigate if there‚Äôs a relationship between the average summertime temperature and the likelihood of a forest fire occurring."
  },
  {
    "objectID": "slides/lecture07-glm.html#data",
    "href": "slides/lecture07-glm.html#data",
    "title": "Generalized Linear Models",
    "section": "Data",
    "text": "Data"
  },
  {
    "objectID": "slides/lecture07-glm.html#likelihood",
    "href": "slides/lecture07-glm.html#likelihood",
    "title": "Generalized Linear Models",
    "section": "Likelihood",
    "text": "Likelihood\nFor each data point, we can use a Bernoulli distribution to model the occurrence of a forest fire \\[\ny_i \\sim \\mathrm{Bernoulli}(p_i)\n\\] where the Bernoulli PDF is \\[\nf(x | p) = p^x (1 - p)^{1 - x}\n\\]"
  },
  {
    "objectID": "slides/lecture07-glm.html#data-dependence",
    "href": "slides/lecture07-glm.html#data-dependence",
    "title": "Generalized Linear Models",
    "section": "Data dependence",
    "text": "Data dependence\nWe have \\(y_i \\sim \\mathrm{Bernoulli}(p_i)\\). We want to model \\(p_i\\) as some function of the average summertime temperature \\(x_i\\): \\[\nf(p_i)= \\alpha + \\beta x_i\n\\]\n\nThis looks a lot like the linear regression model from before, except:\n\n\nThe likelihood is Binomial rather than Normal. No big deal ‚Äì we‚Äôve seen that we can use arbitrary probability distributions to model processes we‚Äôre interested in\nWe have \\(f(p_i)\\) rather than just \\(\\mu_i\\). This is called a link function. GLMs need a link function to map the linear space of \\(\\alpha + \\beta x_i \\in (-\\infty, \\infty)\\) onto the allowed space of the parameter."
  },
  {
    "objectID": "slides/lecture07-glm.html#link-function",
    "href": "slides/lecture07-glm.html#link-function",
    "title": "Generalized Linear Models",
    "section": "Link function",
    "text": "Link function\n\n\nFor our Binomial model, \\(p_i \\in (0, 1)\\)\n\nwe need something to that maps \\((-\\infty, \\infty) \\rightarrow (0, 1)\\).\n\nMany possible options\nThe ‚Äúcanonical‚Äù one is a logit link:\n\n\n\n\\[\n\\begin{align}\n\\textrm{logit}(p_i) &= \\alpha + \\beta x_i \\\\\n\\log \\frac{p_i}{1 - p_i} &= \\alpha + \\beta x_i \\\\\np_i &= \\frac{\\exp(\\alpha + \\beta x_i)}{1 + \\exp(\\alpha + \\beta x_i)}\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/lecture07-glm.html#probit-link",
    "href": "slides/lecture07-glm.html#probit-link",
    "title": "Generalized Linear Models",
    "section": "Probit link",
    "text": "Probit link\nThis isn‚Äôt the only possible link function, though. For example, economists like to use the probit link ‚Äì the Probit is the inverse of a standard Nomal distribution. These can be subtly different:"
  },
  {
    "objectID": "slides/lecture07-glm.html#inference-i",
    "href": "slides/lecture07-glm.html#inference-i",
    "title": "Generalized Linear Models",
    "section": "Inference I",
    "text": "Inference I\n\nHere we are saying that x and y have to be vectors, but we don‚Äôt care what kind of vector (e.g.¬†Vector{Float64}, Vector{Int}, etc.)\nThis is a more robust way to write for i in 1:length(y)\ninv_logit is defined above."
  },
  {
    "objectID": "slides/lecture07-glm.html#inference-ii",
    "href": "slides/lecture07-glm.html#inference-ii",
    "title": "Generalized Linear Models",
    "section": "Inference II",
    "text": "Inference II\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis sets the random number generator seed so that we get the same results every time we run this code. This is useful for reproducibility, but you don‚Äôt need to do this in your own code!"
  },
  {
    "objectID": "slides/lecture07-glm.html#motivation-1",
    "href": "slides/lecture07-glm.html#motivation-1",
    "title": "Generalized Linear Models",
    "section": "Motivation",
    "text": "Motivation\nImagine a national park where we‚Äôve recorded the number of wildlife sightings over several months. For each month, we also have the average number of visitors. We want to investigate if there‚Äôs a relationship between the average number of visitors and the number of wildlife sightings."
  },
  {
    "objectID": "slides/lecture07-glm.html#data-1",
    "href": "slides/lecture07-glm.html#data-1",
    "title": "Generalized Linear Models",
    "section": "Data",
    "text": "Data"
  },
  {
    "objectID": "slides/lecture07-glm.html#likelihood-1",
    "href": "slides/lecture07-glm.html#likelihood-1",
    "title": "Generalized Linear Models",
    "section": "Likelihood",
    "text": "Likelihood\nFor each data point, we can use a Poisson distribution to model the number of wildlife sightings: \\[\ny_i \\sim \\mathrm{Poisson}(\\lambda_i)\n\\] where the Poisson PMF is \\[\nf(k | \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}.\n\\]"
  },
  {
    "objectID": "slides/lecture07-glm.html#data-dependence-1",
    "href": "slides/lecture07-glm.html#data-dependence-1",
    "title": "Generalized Linear Models",
    "section": "Data dependence",
    "text": "Data dependence\nWe have \\(y_i \\sim \\mathrm{Poisson}(\\lambda_i)\\). We want to model \\(\\lambda_i\\) as some function of the average number of visitors \\(x_i\\): \\[\nf(\\lambda_i) = \\alpha + \\beta x_i\n\\] We need \\(\\lambda_i &gt; 0\\) for the Poisson distribution. The canonical link function is \\(\\log\\).‚Äã"
  },
  {
    "objectID": "slides/lecture07-glm.html#inference-i-1",
    "href": "slides/lecture07-glm.html#inference-i-1",
    "title": "Generalized Linear Models",
    "section": "Inference I",
    "text": "Inference I\n\n@. means all the operations to the right use dot syntax ‚Äì this is equivalent to exp.(Œ± .+ Œ≤ .* x)."
  },
  {
    "objectID": "slides/lecture07-glm.html#inference-ii-1",
    "href": "slides/lecture07-glm.html#inference-ii-1",
    "title": "Generalized Linear Models",
    "section": "Inference II",
    "text": "Inference II"
  },
  {
    "objectID": "slides/lecture07-glm.html#posterior-check",
    "href": "slides/lecture07-glm.html#posterior-check",
    "title": "Generalized Linear Models",
    "section": "Posterior check",
    "text": "Posterior check\nTo our scatter plot, we can add the posterior predictive distribution, which we will visualize as percentiles\n\n\nRecall: we want \\(\\mathbb{E}[f(\\lambda_i)]\\)\nHere: \\(f\\) will be some percentile of the Poisson distribution with parameter \\(\\lambda_i\\)\nFor each value we want to predict at, we need to compute this percentile for each posterior sample and then average\n\nWhy can‚Äôt we average the values of \\(\\lambda\\) and then compute the percentile?"
  },
  {
    "objectID": "slides/lecture07-glm.html#posterior-check-ii",
    "href": "slides/lecture07-glm.html#posterior-check-ii",
    "title": "Generalized Linear Models",
    "section": "Posterior check II",
    "text": "Posterior check II"
  },
  {
    "objectID": "slides/lecture07-glm.html#other-common-models",
    "href": "slides/lecture07-glm.html#other-common-models",
    "title": "Generalized Linear Models",
    "section": "Other common models",
    "text": "Other common models\n\nNegative Binomial regression: \\(y_i \\sim \\textrm{NegBin}(\\mu_i, r)\\) with \\(\\log(\\mu_i)= \\alpha + \\beta x_i\\)\n‚ÄúRobust‚Äù regression: \\(y_i = \\alpha + \\beta x_i + \\epsilon_i\\) with \\(\\epsilon_i \\sim \\textrm{T}^\\nu(\\sigma)\\) (\\(\\nu=4\\) is common)\nLots more"
  },
  {
    "objectID": "slides/lecture07-glm.html#simple-interfaces",
    "href": "slides/lecture07-glm.html#simple-interfaces",
    "title": "Generalized Linear Models",
    "section": "Simple interfaces",
    "text": "Simple interfaces\n\nTuringGLM\n\nstill a work in progress\nconvert a formula @formula(y ~ x1 + x2 + x3) into a Turing model\n\nInspired by\n\nBRMS in R, using Stan backend\nbambi in Python, using PyMC3 backend\n\n\nThese tools can be useful for fast model building, but when you write final versions of your results you should make sure you know what priors, data re-scaling, etc. you have used!"
  },
  {
    "objectID": "slides/lecture07-glm.html#summary",
    "href": "slides/lecture07-glm.html#summary",
    "title": "Generalized Linear Models",
    "section": "Summary",
    "text": "Summary\nWorking with non-Gaussian likelihoods is pretty straightforward\n\n\nNeed a ‚Äúlink function‚Äù\nCan use our same software tools to get MLE or Bayesian estimates\n\n\n\nMcElreath (2020) offers useful workflow suggestions:\n\nUse sensitivity analysis\nDo prior predictive checks (see chapter 11 for an example with Poisson regression)"
  },
  {
    "objectID": "slides/lecture07-glm.html#logistics",
    "href": "slides/lecture07-glm.html#logistics",
    "title": "Generalized Linear Models",
    "section": "Logistics",
    "text": "Logistics\n\nLab 5 due next Monday\nProject 1 to be posted soon (builds on lab 5)\nExam 1 revisions due Friday in class"
  },
  {
    "objectID": "slides/lecture07-glm.html#read-more",
    "href": "slides/lecture07-glm.html#read-more",
    "title": "Generalized Linear Models",
    "section": "Read more",
    "text": "Read more\nSome optional reading:\n\nChapters 10 and 11 of McElreath (2020) (in particular section 10.2)\n\nThis book spends a lot of time talking about Maximum Entropy likelihoods, which we won‚Äôt worry about much.\n\nChapter 16 of Gelman et al. (2014)\n\nBecause this comes towards the end of the book, it builds on a fair bit of content we haven‚Äôt yet seen"
  },
  {
    "objectID": "slides/lecture07-glm.html#references",
    "href": "slides/lecture07-glm.html#references",
    "title": "Generalized Linear Models",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nGelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (2014). Bayesian Data Analysis (3rd ed.). Chapman & Hall/CRC Boca Raton, FL, USA.\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second edition.). Boca Raton ; CRC Press, Taylor & Francis Group.\n\n\nR√∂zer, V., Kreibich, H., Schr√∂ter, K., M√ºller, M., Sairam, N., Doss-Gollin, J., et al. (2019). Probabilistic models significantly reduce uncertainty in Hurricane Harvey pluvial flood loss estimates. Earth‚Äôs Future, 7(4). https://doi.org/10.1029/2018ef001074"
  },
  {
    "objectID": "slides/lecture02-distributions.html#a-quick-note-on-pacing",
    "href": "slides/lecture02-distributions.html#a-quick-note-on-pacing",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "A quick note on pacing",
    "text": "A quick note on pacing\nWe will move through this module (‚Äúfundamentals‚Äù) at a fairly brisk pace\n\nReview course slides\nAsk questions on Canvas or in office hours\nTo help you learn to code, I am exposing you to code early and often\n\nI don‚Äôt expect that you are able to replicate all the code in this notebook\nI have added annotations where appropriate\nThe labs will give you practice\nYou will not need to write code from scratch for the exams\n\n\n\nThis is a long slide deck. We will probably finish on Wednesday"
  },
  {
    "objectID": "slides/lecture02-distributions.html#what-is-a-package",
    "href": "slides/lecture02-distributions.html#what-is-a-package",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "What is a package?",
    "text": "What is a package?\n\nCode that is bundled for easy use\nProvides functionality that is not part of the base language\n\nMost stuff in Julia requires packages, as we will see\n\nNeed to be installed\nDeveloped by the community"
  },
  {
    "objectID": "slides/lecture02-distributions.html#where-do-i-get-packages",
    "href": "slides/lecture02-distributions.html#where-do-i-get-packages",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Where do I get packages?",
    "text": "Where do I get packages?\n\nJulia has a built-in package manager for installing add-on functionality written in Julia. It can also install external libraries using your operating system‚Äôs standard system for doing so, or by compiling from source."
  },
  {
    "objectID": "slides/lecture02-distributions.html#where-are-packages-stored",
    "href": "slides/lecture02-distributions.html#where-are-packages-stored",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Where are packages stored?",
    "text": "Where are packages stored?\nEach project has an *environment, which is defined by the following files (do not edit them manually):\n\nProject.toml: lists the specified dependencies of the project\nManifest.toml: lists the exact versions of the packages that are used in the project\n\nThe actual packages are stored on your computer and you don‚Äôt need to worry"
  },
  {
    "objectID": "slides/lecture02-distributions.html#workflow-activate",
    "href": "slides/lecture02-distributions.html#workflow-activate",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Workflow: activate",
    "text": "Workflow: activate\nWe activate a project to tell Julia that we want to use the packages in that project. These steps are equivalent:\n\n\n\nOpen the REPL\nusing Pkg\nPkg.activate(\".\")\n\n\n\nOpen the REPL\nPress ] to enter the package manager\nactivate ."
  },
  {
    "objectID": "slides/lecture02-distributions.html#workflow-install",
    "href": "slides/lecture02-distributions.html#workflow-install",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Workflow: install",
    "text": "Workflow: install\nWe add a package to install it in the current project\n\n\n\nOpen the REPL\nusing Pkg\nPkg.add(\"DataFrames\")\n\n\n\nOpen the REPL\nPress ] to enter the package manager\nadd DataFrames"
  },
  {
    "objectID": "slides/lecture02-distributions.html#workflow-instantiate",
    "href": "slides/lecture02-distributions.html#workflow-instantiate",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Workflow: instantiate",
    "text": "Workflow: instantiate\nWhen working with someone else‚Äôs project, we need to install the packages that they use.\n\nactivate does not install anything, just tells Julia which packages to use\ninstantiate is your friend to make sure an environment is ready to use. If there‚Äôs nothing to do, instantiate does nothing."
  },
  {
    "objectID": "slides/lecture02-distributions.html#learn-more",
    "href": "slides/lecture02-distributions.html#learn-more",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Learn more",
    "text": "Learn more\n\nPkg.jl docs\n\nSee ‚ÄúUsing someone else‚Äôs projecg‚Äù for more on instantiate\n\nWell-worked blog post by Julies Krumbiegel"
  },
  {
    "objectID": "slides/lecture02-distributions.html#lab-01-issues",
    "href": "slides/lecture02-distributions.html#lab-01-issues",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Lab 01 issues",
    "text": "Lab 01 issues\n\n\nBe sure to submit your assignment\nCanvas discussion: ‚ÄúLab 01 Discussion‚Äù\nERROR: Jupyter kernel 'julia-1.9' not found. x4"
  },
  {
    "objectID": "slides/lecture02-distributions.html#lab-01-fix",
    "href": "slides/lecture02-distributions.html#lab-01-fix",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Lab 01 fix",
    "text": "Lab 01 fix\n\nIn order to run codes using Quarto, you need the IJulia package\n\nListed in Manifest.toml but you need to instantiate\n\nIf that doesn‚Äôt work:\n\nRun Pkg.build(\"IJulia\") in the REPL (after you activate and instantiate)\n\nI‚Äôve updated the instructions"
  },
  {
    "objectID": "slides/lecture02-distributions.html#stats-without-the-agonizing-details",
    "href": "slides/lecture02-distributions.html#stats-without-the-agonizing-details",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Stats without the agonizing details",
    "text": "Stats without the agonizing details\nIn this class we will use computation and simulation to build fundamental insight into statistical processes without dwelling on ‚Äúagonizing‚Äù details."
  },
  {
    "objectID": "slides/lecture02-distributions.html#motivating-question",
    "href": "slides/lecture02-distributions.html#motivating-question",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Motivating question",
    "text": "Motivating question\n\nDoes drinking beer reduce the likelihood of being bitten by mosquitos?"
  },
  {
    "objectID": "slides/lecture02-distributions.html#raw-data",
    "href": "slides/lecture02-distributions.html#raw-data",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Raw data",
    "text": "Raw data\nCreate a variable called beer to hold the number of mosquito bites for beer drinkers:\n\n\n25-element Vector{Int64}:\n 27\n 20\n 21\n 26\n 27\n 31\n 24\n 21\n 20\n 19\n 23\n 24\n 28\n 19\n 24\n 29\n 18\n 20\n 17\n 31\n 20\n 25\n 28\n 21\n 27"
  },
  {
    "objectID": "slides/lecture02-distributions.html#what-is-beer",
    "href": "slides/lecture02-distributions.html#what-is-beer",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "What is beer?",
    "text": "What is beer?\nWe can learn a bit more about it:\n\n\n\nVector{Int64} (alias for Array{Int64, 1})\n\n\n\n\n\n25\n\n\n\n\n(25,)\n\n\n\n\n23.6"
  },
  {
    "objectID": "slides/lecture02-distributions.html#more-raw-data",
    "href": "slides/lecture02-distributions.html#more-raw-data",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "More raw data",
    "text": "More raw data\nWe can do the same for water drinkers:\n\nBy putting the ; at the end of our statement, we keep the notebook from showing the output"
  },
  {
    "objectID": "slides/lecture02-distributions.html#a-simple-analysis",
    "href": "slides/lecture02-distributions.html#a-simple-analysis",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "A simple analysis",
    "text": "A simple analysis\nLet‚Äôs calculate the difference between the average number of bites in each group.\n\n\n4.37777777777778\n\n\n\nThis gives us the mean function from the StatsBase package"
  },
  {
    "objectID": "slides/lecture02-distributions.html#the-skeptics-argument",
    "href": "slides/lecture02-distributions.html#the-skeptics-argument",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "The skeptic‚Äôs argument",
    "text": "The skeptic‚Äôs argument\nThe skeptic asks whether this might be random chance.\n\n\nWe could answer this with a T test\n\nDetermine if there is a significant difference between the means of two groups\nAssumes (approximate) normality\nAssumptions hidden behind a software package\n\nSimulation approach:\n\nSuppose the skeptic is right ‚Äì the two groups are samped from the same distribution\nShuffle the data (randomly divide into two groups by assuming that there is no difference between the two groups)\nCalculate the difference between each group\nRepeat many times and examine the distribution of differences\n\n\n\n\nMake clear that we want to relax our assumptions and to simplify the analysis. Some of the things T tests make you think about, like whether data is paired or not, whether the variances are equal, whether you want one or two direction, etc are important and figure into the design of your simulation approach."
  },
  {
    "objectID": "slides/lecture02-distributions.html#implementation",
    "href": "slides/lecture02-distributions.html#implementation",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Implementation",
    "text": "Implementation\n\n\n1.224444444444444\n\n\n\nUse the shuffle function from the Random package\nDefine a function. Its arguments are y1 and y2\nend closes the function definition\nCall the function with our data"
  },
  {
    "objectID": "slides/lecture02-distributions.html#running",
    "href": "slides/lecture02-distributions.html#running",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Running",
    "text": "Running\nWe want to learn about the sampling distribution of the group differences: repeat this experiment many times over and plott the results\n\n\n50000\n\n\n\nThis is a list comprehension. It‚Äôs a way to create a list by looping over something. Here, we loop over the numbers 1 to 50,000 and call get_shuffled_difference each time.\nlength tells us the size of a vector"
  },
  {
    "objectID": "slides/lecture02-distributions.html#plotting",
    "href": "slides/lecture02-distributions.html#plotting",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Plotting",
    "text": "Plotting\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe need the Plots package to make plots\nDefine a function. Its arguments are diffs and obs\nhistogram is a function from the Plots package\nCreate a histogram using the diffs object. ; separates the positional arguments from the keyword arguments\nxlabel is a ‚Äúkeyword argument‚Äù specifying the text for the x-axis label\nthe y-axis label\nthe label to use in the legend\nspecify the bins to use in the histogram\nspecify the location of the legend\nnormalize the histogram so that the area under the curve is 1\nadd a vertical line (vline!) at the observed difference\nmany functions return their output ‚Äì in this case the plot we created from the inputs"
  },
  {
    "objectID": "slides/lecture02-distributions.html#alternative",
    "href": "slides/lecture02-distributions.html#alternative",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Alternative",
    "text": "Alternative\nWe could have done this with a parametric test\n\nWe need the HypothesisTests package\nWe don‚Äôt need to include the HypothesisTests., but it adds clarity\nRecall: ; suppresses output\n\n\n\n\n\nTwo sample t-test (equal variance)\n----------------------------------\nPopulation details:\n    parameter of interest:   Mean difference\n    value under h_0:         0\n    point estimate:          4.37778\n    95% confidence interval: (1.913, 6.843)\n\nTest summary:\n    outcome with 95% confidence: reject h_0\n    two-sided p-value:           0.0009\n\nDetails:\n    number of observations:   [25,18]\n    t-statistic:              3.5869843832143413\n    degrees of freedom:       41\n    empirical standard error: 1.220461900604875\n\n\n\n\n\nTwo sample t-test (unequal variance)\n------------------------------------\nPopulation details:\n    parameter of interest:   Mean difference\n    value under h_0:         0\n    point estimate:          4.37778\n    95% confidence interval: (1.957, 6.798)\n\nTest summary:\n    outcome with 95% confidence: reject h_0\n    two-sided p-value:           0.0007\n\nDetails:\n    number of observations:   [25,18]\n    t-statistic:              3.658244539721401\n    degrees of freedom:       39.11341478045414\n    empirical standard error: 1.196688119190407"
  },
  {
    "objectID": "slides/lecture02-distributions.html#discussion",
    "href": "slides/lecture02-distributions.html#discussion",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Discussion",
    "text": "Discussion\n\n\nThis is called a bootstrap and is a very powerful tool in many situations\nWhat would we expect to see if the skeptic was correct?\nP-value:\n\nthe likelihood of the data if the null hypothesis is correct\nskeptic‚Äôs (null) hypothesis: no difference between groups\n\n\n\n\n\n\n0.00052\n\n\n\n. is the dot operator. It applies the function to each element of the vector individually."
  },
  {
    "objectID": "slides/lecture02-distributions.html#the-normal-distribution",
    "href": "slides/lecture02-distributions.html#the-normal-distribution",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "The Normal distribution",
    "text": "The Normal distribution\nThe Normal (Gaussian) distribution has probability distribution function:\n\\[\np(y | \\mu, \\theta) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\n\\left(\n-\\frac{1}{2}\\left(\n\\frac{x-\\mu}{\\sigma}\n\\right)^{\\!2}\n\\,\n\\right)\n\\]\n\nMean \\(\\mu\\)\n\nMedian equal to mean\n\nVariance \\(\\sigma^2\\)\nSymmetric"
  },
  {
    "objectID": "slides/lecture02-distributions.html#central-limit-theorem",
    "href": "slides/lecture02-distributions.html#central-limit-theorem",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Central limit theorem",
    "text": "Central limit theorem\nThe central limit theorem says that the sum of many independent random variables is approximately normally distributed.\nWe can see this with an example:\n\nFor each sample \\(i = 1, \\ldots, N\\):\n\nDraw J draws from a non-Gaussian distribution \\(\\mathcal{D}\\)\nTake the mean and save it as \\(\\bar{y}_i\\)\n\nPlot the distribution of \\(\\bar{y}_i\\)\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo type yÃÑ, type y then type \\bar and hit tab. Julia allows unicode (or emojis) in variable names\nTo type ‚àà , type \\in and hit tab. The _ isn‚Äôt doing anything special and we could name it i or üò∂ or whatever we want but _ suggests it‚Äôs a throwaway\nThis is another list comprehension"
  },
  {
    "objectID": "slides/lecture02-distributions.html#notation",
    "href": "slides/lecture02-distributions.html#notation",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Notation",
    "text": "Notation\nWe will get tired of writing\n\\[\np(y | \\mu, \\theta) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\n\\left(\n-\\frac{1}{2}\\left(\n\\frac{x-\\mu}{\\sigma}\n\\right)^{\\!2}\n\\,\n\\right)\n\\]\nInstead, we will often use shorthand:\n\\[\ny \\sim \\mathcal{N}(\\mu, \\sigma^2)\n\\]"
  },
  {
    "objectID": "slides/lecture02-distributions.html#normal-pdf",
    "href": "slides/lecture02-distributions.html#normal-pdf",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Normal PDF",
    "text": "Normal PDF\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nL\"&lt;string&gt;\" allows us to use LaTeX in strings\nThis notation specifies the values of \\(\\mu\\) and \\(\\sigma\\)"
  },
  {
    "objectID": "slides/lecture02-distributions.html#bernoulli-distribution",
    "href": "slides/lecture02-distributions.html#bernoulli-distribution",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Bernoulli distribution",
    "text": "Bernoulli distribution\nA Bernoulli distribution models a coin flip.\n\n\n5-element Vector{Bool}:\n 0\n 0\n 0\n 0\n 0\n\n\n\nDraw 5 samples from the Bernoulli distribution with parameter p"
  },
  {
    "objectID": "slides/lecture02-distributions.html#binomial-distribution",
    "href": "slides/lecture02-distributions.html#binomial-distribution",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Binomial distribution",
    "text": "Binomial distribution\nA Binomial distribution models the distribution of n consecutive flips of the same coin\n\n\n5-element Vector{Int64}:\n 3\n 4\n 5\n 1\n 3"
  },
  {
    "objectID": "slides/lecture02-distributions.html#multinomial-distribution",
    "href": "slides/lecture02-distributions.html#multinomial-distribution",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Multinomial distribution",
    "text": "Multinomial distribution\nThe Multinomial extends the Binomial to multiple categories. Note that p is a vector. If there are 2 categories (\\(K=2\\)), it‚Äôs just the binomial with \\(p_\\text{multinomial} = [p, 1-p]\\).‚Äù\n\n\n3√ó5 Matrix{Int64}:\n 3  3  0  4  2\n 2  0  0  1  1\n 0  2  5  0  2\n\n\n\nTo be more concise, we could write rand(Multinimial([0.5, 0.3, 0.2], 5), 5). Which is more readable?"
  },
  {
    "objectID": "slides/lecture02-distributions.html#poisson-distribution",
    "href": "slides/lecture02-distributions.html#poisson-distribution",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Poisson distribution",
    "text": "Poisson distribution\nThe Poisson distribution is used to model count data. It is the limit of a Binomial distribution with \\(p=\\lambda/N\\), as \\(N \\rightarrow \\infty\\).\nA Poisson distribution has mean and variance equal to \\(\\lambda\\).\n\n\n10-element Vector{Int64}:\n 1\n 4\n 4\n 1\n 3\n 3\n 3\n 2\n 2\n 4\n\n\n\nThe Poisson distribution has one parameter, \\(\\lambda\\)\nDraw 10 samples from the Poisson distribution"
  },
  {
    "objectID": "slides/lecture02-distributions.html#negative-binomial-distribution",
    "href": "slides/lecture02-distributions.html#negative-binomial-distribution",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Negative binomial distribution",
    "text": "Negative binomial distribution\nThe NegativeBinomaial distribution relaxes the Poisson‚Äôs assumotion that \\(\\text{mean} = \\text{variance}\\).\nThis distribution models the number of successes in a sequence of independent and identically distributed Bernoulli trials with probability p before a specified (non-random) number of failures (r) occurs. For example, we can define rolling a 6 on a dice as a failure, and rolling any other number as a success, and ask how many successful rolls will occur before we see the third failure (p = 1/6 and r = 3)."
  },
  {
    "objectID": "slides/lecture02-distributions.html#what-other-distributions-do-you-know",
    "href": "slides/lecture02-distributions.html#what-other-distributions-do-you-know",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "What other distributions do you know?",
    "text": "What other distributions do you know?\n\n\nUniform\nExponential\nGamma (see above)\nBeta\nPareto\nStudent t\nBoltzmann\nMany more!"
  },
  {
    "objectID": "slides/lecture02-distributions.html#mean",
    "href": "slides/lecture02-distributions.html#mean",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Mean",
    "text": "Mean\nThe mean of a sample is just the sample average: \\[\n\\bar{y} = \\frac{1}{N} \\sum_{i=1}^N y_i\n\\]\n\nThe mean of a distribution is the expected value of the distribution: \\[\n\\mathbb{E}(u) = \\int u p(u) \\, du\n\\]"
  },
  {
    "objectID": "slides/lecture02-distributions.html#variance",
    "href": "slides/lecture02-distributions.html#variance",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Variance",
    "text": "Variance\nVariance measures how points differ from the mean\n\nYou may be familiar with sample variance: \\[\nS^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}\n\\]\n\n\nFor a distribution: \\[\n\\mathbb{V}(u) = \\int (u - \\mathbb{E}(u))^2 p(u) \\, du\n\\] or, for a vector \\[\n\\mathbb{V}(u) = \\int (u - \\mathbb{E}(u)) (u - \\mathbb{E}(u))^T p(u) \\, du\n\\]"
  },
  {
    "objectID": "slides/lecture02-distributions.html#coming-up",
    "href": "slides/lecture02-distributions.html#coming-up",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Coming up",
    "text": "Coming up\n\nWednesday: working with probability distributions\nFriday:\n\nLabs‚Äù 02: Working with tabular data in Julia\nLab 01 due at start of class"
  },
  {
    "objectID": "slides/lecture02-distributions.html#office-hours",
    "href": "slides/lecture02-distributions.html#office-hours",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Office hours",
    "text": "Office hours\nIf you haven‚Äôt filled out the Doodle, please do so ASAP"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "This course covers the use of tools from data science (statistics, machine learning, and programming) to model climate hazards such as floods and droughts. Through hands-on programming assignments based on state-of-the-art published research, students will learn to apply methods to real-world problems with a strong emphasis on probabilistic methods and uncertainty quantification."
  },
  {
    "objectID": "syllabus.html#course-overview",
    "href": "syllabus.html#course-overview",
    "title": "Syllabus",
    "section": "",
    "text": "This course covers the use of tools from data science (statistics, machine learning, and programming) to model climate hazards such as floods and droughts. Through hands-on programming assignments based on state-of-the-art published research, students will learn to apply methods to real-world problems with a strong emphasis on probabilistic methods and uncertainty quantification."
  },
  {
    "objectID": "syllabus.html#course-information",
    "href": "syllabus.html#course-information",
    "title": "Syllabus",
    "section": "Course Information",
    "text": "Course Information\n\n\n\nInstructor\n\n James Doss-Gollin\n jdossgollin@rice.edu\n Ryon 215\n\n\n\n\nTA\n\n Yuchen Lu\n yl238@rice.edu\n Ryon B28\n\n\n\n\nMeetings\n\n MWF\n 11-11:50am\n Keck 107\n\n\n\n\n\nLearning Objectives\nAt the end of this class, students will:\n\nWrite down generative or statistical models for climate hazards;\nUse Bayesian and maximum likelihood methods to estimate the parameters of simple statistical models (‚Äúinverse modeling‚Äù);\nUse simulation models (‚Äúforward modeling‚Äù) to assess the logical implications of statistical models;\nUnderstand and apply extreme value theory to estimate the probability of rare climate hazards;\nCritically interpret statistical analyses of environmental data applied in academic journals, government, and industry; and\nUnderstand and communicate subjective modeling choices to technical (e.g., scientist) and non-technical (e.g., policy-maker) audiences.\n\n\n\nPrerequisites & Preparation\n\nLinear algebra (you should be comfortable with matrix notation and basic operations)\nA course in applied statistics (e.g., STAT 419/519)\nSome exposure to Python, Julia, Matlab, R, or another programming language\n\nIn addition, a course covering machine learning, Bayesian statistics, or applied statistics is encouraged but not required. If you are unsure whether your background gives you an adequate preparation for this course, please contact the instructor!\n\n\n\n\n\n\nWhat If My Skills Are Rusty?\n\n\n\nIf your programming, mathematics, or statistics skills are a little rusty, don‚Äôt worry! We will review concepts and build skills over the course of the semester.\n\n\n\n\nTopics\n\n\nThe course will build core skills in:\n\nStatistical inference\nMachine learning\nData exploration and visualization\nExtreme value statistics\nModel selection, validation, and comparison\n\n\nWe will apply these methods to a variety of case studies, including three project-based assignments that cover:\n\nPrecipitation frequency analysis\nFlood extent estimation\nStochastic streamflow generation\n\n\n\nFor a full list of topics to be covered, see the course schedule.\n\n\nRequired Materials\nNo textbook is required for this course. All materials will be posted as open source on the course website or as PDFs on Canvas."
  },
  {
    "objectID": "syllabus.html#a-community-of-learning",
    "href": "syllabus.html#a-community-of-learning",
    "title": "Syllabus",
    "section": "A Community of Learning",
    "text": "A Community of Learning\nRice‚Äôs core values are responsibility, integrity, community, and excellence. Our goal is to create a learning community aligned with these core values.\n\nCore Expectations\nCourse success involves a dual responsibility on the part of the instructor and the student.\n\n\nAs the instructor, my responsibility is to provide you with a structure and opportunity to learn. To this end, I commit to:\n\nprovide organized and focused lectures, in-class activities, and assignments;\nencourage students to regularly evaluate and provide feedback on the course;\nmanage the classroom atmosphere to promote learning;\nschedule sufficient out-of-class contact opportunities, such as office hours;\nallow adequate time for assignment completion;\nmake lecture materials, class policies, activities, and assignments accessible to students.\n\n\nStudents are responsible for their own learning in the course and should commit to:\n\nattending all lectures;\ndoing all required preparatory work before class;\nactively participating in online and in-class discussions;\nbeginning assignments and other work early; and\nattending office hours as needed.\n\n\n\n\n\n\n\n\n\nWhat If I‚Äôm Sick?\n\n\n\nPlease stay home if you‚Äôre feeling sick! This is beneficial for both for your own recovery and the health and safety of your classmates. We will also make any necessary arrangements for you to stay on top of the class material and if whatever is going on will negatively impact your grade, for example by causing you to be unable to submit an assignment on time.\n\n\n\n\n\n\n\n\nCanvas Discussions\n\n\n\n\nIf you wait until the day an assignment is due (or even late the previous night) to ask a question on Canvas, there is a strong chance that I will not see your post prior to the deadline.\nBut if you see unanswered questions and you have some insight, please answer! This class will work best when we all work together as a community.\n\n\n\n\n\nDiversity, Equity, and Inclusion\nRice is committed to building and maintaining an equitable and inclusive campus community. Diversity can refer to multiple ways that we identify ourselves, including but not limited to race, color, national origin, language, sex, disability, age, sexual orientation, gender identity, religion, creed, ancestry, belief, veteran status, or genetic information. Each of these diverse identities, along with many others not mentioned here, shape the perspectives our students, faculty, and staff bring to our campus. We, at Rice, will work to promote diversity, equity and inclusion not only because diversity fuels excellence and innovation, but because we want to pursue justice. We acknowledge our imperfections while we also fully commit to the work, inside and outside of our classrooms, of building and sustaining a campus community that increasingly embraces these core values.\nEach of us is responsible for creating a safer, more inclusive environment.y.\n\n\nAccommodation for Students with Disabilities\nIf you have a documented disability or other condition that may affect academic performance you should: 1) make sure this documentation is on file with the Disability Resource Center (Allen Center, Room 111 / adarice@rice.edu / x5841) to determine the accommodations you need; and 2) talk with me to discuss your accommodation needs.\n\n\nAccommodation for Scheduling Conflicts\nIf any of our class meetings conflict with your religious events, student athletics, or other non-negotiable scheduling conflict, please let me know ASAP so that we can make arrangements for you.\n\n\nMask Policies\nMasks are welcome but not required in the classroom. However, I strongly encourage compliance with requests to mask from students, faculty, and staff who are concerned about the risk of infection. Please be respectful of these concerns and requests and do not ask someone making such a request to disclose their underlying medical condition. If for some reason you need your instructor or classmates to wear a mask, please let me know and I will communicate this to the class without disclosing your identity.\nThese policies may change over the course of the semester as the situation evolves.\n\n\nPolicy on Web Posting of Course Materials\nUploading course materials to web sites is not an authorized use of the course material. Both the poster and the user are in violation of the university policy, which is actionable.\n\n\nAcademic Integrity\nThis class is designed to encourage collaboration, and students are encouraged to discuss their work with other students. Engineering as a profession relies upon the honesty and integrity of its practitioners (see e.g. the American Society for Civil Engineers‚Äô Code of Ethics). All work submitted must represent the students‚Äô own work and understanding, whether individually or as a group (depending on the particulars of the assignment). This includes analyses, code, software runs, and reports.\nMore specifically, all students will be held to the standards of the Rice Honor Code, a code that you pledged to honor when you matriculated at this institution. If you are unfamiliar with the details of this code and how it is administered, you should consult the Honor System Handbook at honor.rice.edu/honor-system-handbook/. This handbook outlines the University‚Äôs expectations for the integrity of your academic work, the procedures for resolving alleged violations of those expectations, and the rights and responsibilities of students and faculty members throughout the process.\nIf you are ever unclear about academic integrity, please ask! Additionally, always err on the side of providing more information.)\n\n\nAI/ML Resource Policy\nAI/ML tools, like ChatGPT, can be incredibly powerful aids in learning, and can help beginner programmers with syntax and code structure. However, the use of these tools poses risks including the potential for plagiarism and the potential for students to rely on these tools without developing their own understanding.\nAs noted, all work submitted for a grade in this course must reflect your own understanding. You are welcome to use AI/ML tools to help you learn, but you must describe how you used the tool when you turn in your assignment. Moreover, you should not submit code that you do not understand as you be held responsible for explaining any code you submit. For more, see our page on LLMs."
  },
  {
    "objectID": "syllabus.html#grading-labs-10",
    "href": "syllabus.html#grading-labs-10",
    "title": "Syllabus",
    "section": "Grading### Labs: 10%",
    "text": "Grading### Labs: 10%\nOn most Fridays we will use class time for hands-on programming exercises (‚Äúlabs‚Äù) to give you guided practice applying the concepts and methods from class. These labs will be announced on the course website ahead of time so anyone who is able can bring a laptop to class. These labs can be done in groups; if you cannot bring a laptop to class for whatever reason, you will be able to (and are encouraged to) work with other students, though you must turn in your own assignment for grading.\nSome details on lab logistics:\n\nLabs will be designed to be completed in class, but you may occasionally require additional time to complete them.\nLabs will be graded on a 3-point scale: strong (3/3), acceptable (2/3), lacking (1/3), or missing (0/3).\nDetailed solutions will be provided and you will be responsible for reviewing them on your own. Material covered on labs may be covered in tests.\n\n\nTests: 40%\nIn-class written exams will be given for each of the four modules of the course, on the dates listed on the schedule. Tests will cover material from lectures and labs, and we will dedicate a class to review before each exam.\nBased on past experience, students enter the class with a wide range of backgrounds and experience. The tests are designed so that students who meet the pre-requisites, but do not have extensive additional experience, can do well. Students with backgrounds that exceed the minimum pre-requisites may find the tests relatively straightforward.\n\n\nProjects: 40%\nModules 2-4 will culminate with a project designed to apply the tools we learn in class to a real-world problem. These projects will be introduced at the start of each module, will motivate the material we cover in class, and give you an opportunity to apply the methods we learn to a problem of your choosing. Projects also offer an opportunity for students with more experience to dig deeper.\nSpecific instructions and rubrics will be provided for each project. You will submit your projects as a Quarto notebook (.qmd file) on Canvas using the provided GitHub classroom link.\n\n\nParticipation: 10%\nParticipating fully in the class allows you to gain more from the class and contribute more to the learning of your classmates. Some ways to participate include:\n\nAttending every class\nAsking questions in class\nAnswering questions on Canvas\nComing to office hours\n\nYou will be asked to evaluate your own participation over the course of the semester, and I will provide feedback on your participation as well\n\n\nLate Work Policy\n\nLate projects will be subjected to a 10% penalty per day, which can accumulate to 100% of the total grade.\nLate labs will not be accepted, because we will discuss solutions in class.\nSometimes things come up in life. Please reach out ahead of time if you have extenuating circumstances (including University-approved absences or illnesses) which would make it difficult for you to submit your work on time. Work which would be late for appropriate reasons will be given extensions and the late penalty will be waived."
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Setting Up Your Computer",
    "section": "",
    "text": "Getting set up for this course requires the following steps. If you are an experienced programmer, you are free to follow your own workflow to set up these tools. You will absolutely need Quarto, GitHub, and Julia. If you are not an experienced programmer, the following steps are not the only way to get these tools set up, but they are a very good way.\nIf you install course tools using steps other than the ones provided on this page, be aware that your instructors may be able to provide you with only limited support."
  },
  {
    "objectID": "setup.html#install-julia",
    "href": "setup.html#install-julia",
    "title": "Setting Up Your Computer",
    "section": "Install Julia",
    "text": "Install Julia\nI recommend installing Julia using the juliaup tool, which will let you easily manage versions in the future and works seamlessly with VS Code. The instructions can be found at the JuliaUp GitHub repository, but we will summarize them here.\n\nInstalling Juliaup on Windows\nIf your computer uses Windows, you can install Juliaup from the Windows Store.\n\n\nInstalling Juliaup on MacOS\nIf you have a Mac, open a terminal (such as the Terminal app) and enter:\ncurl -fsSL https://install.julialang.org | sh\n\n\nInstalling Julia 1.9\nOnce you install Juliaup, install Julia version 1.9 by opening a terminal (in MacOS or Linux) or the command line (in Windows) and entering:\njulia add 1.9\njulia default 1.9\nThis will install Julia 1.9 and make it the default version, which should maximize package compatibility throughout this course. Going forward, if you want to add new versions or change the default, you can follow the Juliaup instructions.\n\n\nMore Resources\nSee this tutorial for more information on using Julia."
  },
  {
    "objectID": "setup.html#install-and-set-up-vs-code",
    "href": "setup.html#install-and-set-up-vs-code",
    "title": "Setting Up Your Computer",
    "section": "Install and Set Up VS Code",
    "text": "Install and Set Up VS Code\nYou can skip this section if you already have an IDE you like; just set it up to work with Julia. Otherwise, VS Code is as close to an officially supported editor for Julia as you can get. We will follow this guide for setting up VS Code with Julia.\n\nInstalling VS Code\nYou can download it here; open the downloaded file to install. Make sure to select the correct version for your operating system. If you have a recent Apple mac, make sure to check whether you have an Intel or Apple chip.\n\n\nInstall the Julia Extension\n\nOpen VS Code.\nSelect View and click Extensions to open the Extension View.\nSearch for julia in the search box. Click the green install button.\nRestart VS Code once the installation is complete. It should automatically find your Julia installation; reach out if not.\n\nThe Julia VS Code extension offers you some nice features. You can start a REPL (an interactive Julia coding environment) by opening the ‚ÄúCommand Palette‚Äù (View -&gt; Command Palette, or CTRL/CMD+SHIFT+P) and typing ‚ÄúREPL‚Äù to bring up ‚ÄúJulia: Start REPL‚Äù. You can also create .jl and .qmd files to write Julia code and execute line by line.\n\n\nMore Resources\nSee this tutorial for more information on using Julia."
  },
  {
    "objectID": "setup.html#set-up-github",
    "href": "setup.html#set-up-github",
    "title": "Setting Up Your Computer",
    "section": "Set Up GitHub",
    "text": "Set Up GitHub\n\nCreate GitHub Account\nIf you already have a GitHub account, you can use that for this course and do not need to create a new account. Otherwise, create an account. It doesn‚Äôt have to be linked to your Rice email or your NetID.\nFor labs and projects, you should use the GitHub Classroom link posted on Canvas to ‚Äúaccept‚Äù the assignment, which will give you your own GitHub repository for that assignment. The first time you click one of these links, you will need to link your place on the course roster with your GitHub account.\n\n\nGitHub Desktop (Optional)\nYou can do almost everything that you will need to do for this course with GitHub directly through VS Code. The GitHub desktop app is also great, or alternatively you may work directly through the terminal if you have prior experience.\n\n\nInstall Git\ngit is a version control software that powers GitHub under the hood (git is the version control software, GitHub is an online platform). Based on past experience with the course, you probably already have git installed. If you‚Äôre not sure if it‚Äôs installed, see instructions here.\n\n\nMore Resources\nSee GitHub official tutorials for more helpful resources and tutorials."
  },
  {
    "objectID": "setup.html#set-up-quarto",
    "href": "setup.html#set-up-quarto",
    "title": "Setting Up Your Computer",
    "section": "Set up Quarto",
    "text": "Set up Quarto\nQuarto combines the best of Jupyter notebooks and R Markdown to create a document format that is ideal for conducting and communicating data science. We will use Quarto to create and share our work in this course; this website is also built using Quarto.||\n\nInstall Quarto\nFollow the directions at https://quarto.org/docs/get-started/ to install Quarto. Be sure to ensure that you have the right version for your operating system.\n\n\nInstall the Quarto Extension for VS Code\nUnder ‚ÄúStep 2‚Äù, click on the VS Code icon.\n\n\nInstall Jupyter\nUnder the hood, Quarto uses Jupyter to run code. You don‚Äôt need to know how Jupyter works or worry about it, because it runs under the hood, but we will need to install it.\n\nIf you have Python installed\nIn your Terminal (open VS Code then open the terminal), run\npython3 -m pip install jupyter\nIf this throws an error, copy and paste the error onto Canvas\n\n\nIf you don‚Äôt have Python installed\n\nInstall Python\n\nWindows: see Microsoft instructions or Conda documentation\nMac/Linux: see Conda documentation\n\nFollow the instructions above\n\n\n\n\nMore Resources\nSee this tutorial for more information on using Quarto."
  },
  {
    "objectID": "setup.html#verify",
    "href": "setup.html#verify",
    "title": "Setting Up Your Computer",
    "section": "Verify",
    "text": "Verify\n\nLab 01 has a section that will help you verify that your setup is working.\nIf you have any trouble:\n\nOpen VS Code\nIn the Terminal, run quarto check\nTry to troubleshoot on your own; if you can‚Äôt, post the results of quarto check to Canvas"
  }
]