[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "CEVE 543: Data Science for Climate Risk Assessment",
    "section": "",
    "text": "This page contains a schedule of the topics, content, and assignments for the semester. This schedule will be updated as necessary over the course of the semester to accommodate logistics and to adapt to student needs.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nModule\n\n\nCategory\n\n\n\n\n\n\nMon., Aug. 21\n\n\nWelcome to CEVE 543!!\n\n\n1\n\n\nLecture\n\n\n\n\nWed., Aug. 23\n\n\nWhat drives uncertain climate hazard?\n\n\n1\n\n\nLecture\n\n\n\n\nFri., Aug. 25\n\n\nSetting up Julia, GitHub, and Quarto\n\n\n1\n\n\nLecture\n\n\n\n\nFri., Aug. 25\n\n\nLab 01\n\n\n1\n\n\nLab\n\n\n\n\nMon., Aug. 28\n\n\nFundamentals of probability distributions and statistics\n\n\n1\n\n\nLecture\n\n\n\n\nWed., Aug. 30\n\n\nMarginal, conditional, and joint distributions\n\n\n1\n\n\nLecture\n\n\n\n\nFri., Sep. 1\n\n\nLab 02\n\n\n1\n\n\nLab\n\n\n\n\nMon., Sep. 4\n\n\nNo Class – Labor Day\n\n\n\n\n\n\n\n\n\n\nWed., Sep. 6\n\n\nLikelihood and maximum likelihood estimation\n\n\n1\n\n\nLecture\n\n\n\n\nFri., Sep. 8\n\n\nLab 03\n\n\n1\n\n\nLab\n\n\n\n\nMon., Sep. 11\n\n\nParametric uncertainty and Monte Carlo\n\n\n1\n\n\nLecture\n\n\n\n\nWed., Sep. 13\n\n\nBayesian statistics and computation\n\n\n1\n\n\nLecture\n\n\n\n\nFri., Sep. 15\n\n\nLab 04\n\n\n1\n\n\nLab\n\n\n\n\nMon., Sep. 18\n\n\nNo Class\n\n\n\n\n\n\n\n\n\n\nWed., Sep. 20\n\n\nModule I Review\n\n\n1\n\n\n\n\n\n\n\nFri., Sep. 22\n\n\nModule I Exam\n\n\n1\n\n\n\n\n\n\n\nMon., Sep. 25\n\n\nModule 2 motivation: downscaling\n\n\n2\n\n\nLecture\n\n\n\n\nWed., Sep. 27\n\n\nIn-class workshop\n\n\n\n\n\n\n\n\n\n\nFri., Sep. 29\n\n\nModule 1 reflection\n\n\n1\n\n\nLecture\n\n\n\n\nMon., Oct. 2\n\n\nLab 05\n\n\n2\n\n\nLab\n\n\n\n\nWed., Oct. 4\n\n\nGeneralized Linear Models\n\n\n2\n\n\nLecture\n\n\n\n\nFri., Oct. 6\n\n\nIntro to Machine Learning\n\n\n2\n\n\nLecture\n\n\n\n\nMon., Oct. 9\n\n\nNo Class – Fall Break\n\n\n\n\n\n\n\n\n\n\nWed., Oct. 11\n\n\nPrincipal Components Analysis (PCA)\n\n\n2\n\n\nLecture\n\n\n\n\nFri., Oct. 13\n\n\nLab 06\n\n\n2\n\n\nLab\n\n\n\n\nWed., Oct. 18\n\n\nRandom Forest Models\n\n\n2\n\n\nLecture\n\n\n\n\nFri., Oct. 20\n\n\nHyperparameter tuning\n\n\n2\n\n\nLecture\n\n\n\n\nMon., Oct. 23\n\n\nProject Workshop\n\n\n2\n\n\n\n\n\n\n\nWed., Oct. 25\n\n\nModule II Review\n\n\n2\n\n\n\n\n\n\n\nFri., Oct. 27\n\n\nModule II Exam\n\n\n2\n\n\n\n\n\n\n\nMon., Oct. 30\n\n\nIntro to Frequency Analysis\n\n\n3\n\n\nLecture\n\n\n\n\nWed., Nov. 1\n\n\nExtreme Value Statistics\n\n\n3\n\n\nLecture\n\n\n\n\nFri., Nov. 3\n\n\nLab 07\n\n\n3\n\n\nLab\n\n\n\n\nMon., Nov. 6\n\n\nExtreme Value Theory and Models\n\n\n3\n\n\nLecture\n\n\n\n\nWed., Nov. 8\n\n\nNonstationary GEV\n\n\n3\n\n\nLecture\n\n\n\n\nMon., Nov. 13\n\n\nRegionalization and Spatial Pooling\n\n\n3\n\n\nLecture\n\n\n\n\nWed., Nov. 15\n\n\nQuantitative and Graphical Model Selection\n\n\n3\n\n\nPlanned\n\n\n\n\nFri., Nov. 17\n\n\nLab 08: \\(K\\)-Means Clustering\n\n\n3\n\n\nLab\n\n\n\n\nMon., Nov. 20\n\n\nTBD\n\n\n3\n\n\nLecture\n\n\n\n\nWed., Nov. 22\n\n\nNo Class – Thanksgiving Week\n\n\n\n\n\n\n\n\n\n\nFri., Nov. 24\n\n\nNo Class – Thanksgiving Week\n\n\n\n\n\n\n\n\n\n\nMon., Nov. 27\n\n\nProject Workshop\n\n\n3\n\n\nPlanned\n\n\n\n\nWed., Nov. 29\n\n\nModule III Review\n\n\n3\n\n\nPlanned\n\n\n\n\nFri., Dec. 1\n\n\nModule III Exam\n\n\n3\n\n\nPlanned\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "recommended_reading.html",
    "href": "recommended_reading.html",
    "title": "Recommended Reading",
    "section": "",
    "text": "There are lots of great resources beyond this website. Here are some especially good ones."
  },
  {
    "objectID": "recommended_reading.html#programming",
    "href": "recommended_reading.html#programming",
    "title": "Recommended Reading",
    "section": "Programming",
    "text": "Programming\n\nNazarathy & Klok (2021) offers tutorials on using Julia for statistics and machine learning; reading good code is a great way to improve your coding\nLauwens & Downey (2019) teaches key concepts in programming, using Julia for all examples. It’s a great resource for beginners trying to understand code and for more experienced programmers who want to write better code."
  },
  {
    "objectID": "recommended_reading.html#fundamentals",
    "href": "recommended_reading.html#fundamentals",
    "title": "Recommended Reading",
    "section": "Fundamentals",
    "text": "Fundamentals\n\nBlitzstein & Hwang (2019) provides a thorough introduction to key concepts and ideas in probability. The book accompanies a free online course, Stat 110, which is a great resource for learning probability and statistics. Practice problems and solutions, handouts, and lecture videos are all available online.\nDowney (2021) offers an introduction to Bayesian statistics using computational methods. It’s not environment focused but provides code and a clear explanation of core concepts."
  },
  {
    "objectID": "recommended_reading.html#digging-deeper",
    "href": "recommended_reading.html#digging-deeper",
    "title": "Recommended Reading",
    "section": "Digging deeper",
    "text": "Digging deeper\n\nGelman et al. (2014) is a classical and detailed textbook on Bayesian inference.\nGelman (2021) is a less technical textbook with clear and well-worked examples (mostly not environmental).\nFriedman et al. (2001) is a classic introduction to machine learning, which complements the Bayesian perspective nicely\nCressie & Wikle (2011) provides a detailed exploration of hierarchical space-time models. There have been some computational advances since then that are worth keeping in mind before you apply these models directly, but it’s a clearly written and overview.\nThe documentation for the Turing, PyMC, and (especially) stan probabilistic programming languages offer outstanding tutorials on statistical modeling"
  },
  {
    "objectID": "recommended_reading.html#environmental-applications",
    "href": "recommended_reading.html#environmental-applications",
    "title": "Recommended Reading",
    "section": "Environmental applications",
    "text": "Environmental applications\n\nHelsel et al. (2020) is a textbook also produced by the USGS covering some general statistical methods with a water resources focus. This book is very focused on hydrological statistics, hypothesis testing, and frequency estimates, but there are still some valuable insights."
  },
  {
    "objectID": "recommended_reading.html#references",
    "href": "recommended_reading.html#references",
    "title": "Recommended Reading",
    "section": "References",
    "text": "References\n\n\nBlitzstein, J. K., & Hwang, J. (2019). Introduction to Probability, Second Edition (2nd Edition). Boca Raton: Chapman and Hall/CRC. Retrieved from http://probabilitybook.net\n\n\nCressie, N. A. C., & Wikle, C. K. (2011). Statistics for spatio-temporal data. Hoboken, N.J.: Wiley.\n\n\nDowney, A. B. (2021). Think Bayes. \"O’Reilly Media, Inc.\". Retrieved from https://allendowney.github.io/ThinkBayes2/\n\n\nFriedman, J., Hastie, T., & Tibshirani, R. (2001). The Elements of Statistical Learning (Vol. 1). Springer series in statistics Springer, Berlin.\n\n\nGelman, A. (2021). Regression and other stories. Cambridge, United Kingdom ; Cambridge University Press.\n\n\nGelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (2014). Bayesian Data Analysis (3rd ed.). Chapman & Hall/CRC Boca Raton, FL, USA.\n\n\nHelsel, D. R., Hirsch, R. M., Ryberg, K. R., Archfield, S. A., & Gilroy, E. J. (2020). Statistical methods in water resources. Techniques and Methods. U.S. Geological Survey. https://doi.org/10.3133/tm4A3\n\n\nLauwens, B., & Downey, A. B. (2019). Think Julia: How to think like a computer scientist. O’Reilly Media.\n\n\nNazarathy, Y., & Klok, H. (2021). Statistics with Julia: Fundamentals for Data Science, Machine Learning and Artificial Intelligence. Springer International Publishing. https://doi.org/10.1007/978-3-030-70901-3"
  },
  {
    "objectID": "slides/2023-08-21.html#about-me",
    "href": "slides/2023-08-21.html#about-me",
    "title": "Welcome to CEVE 543!!",
    "section": "About me",
    "text": "About me\n\n\n\n\nDr. James Doss-Gollin\nAssistant professor in CEVE\nInterested in bridging Earth science, data science, and decision science to improve climate risk management and long-term infrastructure planning\nHometown: New Haven, CT (❤️ for Houston, NYC, and Luque, Paraguay)\nDoss-Gollin lab\n\n\n\n\n\n\nJames Doss-Gollin\n\n\n\n\n\nOffice hours MWF TBD"
  },
  {
    "objectID": "slides/2023-08-21.html#ta",
    "href": "slides/2023-08-21.html#ta",
    "title": "Welcome to CEVE 543!!",
    "section": "TA",
    "text": "TA\n\n\n\n\n\nYuchen Lu\n\n\n\n\nYuchen Lu\nThird year Ph.D. student in CEVE\nCurrently researching statistical methods to estimate the probability of extreme precipitation\nHometown: Wuhan, China (via Pittsburgh)\n\n\n\n\nYuchen will mainly help me with grading, but you can also reach out to her with questions"
  },
  {
    "objectID": "slides/2023-08-21.html#your-turn",
    "href": "slides/2023-08-21.html#your-turn",
    "title": "Welcome to CEVE 543!!",
    "section": "Your turn!",
    "text": "Your turn!\n\nYour name\nYour field and program of study\nYour hometown(s)"
  },
  {
    "objectID": "slides/2023-08-21.html#what-is-one-thing-you-hope-to-learn-this-semester",
    "href": "slides/2023-08-21.html#what-is-one-thing-you-hope-to-learn-this-semester",
    "title": "Welcome to CEVE 543!!",
    "section": "What is one thing you hope to learn this semester?",
    "text": "What is one thing you hope to learn this semester?\n\nTake a moment to think, write it down, and then we’ll share.\n\n\n\nGood to get a sense of background and goals\nInvite students to share – this will help me get a sense of how forthcoming they are"
  },
  {
    "objectID": "slides/2023-08-21.html#floods-in-paraguay-2015",
    "href": "slides/2023-08-21.html#floods-in-paraguay-2015",
    "title": "Welcome to CEVE 543!!",
    "section": "Floods in Paraguay, 2015",
    "text": "Floods in Paraguay, 2015\n\nFigure 1: Municipalidad de Asunción"
  },
  {
    "objectID": "slides/2023-08-21.html#tx-cold-snap-2021",
    "href": "slides/2023-08-21.html#tx-cold-snap-2021",
    "title": "Welcome to CEVE 543!!",
    "section": "TX Cold Snap, 2021",
    "text": "TX Cold Snap, 2021\n\n\n\n\n\n\n\n\n\n\nFigure 2: Go Nakamura for Getty Images"
  },
  {
    "objectID": "slides/2023-08-21.html#what-other-climate-hazards-do-you-know-about",
    "href": "slides/2023-08-21.html#what-other-climate-hazards-do-you-know-about",
    "title": "Welcome to CEVE 543!!",
    "section": "What other climate hazards do you know about?",
    "text": "What other climate hazards do you know about?\n\nTake a moment to think, write it down, and then we’ll share."
  },
  {
    "objectID": "slides/2023-08-21.html#how-do-we-manage-climate-risks",
    "href": "slides/2023-08-21.html#how-do-we-manage-climate-risks",
    "title": "Welcome to CEVE 543!!",
    "section": "How do we manage climate risks?",
    "text": "How do we manage climate risks?\n\n\nReduce emissions to prevent future climate change (“mitigation”)\nReal-time monitoring and forecasting\nBuilding codes and design standards\nInsurance\nand much more!"
  },
  {
    "objectID": "slides/2023-08-21.html#bayesian-decision-theory",
    "href": "slides/2023-08-21.html#bayesian-decision-theory",
    "title": "Welcome to CEVE 543!!",
    "section": "Bayesian Decision Theory",
    "text": "Bayesian Decision Theory\nExpected reward \\(R\\) (equivalently utility, loss, etc.) for taking some decision \\(a \\in \\mathcal{A}\\): \\[\n\\mathbb{E}(R(a)) = \\int_{\\mathcal{S}} R(a, {\\bf{s}}) p({\\bf{s}}) d{\\bf{s}}\n\\] Crucial insight: \\[\n\\mathbb{E}(R(a)) \\neq R(a, \\mathbb{E}(\\bf{s}))\n\\]"
  },
  {
    "objectID": "slides/2023-08-21.html#implications",
    "href": "slides/2023-08-21.html#implications",
    "title": "Welcome to CEVE 543!!",
    "section": "Implications",
    "text": "Implications\n\n\nWe often care about extremes\nUncertainty (especially of extremes) matters\nWhat makes a “good” estimates of \\(p(\\bf{s})\\)?\n\nPhysically accurate / realistic\nHigh spatial and temporal resolution to quantify impacts on people and infrastructure\nLarge ensemble sizes to quantify uncertainty\nMultiple scenarios (of “deep” uncertainties)"
  },
  {
    "objectID": "slides/2023-08-21.html#fat-tails",
    "href": "slides/2023-08-21.html#fat-tails",
    "title": "Welcome to CEVE 543!!",
    "section": "Fat tails",
    "text": "Fat tails\n\nBonnafous et al. (2017)"
  },
  {
    "objectID": "slides/2023-08-21.html#quasi-periodic-oscillations",
    "href": "slides/2023-08-21.html#quasi-periodic-oscillations",
    "title": "Welcome to CEVE 543!!",
    "section": "Quasi-periodic oscillations",
    "text": "Quasi-periodic oscillations\n\nDoss-Gollin et al. (2019)"
  },
  {
    "objectID": "slides/2023-08-21.html#nonstationarity",
    "href": "slides/2023-08-21.html#nonstationarity",
    "title": "Welcome to CEVE 543!!",
    "section": "Nonstationarity",
    "text": "Nonstationarity\n\nFagnant et al. (2020)"
  },
  {
    "objectID": "slides/2023-08-21.html#vary-on-multiple-temporal-scales",
    "href": "slides/2023-08-21.html#vary-on-multiple-temporal-scales",
    "title": "Welcome to CEVE 543!!",
    "section": "Vary on multiple temporal scales",
    "text": "Vary on multiple temporal scales\n\nDoss-Gollin et al. (2019)"
  },
  {
    "objectID": "slides/2023-08-21.html#spatial-structure",
    "href": "slides/2023-08-21.html#spatial-structure",
    "title": "Welcome to CEVE 543!!",
    "section": "Spatial structure",
    "text": "Spatial structure\n\nFarnham et al. (2018)"
  },
  {
    "objectID": "slides/2023-08-21.html#emphasis-on-extremes",
    "href": "slides/2023-08-21.html#emphasis-on-extremes",
    "title": "Welcome to CEVE 543!!",
    "section": "Emphasis on extremes",
    "text": "Emphasis on extremes\n\nDoss-Gollin & Keller (2023)"
  },
  {
    "objectID": "slides/2023-08-21.html#deep-uncertainty",
    "href": "slides/2023-08-21.html#deep-uncertainty",
    "title": "Welcome to CEVE 543!!",
    "section": "Deep Uncertainty",
    "text": "Deep Uncertainty\n\nWalker et al. (2013)"
  },
  {
    "objectID": "slides/2023-08-21.html#syllabus",
    "href": "slides/2023-08-21.html#syllabus",
    "title": "Welcome to CEVE 543!!",
    "section": "Syllabus",
    "text": "Syllabus\nSyllabus"
  },
  {
    "objectID": "slides/2023-08-21.html#lectures",
    "href": "slides/2023-08-21.html#lectures",
    "title": "Welcome to CEVE 543!!",
    "section": "Lectures",
    "text": "Lectures\n\nTests will cover material from lectures and labs\n\nSlides will be posted ahead of time on course website (see instructions for printing to PDF)\n\nOccasional readings (assigned ahead of time on Canvas)\nI am not a mind reader! Ask questions."
  },
  {
    "objectID": "slides/2023-08-21.html#labs-10",
    "href": "slides/2023-08-21.html#labs-10",
    "title": "Welcome to CEVE 543!!",
    "section": "Labs (10%)",
    "text": "Labs (10%)\n\nBuild your hands-on computational skills\nMost weeks, generally Friday\nApply conecpts from lectures to simple problems\nGraded on a 3 point scale\nDue one week after the in-class lab; solutions will be posted and discussed so no late submissions (turn in what you have)"
  },
  {
    "objectID": "slides/2023-08-21.html#tests-40",
    "href": "slides/2023-08-21.html#tests-40",
    "title": "Welcome to CEVE 543!!",
    "section": "Tests (40%)",
    "text": "Tests (40%)\n\nMaterial from lecture, assigned readings, and labs\nAlways a review session"
  },
  {
    "objectID": "slides/2023-08-21.html#projects-40",
    "href": "slides/2023-08-21.html#projects-40",
    "title": "Welcome to CEVE 543!!",
    "section": "Projects (40%)",
    "text": "Projects (40%)\n\nApply concepts from class and lab to a more complex and open-ended problem\nEach module (except intro) centers on a project\n\nThree rainfall-focused projects planned:\n\nDownscaling\nFrequency analysis\nWeather typing"
  },
  {
    "objectID": "slides/2023-08-21.html#participation-10",
    "href": "slides/2023-08-21.html#participation-10",
    "title": "Welcome to CEVE 543!!",
    "section": "Participation (10%)",
    "text": "Participation (10%)\nSome ways to participate include:\n\nAttending every class\nAsking questions in class\nAnswering questions on Canvas\nComing to office hours\n\nWe will co-grade your participation for every module"
  },
  {
    "objectID": "slides/2023-08-21.html#job-opportunities",
    "href": "slides/2023-08-21.html#job-opportunities",
    "title": "Welcome to CEVE 543!!",
    "section": "Job opportunities",
    "text": "Job opportunities\nGrowing climate analytics opportunities in:\n\nInsurance\nFinance\nAgriculture\nEngineering\n\nAnd more! These require an understanding of climate, probability, statistics, coding, and communication."
  },
  {
    "objectID": "slides/2023-08-21.html#pre-requistes-linear-algebra",
    "href": "slides/2023-08-21.html#pre-requistes-linear-algebra",
    "title": "Welcome to CEVE 543!!",
    "section": "Pre-requistes: Linear Algebra",
    "text": "Pre-requistes: Linear Algebra\nYou need basic matrix notation and multiplication, but not much more. Let \\[\nA = \\left[ \\begin{matrix} a & b \\\\ c & d \\end{matrix} \\right], \\quad\nB = \\left[ \\begin{matrix} e & f \\\\ g & h \\end{matrix} \\right], \\quad\nx = \\left[ \\begin{matrix} k \\\\ \\ell \\end{matrix} \\right], \\quad\n\\]\nYou should be able to (with note-checking as needed!) figure out:\n\n\n\n\\(A_{2,1}\\)\n\\(A + B\\)\n\\(AB\\)\n\n\n\n\\(A x\\)\n\\(x^T x\\)\n\\(x x^T\\)"
  },
  {
    "objectID": "slides/2023-08-21.html#pre-requisites-probability-and-statistics",
    "href": "slides/2023-08-21.html#pre-requisites-probability-and-statistics",
    "title": "Welcome to CEVE 543!!",
    "section": "Pre-requisites: Probability and Statistics",
    "text": "Pre-requisites: Probability and Statistics\nYou should have a course in applied statistics. You should be able to:\n\nCompute summary statistics of a sample\nDefine joint, marginal, and conditional distributions\nUnderstand probability density functions, quantiles, and cumulative distribution functions\nExplain a few probability distributions and where they are appropriate\nPerform and interpret linear regressions"
  },
  {
    "objectID": "slides/2023-08-21.html#pre-requisites-coding",
    "href": "slides/2023-08-21.html#pre-requisites-coding",
    "title": "Welcome to CEVE 543!!",
    "section": "Pre-requisites: Coding",
    "text": "Pre-requisites: Coding\nWe will use the Julia programming language. I think you’ll find it easy and fun to learn!\n\nNo experience in Julia is expected\nSome prior experience coding (R, Python, Matlab, C, etc.) is suggested\nIf you have no prior coding experience, you will need to put in extra effort to learn the basics"
  },
  {
    "objectID": "slides/2023-08-21.html#questions",
    "href": "slides/2023-08-21.html#questions",
    "title": "Welcome to CEVE 543!!",
    "section": "Questions?",
    "text": "Questions?\n\nWednesday: “What drives uncertain climate hazard?”\nFriday: “Lab 01: Setting up Julia, GitHub, and Quarto”"
  },
  {
    "objectID": "slides/2023-08-21.html#references",
    "href": "slides/2023-08-21.html#references",
    "title": "Welcome to CEVE 543!!",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nBonnafous, L., Lall, U., & Siegel, J. (2017). A water risk index for portfolio exposure to climatic extremes: Conceptualization and an application to the mining industry. Hydrology and Earth System Sciences, 21(4), 2075–2106. https://doi.org/f96k67\n\n\nDoss-Gollin, J., & Keller, K. (2023). A subjective Bayesian framework for synthesizing deep uncertainties in climate risk management. Earth’s Future, 11(1). https://doi.org/10.1029/2022EF003044\n\n\nDoss-Gollin, J., Farnham, D. J., Steinschneider, S., & Lall, U. (2019). Robust adaptation to multiscale climate variability. Earth’s Future, 7(7), 734–747. https://doi.org/10.1029/2019ef001154\n\n\nFagnant, C., Gori, A., Sebastian, A., Bedient, P. B., & Ensor, K. B. (2020). Characterizing spatiotemporal trends in extreme precipitation in Southeast Texas. Natural Hazards, 104(2), 1597–1621. https://doi.org/10.1007/s11069-020-04235-x\n\n\nFarnham, D. J., Doss-Gollin, J., & Lall, U. (2018). Regional extreme precipitation events: Robust inference from credibly simulated GCM variables. Water Resources Research, 54(6). https://doi.org/10.1002/2017wr021318\n\n\nWalker, W. E., Lempert, R. J., & Kwakkel, J. H. (2013). Deep Uncertainty. In S. I. Gass & M. C. Fu (Eds.), Encyclopedia of Operations Research and Management Science (pp. 395–402). Boston, MA: Springer US. https://doi.org/10.1007/978-1-4419-1153-7_1140"
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#a-quick-note-on-pacing",
    "href": "slides/2023-08-28-fundamentals.html#a-quick-note-on-pacing",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "A quick note on pacing",
    "text": "A quick note on pacing\nWe will move through this module (“fundamentals”) at a fairly brisk pace\n\nReview course slides\nAsk questions on Canvas or in office hours\nTo help you learn to code, I am exposing you to code early and often\n\nI don’t expect that you are able to replicate all the code in this notebook\nI have added annotations where appropriate\nThe labs will give you practice\nYou will not need to write code from scratch for the exams\n\n\n\nThis is a long slide deck. We will probably finish on Wednesday"
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#what-is-a-package",
    "href": "slides/2023-08-28-fundamentals.html#what-is-a-package",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "What is a package?",
    "text": "What is a package?\n\nCode that is bundled for easy use\nProvides functionality that is not part of the base language\n\nMost stuff in Julia requires packages, as we will see\n\nNeed to be installed\nDeveloped by the community"
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#where-do-i-get-packages",
    "href": "slides/2023-08-28-fundamentals.html#where-do-i-get-packages",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Where do I get packages?",
    "text": "Where do I get packages?\n\nJulia has a built-in package manager for installing add-on functionality written in Julia. It can also install external libraries using your operating system’s standard system for doing so, or by compiling from source."
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#where-are-packages-stored",
    "href": "slides/2023-08-28-fundamentals.html#where-are-packages-stored",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Where are packages stored?",
    "text": "Where are packages stored?\nEach project has an *environment, which is defined by the following files (do not edit them manually):\n\nProject.toml: lists the specified dependencies of the project\nManifest.toml: lists the exact versions of the packages that are used in the project\n\nThe actual packages are stored on your computer and you don’t need to worry"
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#workflow-activate",
    "href": "slides/2023-08-28-fundamentals.html#workflow-activate",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Workflow: activate",
    "text": "Workflow: activate\nWe activate a project to tell Julia that we want to use the packages in that project. These steps are equivalent:\n\n\n\nOpen the REPL\nusing Pkg\nPkg.activate(\".\")\n\n\n\nOpen the REPL\nPress ] to enter the package manager\nactivate ."
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#workflow-install",
    "href": "slides/2023-08-28-fundamentals.html#workflow-install",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Workflow: install",
    "text": "Workflow: install\nWe add a package to install it in the current project\n\n\n\nOpen the REPL\nusing Pkg\nPkg.add(\"DataFrames\")\n\n\n\nOpen the REPL\nPress ] to enter the package manager\nadd DataFrames"
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#workflow-instantiate",
    "href": "slides/2023-08-28-fundamentals.html#workflow-instantiate",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Workflow: instantiate",
    "text": "Workflow: instantiate\nWhen working with someone else’s project, we need to install the packages that they use.\n\nactivate does not install anything, just tells Julia which packages to use\ninstantiate is your friend to make sure an environment is ready to use. If there’s nothing to do, instantiate does nothing."
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#learn-more",
    "href": "slides/2023-08-28-fundamentals.html#learn-more",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Learn more",
    "text": "Learn more\n\nPkg.jl docs\n\nSee “Using someone else’s projecg” for more on instantiate\n\nWell-worked blog post by Julies Krumbiegel"
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#lab-01-issues",
    "href": "slides/2023-08-28-fundamentals.html#lab-01-issues",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Lab 01 issues",
    "text": "Lab 01 issues\n\n\nBe sure to submit your assignment\nCanvas discussion: “Lab 01 Discussion”\nERROR: Jupyter kernel 'julia-1.9' not found. x4"
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#lab-01-fix",
    "href": "slides/2023-08-28-fundamentals.html#lab-01-fix",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Lab 01 fix",
    "text": "Lab 01 fix\n\nIn order to run codes using Quarto, you need the IJulia package\n\nListed in Manifest.toml but you need to instantiate\n\nIf that doesn’t work:\n\nRun Pkg.build(\"IJulia\") in the REPL (after you activate and instantiate)\n\nI’ve updated the instructions"
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#stats-without-the-agonizing-details",
    "href": "slides/2023-08-28-fundamentals.html#stats-without-the-agonizing-details",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Stats without the agonizing details",
    "text": "Stats without the agonizing details\nIn this class we will use computation and simulation to build fundamental insight into statistical processes without dwelling on “agonizing” details."
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#motivating-question",
    "href": "slides/2023-08-28-fundamentals.html#motivating-question",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Motivating question",
    "text": "Motivating question\n\nDoes drinking beer reduce the likelihood of being bitten by mosquitos?"
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#raw-data",
    "href": "slides/2023-08-28-fundamentals.html#raw-data",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Raw data",
    "text": "Raw data\nCreate a variable called beer to hold the number of mosquito bites for beer drinkers:\n\n\n25-element Vector{Int64}:\n 27\n 20\n 21\n 26\n 27\n 31\n 24\n 21\n 20\n 19\n 23\n 24\n 28\n 19\n 24\n 29\n 18\n 20\n 17\n 31\n 20\n 25\n 28\n 21\n 27"
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#what-is-beer",
    "href": "slides/2023-08-28-fundamentals.html#what-is-beer",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "What is beer?",
    "text": "What is beer?\nWe can learn a bit more about it:\n\n\n\nVector{Int64} (alias for Array{Int64, 1})\n\n\n\n\n\n25\n\n\n\n\n(25,)\n\n\n\n\n23.6"
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#more-raw-data",
    "href": "slides/2023-08-28-fundamentals.html#more-raw-data",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "More raw data",
    "text": "More raw data\nWe can do the same for water drinkers:\n\nBy putting the ; at the end of our statement, we keep the notebook from showing the output"
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#a-simple-analysis",
    "href": "slides/2023-08-28-fundamentals.html#a-simple-analysis",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "A simple analysis",
    "text": "A simple analysis\nLet’s calculate the difference between the average number of bites in each group.\n\n\n4.37777777777778\n\n\n\nThis gives us the mean function from the StatsBase package"
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#the-skeptics-argument",
    "href": "slides/2023-08-28-fundamentals.html#the-skeptics-argument",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "The skeptic’s argument",
    "text": "The skeptic’s argument\nThe skeptic asks whether this might be random chance.\n\n\nWe could answer this with a T test\n\nDetermine if there is a significant difference between the means of two groups\nAssumes (approximate) normality\nAssumptions hidden behind a software package\n\nSimulation approach:\n\nSuppose the skeptic is right – the two groups are samped from the same distribution\nShuffle the data (randomly divide into two groups by assuming that there is no difference between the two groups)\nCalculate the difference between each group\nRepeat many times and examine the distribution of differences\n\n\n\n\nMake clear that we want to relax our assumptions and to simplify the analysis. Some of the things T tests make you think about, like whether data is paired or not, whether the variances are equal, whether you want one or two direction, etc are important and figure into the design of your simulation approach."
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#implementation",
    "href": "slides/2023-08-28-fundamentals.html#implementation",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Implementation",
    "text": "Implementation\n\n\n1.224444444444444\n\n\n\nUse the shuffle function from the Random package\nDefine a function. Its arguments are y1 and y2\nend closes the function definition\nCall the function with our data"
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#running",
    "href": "slides/2023-08-28-fundamentals.html#running",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Running",
    "text": "Running\nWe want to learn about the sampling distribution of the group differences: repeat this experiment many times over and plott the results\n\n\n50000\n\n\n\nThis is a list comprehension. It’s a way to create a list by looping over something. Here, we loop over the numbers 1 to 50,000 and call get_shuffled_difference each time.\nlength tells us the size of a vector"
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#plotting",
    "href": "slides/2023-08-28-fundamentals.html#plotting",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Plotting",
    "text": "Plotting\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe need the Plots package to make plots\nDefine a function. Its arguments are diffs and obs\nhistogram is a function from the Plots package\nCreate a histogram using the diffs object. ; separates the positional arguments from the keyword arguments\nxlabel is a “keyword argument” specifying the text for the x-axis label\nthe y-axis label\nthe label to use in the legend\nspecify the bins to use in the histogram\nspecify the location of the legend\nnormalize the histogram so that the area under the curve is 1\nadd a vertical line (vline!) at the observed difference\nmany functions return their output – in this case the plot we created from the inputs"
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#alternative",
    "href": "slides/2023-08-28-fundamentals.html#alternative",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Alternative",
    "text": "Alternative\nWe could have done this with a parametric test\n\nWe need the HypothesisTests package\nWe don’t need to include the HypothesisTests., but it adds clarity\nRecall: ; suppresses output\n\n\n\n\n\nTwo sample t-test (equal variance)\n----------------------------------\nPopulation details:\n    parameter of interest:   Mean difference\n    value under h_0:         0\n    point estimate:          4.37778\n    95% confidence interval: (1.913, 6.843)\n\nTest summary:\n    outcome with 95% confidence: reject h_0\n    two-sided p-value:           0.0009\n\nDetails:\n    number of observations:   [25,18]\n    t-statistic:              3.5869843832143413\n    degrees of freedom:       41\n    empirical standard error: 1.220461900604875\n\n\n\n\n\nTwo sample t-test (unequal variance)\n------------------------------------\nPopulation details:\n    parameter of interest:   Mean difference\n    value under h_0:         0\n    point estimate:          4.37778\n    95% confidence interval: (1.957, 6.798)\n\nTest summary:\n    outcome with 95% confidence: reject h_0\n    two-sided p-value:           0.0007\n\nDetails:\n    number of observations:   [25,18]\n    t-statistic:              3.658244539721401\n    degrees of freedom:       39.11341478045414\n    empirical standard error: 1.196688119190407"
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#discussion",
    "href": "slides/2023-08-28-fundamentals.html#discussion",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Discussion",
    "text": "Discussion\n\n\nThis is called a bootstrap and is a very powerful tool in many situations\nWhat would we expect to see if the skeptic was correct?\nP-value:\n\nthe likelihood of the data if the null hypothesis is correct\nskeptic’s (null) hypothesis: no difference between groups\n\n\n\n\n\n\n0.00052\n\n\n\n. is the dot operator. It applies the function to each element of the vector individually."
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#the-normal-distribution",
    "href": "slides/2023-08-28-fundamentals.html#the-normal-distribution",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "The Normal distribution",
    "text": "The Normal distribution\nThe Normal (Gaussian) distribution has probability distribution function:\n\\[\np(y | \\mu, \\theta) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\n\\left(\n-\\frac{1}{2}\\left(\n\\frac{x-\\mu}{\\sigma}\n\\right)^{\\!2}\n\\,\n\\right)\n\\]\n\nMean \\(\\mu\\)\n\nMedian equal to mean\n\nVariance \\(\\sigma^2\\)\nSymmetric"
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#central-limit-theorem",
    "href": "slides/2023-08-28-fundamentals.html#central-limit-theorem",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Central limit theorem",
    "text": "Central limit theorem\nThe central limit theorem says that the sum of many independent random variables is approximately normally distributed.\nWe can see this with an example:\n\nFor each sample \\(i = 1, \\ldots, N\\):\n\nDraw J draws from a non-Gaussian distribution \\(\\mathcal{D}\\)\nTake the mean and save it as \\(\\bar{y}_i\\)\n\nPlot the distribution of \\(\\bar{y}_i\\)\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo type ȳ, type y then type \\bar and hit tab. Julia allows unicode (or emojis) in variable names\nTo type ∈ , type \\in and hit tab. The _ isn’t doing anything special and we could name it i or 😶 or whatever we want but _ suggests it’s a throwaway\nThis is another list comprehension"
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#notation",
    "href": "slides/2023-08-28-fundamentals.html#notation",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Notation",
    "text": "Notation\nWe will get tired of writing\n\\[\np(y | \\mu, \\theta) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\n\\left(\n-\\frac{1}{2}\\left(\n\\frac{x-\\mu}{\\sigma}\n\\right)^{\\!2}\n\\,\n\\right)\n\\]\nInstead, we will often use shorthand:\n\\[\ny \\sim \\mathcal{N}(\\mu, \\sigma^2)\n\\]"
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#normal-pdf",
    "href": "slides/2023-08-28-fundamentals.html#normal-pdf",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Normal PDF",
    "text": "Normal PDF\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nL\"&lt;string&gt;\" allows us to use LaTeX in strings\nThis notation specifies the values of \\(\\mu\\) and \\(\\sigma\\)"
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#bernoulli-distribution",
    "href": "slides/2023-08-28-fundamentals.html#bernoulli-distribution",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Bernoulli distribution",
    "text": "Bernoulli distribution\nA Bernoulli distribution models a coin flip.\n\n\n5-element Vector{Bool}:\n 0\n 0\n 0\n 0\n 0\n\n\n\nDraw 5 samples from the Bernoulli distribution with parameter p"
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#binomial-distribution",
    "href": "slides/2023-08-28-fundamentals.html#binomial-distribution",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Binomial distribution",
    "text": "Binomial distribution\nA Binomial distribution models the distribution of n consecutive flips of the same coin\n\n\n5-element Vector{Int64}:\n 3\n 4\n 5\n 1\n 3"
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#multinomial-distribution",
    "href": "slides/2023-08-28-fundamentals.html#multinomial-distribution",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Multinomial distribution",
    "text": "Multinomial distribution\nThe Multinomial extends the Binomial to multiple categories. Note that p is a vector. If there are 2 categories (\\(K=2\\)), it’s just the binomial with \\(p_\\text{multinomial} = [p, 1-p]\\).”\n\n\n3×5 Matrix{Int64}:\n 3  3  0  4  2\n 2  0  0  1  1\n 0  2  5  0  2\n\n\n\nTo be more concise, we could write rand(Multinimial([0.5, 0.3, 0.2], 5), 5). Which is more readable?"
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#poisson-distribution",
    "href": "slides/2023-08-28-fundamentals.html#poisson-distribution",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Poisson distribution",
    "text": "Poisson distribution\nThe Poisson distribution is used to model count data. It is the limit of a Binomial distribution with \\(p=\\lambda/N\\), as \\(N \\rightarrow \\infty\\).\nA Poisson distribution has mean and variance equal to \\(\\lambda\\).\n\n\n10-element Vector{Int64}:\n 1\n 4\n 4\n 1\n 3\n 3\n 3\n 2\n 2\n 4\n\n\n\nThe Poisson distribution has one parameter, \\(\\lambda\\)\nDraw 10 samples from the Poisson distribution"
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#negative-binomial-distribution",
    "href": "slides/2023-08-28-fundamentals.html#negative-binomial-distribution",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Negative binomial distribution",
    "text": "Negative binomial distribution\nThe NegativeBinomaial distribution relaxes the Poisson’s assumotion that \\(\\text{mean} = \\text{variance}\\).\nThis distribution models the number of successes in a sequence of independent and identically distributed Bernoulli trials with probability p before a specified (non-random) number of failures (r) occurs. For example, we can define rolling a 6 on a dice as a failure, and rolling any other number as a success, and ask how many successful rolls will occur before we see the third failure (p = 1/6 and r = 3)."
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#what-other-distributions-do-you-know",
    "href": "slides/2023-08-28-fundamentals.html#what-other-distributions-do-you-know",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "What other distributions do you know?",
    "text": "What other distributions do you know?\n\n\nUniform\nExponential\nGamma (see above)\nBeta\nPareto\nStudent t\nBoltzmann\nMany more!"
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#mean",
    "href": "slides/2023-08-28-fundamentals.html#mean",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Mean",
    "text": "Mean\nThe mean of a sample is just the sample average: \\[\n\\bar{y} = \\frac{1}{N} \\sum_{i=1}^N y_i\n\\]\n\nThe mean of a distribution is the expected value of the distribution: \\[\n\\mathbb{E}(u) = \\int u p(u) \\, du\n\\]"
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#variance",
    "href": "slides/2023-08-28-fundamentals.html#variance",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Variance",
    "text": "Variance\nVariance measures how points differ from the mean\n\nYou may be familiar with sample variance: \\[\nS^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}\n\\]\n\n\nFor a distribution: \\[\n\\mathbb{V}(u) = \\int (u - \\mathbb{E}(u))^2 p(u) \\, du\n\\] or, for a vector \\[\n\\mathbb{V}(u) = \\int (u - \\mathbb{E}(u)) (u - \\mathbb{E}(u))^T p(u) \\, du\n\\]"
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#coming-up",
    "href": "slides/2023-08-28-fundamentals.html#coming-up",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Coming up",
    "text": "Coming up\n\nWednesday: working with probability distributions\nFriday:\n\nLabs” 02: Working with tabular data in Julia\nLab 01 due at start of class"
  },
  {
    "objectID": "slides/2023-08-28-fundamentals.html#office-hours",
    "href": "slides/2023-08-28-fundamentals.html#office-hours",
    "title": "Fundamentals of probability distributions and statistics",
    "section": "Office hours",
    "text": "Office hours\nIf you haven’t filled out the Doodle, please do so ASAP"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#last-time",
    "href": "slides/2023-09-13-bayesian.html#last-time",
    "title": "Bayesian statistics and computation",
    "section": "Last time",
    "text": "Last time\n\nParametric uncertainty matters for decision-making\nAs we collect more data, fewer combinations of parameters are consistent with observations"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#logistics-preview",
    "href": "slides/2023-09-13-bayesian.html#logistics-preview",
    "title": "Bayesian statistics and computation",
    "section": "Logistics preview",
    "text": "Logistics preview\n\nNo class Monday\nToday: focus on the methods and ideas\nMonday, asynchronously, work through the code"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#rare-disease",
    "href": "slides/2023-09-13-bayesian.html#rare-disease",
    "title": "Bayesian statistics and computation",
    "section": "Rare disease",
    "text": "Rare disease\n\n\nEveryone is tested for CEVE543acitis, a rare and deadly disease\nIt is known that 1 in 1,000 people have CEVE543acitis\nThe test is 99% accurate\nYour test comes back positive. What is the probability that you have CEVE543acitis?"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#bayes-rule-discrete-event-version",
    "href": "slides/2023-09-13-bayesian.html#bayes-rule-discrete-event-version",
    "title": "Bayesian statistics and computation",
    "section": "Bayes’ rule: discrete event version",
    "text": "Bayes’ rule: discrete event version\n\\[\n\\Pr \\left\\{ \\theta | y \\right\\} = \\frac{\\Pr \\left\\{ y | \\theta \\right\\} \\Pr \\left\\{ \\theta \\right\\}}{\\Pr \\left\\{ y \\right\\}}\n\\]"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#application-rare-disease",
    "href": "slides/2023-09-13-bayesian.html#application-rare-disease",
    "title": "Bayesian statistics and computation",
    "section": "Application: rare disease",
    "text": "Application: rare disease\nDefine \\(y\\) is getting a positive test result and \\(\\theta\\) is having the underlying condition. Not that we do not observe \\(\\theta\\) directly! Here \\(y=1\\) and we want to know \\(\\Pr\\left\\{\\theta = 1 \\mid y=1 \\right\\}\\).\nLikelihood:\n\n\n\n\n\n\n\n\n\n\n\\(\\Pr\\left\\{y = 1 \\ldots\\right.\\)\n\\(\\Pr\\left\\{y = 0 |\\ldots \\right.\\)\n\n\n\n\n\\(\\left. ...\\theta=1 \\right\\}\\)\n0.99\n0.01\n\n\n\\(\\left. ...\\theta=0\\right\\}\\)\n0.01\n0.99\n\n\n\n\n. . . A naive application of maximum likelihood: \\(\\Pr\\left\\{y=1 \\mid \\theta=1 \\right\\} &gt; \\Pr\\left\\{y=1 \\mid \\theta=0 \\right\\}\\) so best estimate is \\(\\theta=1\\)"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#solving",
    "href": "slides/2023-09-13-bayesian.html#solving",
    "title": "Bayesian statistics and computation",
    "section": "Solving",
    "text": "Solving\nWe are studying \\(\\Pr\\left\\{\\theta = 1 | y = 1 \\right\\}\\).\n\n\nStep 1: \\(\\Pr\\left\\{ y = 1 \\right\\}\\)\n\n\\(\\Pr\\left\\{y=1\\right\\} = \\Pr \\left\\{ y=1, \\theta=0 \\right\\} + \\Pr \\left\\{ y=1, \\theta=1 \\right\\}\\)\n\\(\\Pr\\left\\{y=1\\right\\} = \\Pr \\left\\{ \\theta=0 \\right\\} \\Pr \\left\\{ y = 1 | \\theta=0 \\right\\} + \\Pr \\left\\{ \\theta=1 \\right\\} \\Pr \\left\\{ y=1 | \\theta=0 \\right\\}\\)\n\\(\\underbrace{\\Pr\\left\\{y=1\\right\\}}_\\text{test +} = \\underbrace{0.999}_\\text{don't have it} \\times \\overbrace{0.01}^\\text{false +} + \\underbrace{0.001}_\\text{have it} \\times \\overbrace{0.99}^\\text{true +}\\)\n\nNow plug in to Bayes’ rule\n\n\\(\\Pr\\left\\{ \\theta=1 | y=1 \\right\\} = \\frac{\\Pr\\left\\{y=1 | \\theta=1 \\right\\} \\Pr\\left\\{ \\theta=1 \\right\\}}{\\Pr\\left\\{y=1\\right\\}}\\)\n\\(\\Pr\\left\\{\\theta=1 | y=1 \\right\\} = \\frac{0.99 \\times 0.001}{\\Pr\\left\\{y=1\\right\\}}\\)"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#implementation",
    "href": "slides/2023-09-13-bayesian.html#implementation",
    "title": "Bayesian statistics and computation",
    "section": "Implementation",
    "text": "Implementation\n\naccuracy = 0.99\npr_disease = 0.001 # p(θ = 1)\npr_positive_test = accuracy * pr_disease + (1 - accuracy) * (1 - pr_disease) # p(y = 1)\npr_disease_given_test = accuracy * pr_disease / pr_positive_test # p(θ = 1 | y = 1)\ndisplay(round(pr_positive_test; digits=5))\ndisplay(round(pr_disease_given_test; digits=5))\n\n0.01098\n\n\n0.09016"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#key-idea",
    "href": "slides/2023-09-13-bayesian.html#key-idea",
    "title": "Bayesian statistics and computation",
    "section": "Key idea",
    "text": "Key idea\n\n\nParameters have probability distributions, not single point values\nStart with some prior distribution for parameters\nGoal: what is the distribution of the parameters given the data?"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#bayes-rule-for-distributions",
    "href": "slides/2023-09-13-bayesian.html#bayes-rule-for-distributions",
    "title": "Bayesian statistics and computation",
    "section": "Bayes’ rule for distributions",
    "text": "Bayes’ rule for distributions\n\\[\np(\\theta \\mid y) = \\frac{p(y \\mid \\theta) p(\\theta)}{p(y)}\n\\]\n\nIf we are drawing samples from a distribution, we can calculate up to a constant of proportionality and – since \\(p(y)\\) doesn’t depend on \\(\\theta\\) – we can usually ignore it.\n\\[\n\\overbrace{p(\\theta \\mid y)}^\\rm{posterior} \\propto \\underbrace{p(y \\mid \\theta)}_\\rm{likelihood} \\overbrace{p(\\theta)}^\\rm{prior}\n\\]## Coin flipping\nWe flip a coin a few times. We want to estimate the probability of heads so that we can make well-calibrated bets on future coin tosses.\n\n\n\ncoin_flips = [\"H\", \"H\", \"H\", \"T\", \"H\", \"H\", \"H\", \"H\", \"H\"]\nheads = [flip == \"H\" for flip in coin_flips]\nN = length(coin_flips)\nn_heads = sum(heads)\n\n8"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#maximum-likelihood",
    "href": "slides/2023-09-13-bayesian.html#maximum-likelihood",
    "title": "Bayesian statistics and computation",
    "section": "Maximum likelihood",
    "text": "Maximum likelihood\nMaximum likelihood estimate (MLE) is the most likely value of \\(\\theta\\) given the data. As before, we can use our log-likelihood.\n\n1flip_log_like(θ) = sum(logpdf.(Bernoulli(θ), heads))\nloss(θ) = -flip_log_like(θ)\n2θ_mle = optimize(loss, 0, 1).minimizer\nplot(flip_log_like, 0.1, 1; label=L\"$p(y | \\theta)$\")\nvline!([θ_mle]; label=\"MLE\", linewidth=3)\n\n\n1\n\nThis builds on what we did last time. A coin flip is represented by a Bernoulli process. In fact, we could use a Binomial distribution to model the number of heads in \\(N\\) flips.\n\n2\n\nThe maximum likelihood estimate can in fact be shown to be exactly n_heads / length(coin_flips)"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#prior",
    "href": "slides/2023-09-13-bayesian.html#prior",
    "title": "Bayesian statistics and computation",
    "section": "Prior",
    "text": "Prior\nWe should be suspicious of our analysis when it concludes that we will continue to see 8 out of 9 flips coming up heads forever.\n\nTo perform a Bayesian analysis, we’ll need a prior. A Beta distribution is a natural choice for a prior on a probability, although we could use a Uniform distribution or even something silly like a truncated Gamma (don’t!)\n\nprior_dist = Beta(5, 5)\nplot(prior_dist; label=false, xlabel=L\"$θ$\", ylabel=L\"$p(θ)$\", linewidth=3)"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#closed-form-solution",
    "href": "slides/2023-09-13-bayesian.html#closed-form-solution",
    "title": "Bayesian statistics and computation",
    "section": "Closed-form solution",
    "text": "Closed-form solution\nCool property: if you have a Beta prior and a Binomial likelihood, the posterior is also Beta distributed. Look up Beta-Binomial conjugacy for more! We will leverage this property to check our answers.\n\nclosed_form = Beta(prior_dist.α + n_heads, prior_dist.β + N - n_heads)"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#markov-chain-monte-carlo",
    "href": "slides/2023-09-13-bayesian.html#markov-chain-monte-carlo",
    "title": "Bayesian statistics and computation",
    "section": "Markov Chain Monte Carlo",
    "text": "Markov Chain Monte Carlo\n\nA class of methods for sampling from a probability distribution\nRandom walkers:\n\nStart at some value\nPropose a new value\nAccept or reject the new value based on some criteria\n\nRepeat to get a “chain” of samples"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#metropolis-hastings",
    "href": "slides/2023-09-13-bayesian.html#metropolis-hastings",
    "title": "Bayesian statistics and computation",
    "section": "Metropolis-Hastings",
    "text": "Metropolis-Hastings\nSee the very good Wikipedia article\n\n\nlog_posterior(θ) = logpdf(prior_dist, θ) + flip_log_like(θ)\n\nθ_samples = []\nθ_sample = 0.5 # initial guess\nproposal_dist(θ) = Uniform(0, 1) # propose new values based on the current value\n\nwhile length(θ_samples) &lt; 10_000\n    proposal = rand(proposal_dist(θ_sample)) # propose a new value\n    p_accept = min(exp(log_posterior(proposal) - log_posterior(θ_sample)), 1)\n    if rand() &lt; p_accept\n        θ_sample = proposal\n    end\n    push!(θ_samples, θ_sample)\nend\nhistogram(\n    θ_samples;\n    normalize=:pdf,\n    label=\"Samples\",\n    legend=:topleft,\n    xlabel=L\"$θ$\",\n    ylabel=L\"$p(θ | y)$\"\n)\nplot!(closed_form; label=\"Exact Posterior\", linewidth=3)"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#limitations",
    "href": "slides/2023-09-13-bayesian.html#limitations",
    "title": "Bayesian statistics and computation",
    "section": "Limitations",
    "text": "Limitations\n\nWorks great for a very simple problem\nComputation blows up in higher dimensions (p_accept gets very small)\nHave to code a new sampler for each problem\n\n\nModern samplers leverage gradients and clever tricks to draw better samples for harder problems. Let’s use them!"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#turing-model-specification",
    "href": "slides/2023-09-13-bayesian.html#turing-model-specification",
    "title": "Bayesian statistics and computation",
    "section": "Turing model specification",
    "text": "Turing model specification\nWe can write down the full Bayesian model in Turing, which uses a syntax very close to our notation!\n\n@model function coinflip(y)\n\n    # to define θ as a random variable, we use ~\n    # anything that's not an input (data) is treated as a parameter to be estimated!\n    θ ~ prior_dist\n\n    # the data generating process\n    return y .~ Bernoulli(θ)\nend"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#turing-sampling",
    "href": "slides/2023-09-13-bayesian.html#turing-sampling",
    "title": "Bayesian statistics and computation",
    "section": "Turing sampling",
    "text": "Turing sampling\nWe can leverage sophisticated machinery for drawing samples from arbitrary posterior distributions. For now, we will trust that it is drawing samples from \\(p(y | \\theta)\\) and not worry about the details.\n\ncoin_chain = let # variables defined in a let...end block are temporary\n    model = coinflip(heads)\n    sampler = externalsampler(DynamicHMC.NUTS())\n    nsamples = 10_000\n    sample(model, sampler, nsamples; drop_warmup=true)\nend\nsummarystats(coin_chain)\n\n\nSummary Statistics\n  parameters      mean       std      mcse    ess_bulk    ess_tail      rhat   ⋯\n      Symbol   Float64   Float64   Float64     Float64     Float64   Float64   ⋯\n           θ    0.6832    0.1021    0.0016   4234.2533   5796.1544    1.0003   ⋯\n                                                                1 column omitted"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#visualize",
    "href": "slides/2023-09-13-bayesian.html#visualize",
    "title": "Bayesian statistics and computation",
    "section": "Visualize",
    "text": "Visualize\nWe can visualize our posterior\nhistogram( coin_chain[:θ]; label=“Samples”, normalize=:pdf, legend=:topleft, xlabel=L”\\(θ\\)“, ylabel=L”\\(p(θ | y)\\)” ) plot!(closed_form; label=“Exact Posterior”, linewidth=3) plot!(prior_dist; label=“Prior”, linewidth=3) vline!([θ_mle]; label=“MLE”, linewidth=3) ```## Compromise\nThe posterior is a compromise between the prior and the likelihood.\n\n\nBad priors lead to bad inferences\nThe choice of prior is subjective, which some people hate,\n\nWe will approach this in a principled manner (Gelman et al., 2020; Gelman & Shalizi, 2013)\nLots of other steps are also subjective (choice of likelihood model, which data to use, problem framing, etc)\nFalse sense of objectivity is dangerous anyways!"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#read-data",
    "href": "slides/2023-09-13-bayesian.html#read-data",
    "title": "Bayesian statistics and computation",
    "section": "Read data",
    "text": "Read data\n\n\nCode\nannmax = CSV.read(\"data/8638610-annmax.csv\", DataFrame)\nannmax.lsl .*= u\"ft\"\nannmax.lsl_ft = ustrip.(u\"ft\", annmax.lsl)\np1 = plot(\n    annmax.year,\n    annmax.lsl;\n    xlabel=\"Year\",\n    ylabel=\"Ann. Max. Water Level\",\n    label=false,\n    marker=:circle\n)\np2 = histogram(\n    annmax.lsl;\n    normalize=:pdf,\n    orientation=:horizontal,\n    label=false,\n    xlabel=false,\n    ylabel=\"\",\n    yticks=[],\n    bins=2:0.3:8,\n    xlims=(0, 0.8)\n)\n\nl = @layout [a{0.7w} b{0.3w}]\nplot(p1, p2; layout=l, link=:y, ylims=(2, 8), suptitle=\"Sewell's Point, VA\")"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#model",
    "href": "slides/2023-09-13-bayesian.html#model",
    "title": "Bayesian statistics and computation",
    "section": "Model",
    "text": "Model\nDefine a LogNormal distribution with very diffuse (flat) priors\n\n@model function lognormal_flatpriors(y)\n    # define the parameters\n    # and assign prior\n    μ ~ Normal(0, 10) # Extremely diffuse prior\n    σ ~ truncated(Normal(0, 10), 0, Inf) # σ must be positive\n\n    # data generating process\n    return y .~ LogNormal(μ, σ)\nend"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#sample",
    "href": "slides/2023-09-13-bayesian.html#sample",
    "title": "Bayesian statistics and computation",
    "section": "Sample",
    "text": "Sample\n\nln_flat_chn = let\n    model = lognormal_flatpriors(annmax.lsl_ft)\n    sampler = externalsampler(DynamicHMC.NUTS())\n    nsamples = 20_000\n    sample(model, sampler, nsamples; drop_warmup=true)\nend\nsummarystats(ln_flat_chn)"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#posterior",
    "href": "slides/2023-09-13-bayesian.html#posterior",
    "title": "Bayesian statistics and computation",
    "section": "Posterior",
    "text": "Posterior\nWe leverage the histogram2d function to visualize the 2D posterior distribution.\n\n\nCode\npost1_scatter = histogram2d(\n    ln_flat_chn[:μ],\n    ln_flat_chn[:σ];\n    label=false,\n    xlabel=L\"\\mu\",\n    ylabel=L\"\\sigma\",\n    title=\"Diffuse Priors\",\n    normalize=:pdf,\n    clims=(0, 1000),\n    bins=100\n)"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#return-period-with-uncertainty",
    "href": "slides/2023-09-13-bayesian.html#return-period-with-uncertainty",
    "title": "Bayesian statistics and computation",
    "section": "Return period with uncertainty",
    "text": "Return period with uncertainty\nEach draw from the posterior represents a plausible value of \\(\\mu\\) and \\(\\sigma\\). We can use these to explore the distribution of return periods.\n\n\nCode\nfor idx in 1:500\n    μ = ln_flat_chn[:μ][idx]\n    σ = ln_flat_chn[:σ][idx]\n    rt = quantile.(LogNormal(μ, σ), aeps)\n    label = idx == 1 ? \"Posterior\" : false\n    plot!(plt_rt, rts, rt; color=:black, alpha=0.05, label=label, linewidth=0.5)\nend\nplt_rt"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#trace-plot",
    "href": "slides/2023-09-13-bayesian.html#trace-plot",
    "title": "Bayesian statistics and computation",
    "section": "Trace plot",
    "text": "Trace plot\nVisualize the samples as a chain\n\nplot(ln_flat_chn)"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#model-1",
    "href": "slides/2023-09-13-bayesian.html#model-1",
    "title": "Bayesian statistics and computation",
    "section": "Model",
    "text": "Model\nWe can treat the priors as parameters so that we don’t have to define a new @model each time we want to update our priors\n\n1@model function lognormal(y, μ_dist, σ_dist)\n    μ ~ μ_dist\n    σ ~ σ_dist\n    return y .~ LogNormal(μ, σ)\nend\n\n\n1\n\nNo reason why we can’t pass distributions as functional arguments"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#guess-and-prior-predictive-check",
    "href": "slides/2023-09-13-bayesian.html#guess-and-prior-predictive-check",
    "title": "Bayesian statistics and computation",
    "section": "Guess and prior predictive check",
    "text": "Guess and prior predictive check\nDefine priors\n\nμ_prior = Normal(3, 3)\nσ_prior = truncated(Normal(0, 3), 0, Inf)\n\nDraw samples from the prior\n\nln_ppc = let\n    model = lognormal(annmax.lsl_ft, μ_prior, σ_prior)\n    sampler = Prior()\n    nsamples = 10_000\n    sample(model, sampler, nsamples; drop_warmup=true)\nend\n\nPlot the consequences of these samples\n\n\nCode\nplt_prior_1 = plot(plt_rt_base; yticks=10 .^ collect(0:12))\nfor idx in 1:1_000\n    μ = ln_ppc[:μ][idx]\n    σ = ln_ppc[:σ][idx]\n    rt = quantile.(LogNormal(μ, σ), aeps)\n    label = idx == 1 ? \"Prior\" : false\n    plot!(plt_prior_1, rts, rt; color=:black, alpha=0.1, label=label)\nend\nplt_prior_1"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#revise",
    "href": "slides/2023-09-13-bayesian.html#revise",
    "title": "Bayesian statistics and computation",
    "section": "Revise",
    "text": "Revise\nIf we are getting return levels of \\(10^{12}\\) ft, we should probably revise our priors\n\n1μ_prior = Normal(1, 1)\nσ_prior = truncated(Normal(0, 1), 0, Inf)\n\n\n1\n\nYes, I’m overwriting the old value\n\n\n\n\nWe can sample\n\n\nCode\nln_ppc = let\n    model = lognormal(annmax.lsl_ft, μ_prior, σ_prior)\n    sampler = Prior()\n    nsamples = 10_000\n    sample(model, sampler, nsamples; drop_warmup=true)\nend\n\nplt_prior_2 = plot(plt_rt_base; yticks=10 .^ collect(0:5))\nfor idx in 1:1_000\n    μ = ln_ppc[:μ][idx]\n    σ = ln_ppc[:σ][idx]\n    rt = quantile.(LogNormal(μ, σ), aeps)\n    label = idx == 1 ? \"Prior\" : false\n    plot!(plt_prior_2, rts, rt; color=:black, alpha=0.1, label=label)\nend\nplt_prior_2"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#getting-closer",
    "href": "slides/2023-09-13-bayesian.html#getting-closer",
    "title": "Bayesian statistics and computation",
    "section": "Getting closer",
    "text": "Getting closer\n\n\nCode\nln_ppc = let\n    model = lognormal(annmax.lsl_ft, μ_prior, σ_prior)\n    sampler = Prior()\n    nsamples = 5000\n    sample(model, sampler, nsamples; drop_warmup=true)\nend\n\nplt_prior_3 = plot(plt_rt_base; yticks=10 .^ collect(0:5))\nfor idx in 1:1_000\n    μ = ln_ppc[:μ][idx]\n    σ = ln_ppc[:σ][idx]\n    rt = quantile.(LogNormal(μ, σ), aeps)\n    label = idx == 1 ? \"Prior\" : false\n    plot!(plt_prior_3, rts, rt; color=:black, alpha=0.1, label=label)\nend\nplt_prior_3\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nμ_prior = Normal(1, 1)\nσ_prior = truncated(Normal(0, 0.5), 0, Inf)"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#now-get-posterior",
    "href": "slides/2023-09-13-bayesian.html#now-get-posterior",
    "title": "Bayesian statistics and computation",
    "section": "Now get posterior",
    "text": "Now get posterior\nWe use the same model to get the posterior. Often we want to run multiple chains with different initial values to make sure we are getting good samples.\n\nln_post = let\n    model = lognormal(annmax.lsl_ft, μ_prior, σ_prior)\n    sampler = externalsampler(DynamicHMC.NUTS())\n    n_per_chain = 5000\n    nchains = 4\n    sample(model, sampler, MCMCThreads(), n_per_chain, nchains; drop_warmup=true)\nend\n\n\nsummarystats(ln_post)\n\n\nSummary Statistics\n  parameters      mean       std      mcse     ess_bulk     ess_tail      rhat ⋯\n      Symbol   Float64   Float64   Float64      Float64      Float64   Float64 ⋯\n           μ    1.3692    0.0194    0.0001   17256.1692   13201.9364    1.0003 ⋯\n           σ    0.1861    0.0138    0.0001   17431.4216   13529.7674    1.0001 ⋯\n                                                                1 column omitted"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#traceplot-for-multiple-chains",
    "href": "slides/2023-09-13-bayesian.html#traceplot-for-multiple-chains",
    "title": "Bayesian statistics and computation",
    "section": "Traceplot for multiple chains",
    "text": "Traceplot for multiple chains\n\nplot(ln_post)"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#visualize-1",
    "href": "slides/2023-09-13-bayesian.html#visualize-1",
    "title": "Bayesian statistics and computation",
    "section": "Visualize",
    "text": "Visualize\n\n\nCode\npost2_scatter = histogram2d(\n    vec(ln_post[:μ]),\n    vec(ln_post[:σ]);\n    label=false,\n    xlabel=L\"\\mu\",\n    title=\"More informed priors\",\n    normalize=:pdf,\n    yticks=[],\n    clims=(0, 1000),\n    bins=100\n)\nplot(plot(post1_scatter), post2_scatter; link=:both)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nHere our likelihood is very informative, so it doesn’t much matter if we use excessively diffuse priors. This is nice, though not something we can count on in general."
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#return-period-with-uncertainty-1",
    "href": "slides/2023-09-13-bayesian.html#return-period-with-uncertainty-1",
    "title": "Bayesian statistics and computation",
    "section": "Return period with uncertainty",
    "text": "Return period with uncertainty\nAs before, we can visualize our posterior distribution in terms of return periods\n\n\nCode\nplt_rt = plot(plt_rt_base)\nscatter!(plt_rt, 1 ./ xp, ys; label=\"Observations\", color=:gray, alpha=1)\nfor idx in 1:500\n    μ = ln_post[:μ][idx]\n    σ = ln_post[:σ][idx]\n    rt = quantile.(LogNormal(μ, σ), aeps)\n    label = idx == 1 ? \"Posterior\" : false\n    plot!(plt_rt, rts, rt; color=:black, alpha=0.05, label=label)\nend\nplt_rt"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#key-value-add-of-bayesian-inference",
    "href": "slides/2023-09-13-bayesian.html#key-value-add-of-bayesian-inference",
    "title": "Bayesian statistics and computation",
    "section": "Key value add of Bayesian inference",
    "text": "Key value add of Bayesian inference\n\n\nDraw samples from tricky posteriors to compute expectations \\(\\mathbb{E}[f(\\theta)]\\)\nQuantify parametric uncertainty\n\nIn practice, sometimes this is a big deal and sometimes model structure uncertainties matter more\n\nForce us to specify a data generating process\nComputational methods fail loudly"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#learning-turing",
    "href": "slides/2023-09-13-bayesian.html#learning-turing",
    "title": "Bayesian statistics and computation",
    "section": "Learning Turing",
    "text": "Learning Turing\nThe official docs are great.\n\n\n\n\n\n\nWarning\n\n\nGoogle will often try to link you to the old site, https://turing.ml/. This is out of date! Use https://turinglang.org/stable/ instead."
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#another-word-on-generative-ai",
    "href": "slides/2023-09-13-bayesian.html#another-word-on-generative-ai",
    "title": "Bayesian statistics and computation",
    "section": "Another word on generative AI",
    "text": "Another word on generative AI\n\n\n\nGitHub Copilot\n\nFree for students: instructions\n\nGPT and related language models\n\n\nDo not just plug in the problem and paste the solution!\n\nLabs are just 10% of your grade\nYou will be resonsible for material on projects / tests\nYou won’t learn\n\nDo use it to for syntax help and code explanations"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#logistics",
    "href": "slides/2023-09-13-bayesian.html#logistics",
    "title": "Bayesian statistics and computation",
    "section": "Logistics",
    "text": "Logistics\n\nFriday:\n\nLab 04\nLab 03 due\n\nMonday 9/18: no class. Work on lab 04 and work through these slides.\nTuesday 9/19: no office hours.\nWednesday 9/20: test I review\nFriday 9/22: test I\nYou may turn in lab 04 by 9/29 (two weeks)"
  },
  {
    "objectID": "slides/2023-09-13-bayesian.html#references",
    "href": "slides/2023-09-13-bayesian.html#references",
    "title": "Bayesian statistics and computation",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nGelman, A., & Shalizi, C. R. (2013). Philosophy and the practice of Bayesian statistics. British Journal of Mathematical and Statistical Psychology, 66(1), 8–38. https://doi.org/f4k2h4\n\n\nGelman, A., Vehtari, A., Simpson, D., Margossian, C. C., Carpenter, B., Yao, Y., et al. (2020, November 3). Bayesian workflow. https://doi.org/10.48550/arXiv.2011.01808"
  },
  {
    "objectID": "slides/2023-11-01-extreme-value-stats.html#objectives",
    "href": "slides/2023-11-01-extreme-value-stats.html#objectives",
    "title": "Extreme Value Statistics",
    "section": "Objectives",
    "text": "Objectives\n\nLikelihood of extreme events\nExtrapolate\nUnivariate (for now)"
  },
  {
    "objectID": "slides/2023-11-01-extreme-value-stats.html#applications",
    "href": "slides/2023-11-01-extreme-value-stats.html#applications",
    "title": "Extreme Value Statistics",
    "section": "Applications",
    "text": "Applications\n\nEngineering design\nEmergency management\nRegulation\nInsurance\nManaging financial assets"
  },
  {
    "objectID": "slides/2023-11-01-extreme-value-stats.html#what-variables",
    "href": "slides/2023-11-01-extreme-value-stats.html#what-variables",
    "title": "Extreme Value Statistics",
    "section": "What variables?",
    "text": "What variables?\n\nStreamflow\nPrecipitation rates or totals\nWind speed\nTemperatures"
  },
  {
    "objectID": "slides/2023-11-01-extreme-value-stats.html#extremes-are-rare",
    "href": "slides/2023-11-01-extreme-value-stats.html#extremes-are-rare",
    "title": "Extreme Value Statistics",
    "section": "Extremes are rare",
    "text": "Extremes are rare\n\nThere are simple and clever methods for estimating the likelihood of rare events\nFundamentally, extrapolating is hard\nKey sources of uncertainty:\n\nEstimation uncertainty\nModel structure uncertainty\nSampling uncertainty"
  },
  {
    "objectID": "slides/2023-11-01-extreme-value-stats.html#harvey-and-addicks-barker",
    "href": "slides/2023-11-01-extreme-value-stats.html#harvey-and-addicks-barker",
    "title": "Extreme Value Statistics",
    "section": "Harvey and Addicks / Barker",
    "text": "Harvey and Addicks / Barker\nI was asked to calculate what would happen under some specific assumptions (no suggestion of unbiasedness!)\n\n\nThese were plausible assumptions, not necessarily the “right” or “best” assumptions (other side had many reasonable objections!)\nLarge differences between estimates made under different assumptions underscore the challenge of “objective” esimates\nA hard problem (interacting drivers of nonstationarity, short records, etc)"
  },
  {
    "objectID": "slides/2023-11-01-extreme-value-stats.html#precipitation-frequencies-in-tx",
    "href": "slides/2023-11-01-extreme-value-stats.html#precipitation-frequencies-in-tx",
    "title": "Extreme Value Statistics",
    "section": "Precipitation frequencies in TX",
    "text": "Precipitation frequencies in TX\n\nTWDB project\nAtlas 14 (Perica et al., 2018) does not account for climate change\nUse more stations and account for climate change\nJoint TWDB/TAMU/Rice project\nVariable studied: precipitation at multiple durations across the entire state"
  },
  {
    "objectID": "slides/2023-11-01-extreme-value-stats.html#how-likely-was-winter-storm-uri",
    "href": "slides/2023-11-01-extreme-value-stats.html#how-likely-was-winter-storm-uri",
    "title": "Extreme Value Statistics",
    "section": "How likely was Winter Storm Uri?",
    "text": "How likely was Winter Storm Uri?\n\nWhat should we plan for?\nWas it an “unprecedented” event?\nVariable studied:\n\ntemperature at grid cells\naggregated “population-weighted” index"
  },
  {
    "objectID": "slides/2023-11-01-extreme-value-stats.html#peak-over-threashold-pot",
    "href": "slides/2023-11-01-extreme-value-stats.html#peak-over-threashold-pot",
    "title": "Extreme Value Statistics",
    "section": "Peak over threashold (POT)",
    "text": "Peak over threashold (POT)\n\nDefine a threshold\nModel the distribution of events above the threshold\nModel the probability of seeing an event over the threshold\nAdvantage: focus on meaningful events, even if they’re rare\nDisadvantage: threshold is arbitrary, modeling arrival turns out to be tricky"
  },
  {
    "objectID": "slides/2023-11-01-extreme-value-stats.html#block-maxima",
    "href": "slides/2023-11-01-extreme-value-stats.html#block-maxima",
    "title": "Extreme Value Statistics",
    "section": "Block maxima",
    "text": "Block maxima\n\nDefine a block size (e.g., 1 year – how you define “a year” matters)\nModel the distribution of the extreme in each block\nAdvantage: easier to communicate and implementations are more flexible\nDisadvantages: timing of extremes; two extremes in one year; sometimes your min/max is not special"
  },
  {
    "objectID": "slides/2023-11-01-extreme-value-stats.html#key-terms",
    "href": "slides/2023-11-01-extreme-value-stats.html#key-terms",
    "title": "Extreme Value Statistics",
    "section": "Key terms",
    "text": "Key terms\n\nExceedance probability (often AEP): \\(p\\)\nReturn period or recurrence interval (\\(T\\)): \\(\\frac{1}{p}\\)\nReturn level: the value that will be exceeded with probability \\(\\frac{1}{T}\\), ie the quantile"
  },
  {
    "objectID": "slides/2023-11-01-extreme-value-stats.html#plotting-position",
    "href": "slides/2023-11-01-extreme-value-stats.html#plotting-position",
    "title": "Extreme Value Statistics",
    "section": "Plotting position",
    "text": "Plotting position\nTwo ideas:\n\nRanks: if you have \\(N\\) events, the largest is rank 1, the second largest rank 2, etc.\nUse the points that you have as return levels and estimate the associated return periods\nCommon estimator is Weibull plotting position \\(p = \\frac{m}{N+1}\\) where \\(m\\) is the rank\nLots of bickering in the literature about right choice\nWhen you see return period plots, if the observations are shown they are using a plotting position of some sort"
  },
  {
    "objectID": "slides/2023-11-01-extreme-value-stats.html#log-pearson-type-iii-distribution",
    "href": "slides/2023-11-01-extreme-value-stats.html#log-pearson-type-iii-distribution",
    "title": "Extreme Value Statistics",
    "section": "Log Pearson Type III distribution",
    "text": "Log Pearson Type III distribution\n\nDifferent variables have different properties\nUSGS likes this distribution for streamflow"
  },
  {
    "objectID": "slides/2023-11-01-extreme-value-stats.html#gev",
    "href": "slides/2023-11-01-extreme-value-stats.html#gev",
    "title": "Extreme Value Statistics",
    "section": "GEV",
    "text": "GEV\n\nModel block extremes\nAs we will see, has strong theoretical justification\nThree parameters: location, scale, shape\nShape can be tricky to estimate \\(\\rightarrow\\) large parameter uncertainty"
  },
  {
    "objectID": "slides/2023-11-01-extreme-value-stats.html#gpd",
    "href": "slides/2023-11-01-extreme-value-stats.html#gpd",
    "title": "Extreme Value Statistics",
    "section": "GPD",
    "text": "GPD\n\nSimilar: location, scale, and shape parameter\nRequires a separate model for “arrivals”"
  },
  {
    "objectID": "slides/2023-11-01-extreme-value-stats.html#parametric-uncertainty",
    "href": "slides/2023-11-01-extreme-value-stats.html#parametric-uncertainty",
    "title": "Extreme Value Statistics",
    "section": "Parametric Uncertainty",
    "text": "Parametric Uncertainty\n\nMany parameter values can be consistent with the data\nBut lead to very different conclusions about the likelihood of extreme events\nHard to pin down\nRelatively easier to resolve through clever approaches we will see"
  },
  {
    "objectID": "slides/2023-11-01-extreme-value-stats.html#model-uncertainty",
    "href": "slides/2023-11-01-extreme-value-stats.html#model-uncertainty",
    "title": "Extreme Value Statistics",
    "section": "Model Uncertainty",
    "text": "Model Uncertainty\n\nDifferent assumptions lead to different inferences\nGEV / GPD are theoretically justified\nAs we add clever approaches, we add model structure uncertainty"
  },
  {
    "objectID": "slides/2023-11-01-extreme-value-stats.html#sampling-uncertainty",
    "href": "slides/2023-11-01-extreme-value-stats.html#sampling-uncertainty",
    "title": "Extreme Value Statistics",
    "section": "Sampling Uncertainty",
    "text": "Sampling Uncertainty\n\nWe have a finite sample of data and are trying to estimate parameters that tell us about rare events\nIf Harvey had never occurred, we would likely have a very different estimate of the 100-year rainfall!"
  },
  {
    "objectID": "slides/2023-11-01-extreme-value-stats.html#nonstationarity",
    "href": "slides/2023-11-01-extreme-value-stats.html#nonstationarity",
    "title": "Extreme Value Statistics",
    "section": "Nonstationarity",
    "text": "Nonstationarity\n\nThese models don’t explicitly account for climate change\nInterannual variability / correlations"
  },
  {
    "objectID": "slides/2023-11-01-extreme-value-stats.html#reading",
    "href": "slides/2023-11-01-extreme-value-stats.html#reading",
    "title": "Extreme Value Statistics",
    "section": "Reading",
    "text": "Reading\n\nColes (2001): canonical extreme value textbook"
  },
  {
    "objectID": "slides/2023-11-01-extreme-value-stats.html#logistics",
    "href": "slides/2023-11-01-extreme-value-stats.html#logistics",
    "title": "Extreme Value Statistics",
    "section": "Logistics",
    "text": "Logistics\n\nExams\n\nWorking on grading exams 1 and 2\n\nClass\n\nFriday 11/3: lab\nMonday 11/6: POT and Block Maxima Models\nWednesday 11/8: GEV models and estimators"
  },
  {
    "objectID": "slides/2023-11-01-extreme-value-stats.html#references",
    "href": "slides/2023-11-01-extreme-value-stats.html#references",
    "title": "Extreme Value Statistics",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nColes, S. (2001). An introduction to statistical modeling of extreme values. London ; Springer.\n\n\nPerica, S., Pavlovic, S., St. Laurent, M., Trypaluk, C., Unruh, D., & Wilhite, O. (2018). NOAA Atlas 14 (No. Volume 11 Version 2.0: Texas) (p. 283). Silver Spring, MD: National Weather Service, National Oceanic and Atmospheric Administration, U.S. Department of Commerce. Retrieved from https://www.weather.gov/media/owp/oh/hdsc/docs/Atlas14_Volume11.pdf"
  },
  {
    "objectID": "slides/2023-11-20-gev-tbd.html#tbd",
    "href": "slides/2023-11-20-gev-tbd.html#tbd",
    "title": "TBD",
    "section": "TBD",
    "text": "TBD\nThis is a placeholder for a planned lecture. The schedule will be modified as needed."
  },
  {
    "objectID": "slides/2023-11-27-workshop.html#tbd",
    "href": "slides/2023-11-27-workshop.html#tbd",
    "title": "Project Workshop",
    "section": "TBD",
    "text": "TBD\nWe will plan to workshop project 2 in class."
  },
  {
    "objectID": "slides/2023-10-04-glm.html#practice-problem-regression",
    "href": "slides/2023-10-04-glm.html#practice-problem-regression",
    "title": "Generalized Linear Models",
    "section": "Practice problem: regression",
    "text": "Practice problem: regression\nYou are given pairs of data \\((x_i, y_i)\\) where \\(x_i\\) is the number of vehicles on the road and \\(y_i\\) is the Air Quality Index (AQI). We model their relationship as \\[\n\\begin{align}\ny_i &\\sim N(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha_i + \\beta x_i\n\\end{align}\n\\] where \\(\\alpha_i\\) and \\(\\beta\\) are parameters to be estimated.\n\nWrite down the log likelihood for a single data point \\(\\log p(y_i | x_i)\\)\nWrite down the log likelihood for the entire dataset \\(\\log p(\\mathbf{y} | \\mathbf{x})\\)\n\nFor reference, the Normal PDF is \\[\nf(x | \\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2 \\sigma^2}\\right)\n\\]"
  },
  {
    "objectID": "slides/2023-10-04-glm.html#linear-regression",
    "href": "slides/2023-10-04-glm.html#linear-regression",
    "title": "Generalized Linear Models",
    "section": "Linear regression",
    "text": "Linear regression\nWe have recently seen models that look like \\[\n\\begin{align}\ny_i &\\sim N(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha_i + \\beta x_i\n\\end{align}\n\\] Or (using another notation) \\[\n\\begin{align}\ny_i &= \\alpha_i + \\beta x_i + \\epsilon_i \\\\\n\\epsilon_i &\\sim N(0, \\sigma)\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/2023-10-04-glm.html#why-linear",
    "href": "slides/2023-10-04-glm.html#why-linear",
    "title": "Generalized Linear Models",
    "section": "Why linear?",
    "text": "Why linear?\n\n\n\\(y = ax + b\\) is a strong assumption, not always physically justifiable, though often useful.\nAnother nice way to think about linear models is that they are Taylor series representations of functions. Michael Betancourt has an excellent and thorough case study."
  },
  {
    "objectID": "slides/2023-10-04-glm.html#motivation-depth-damage",
    "href": "slides/2023-10-04-glm.html#motivation-depth-damage",
    "title": "Generalized Linear Models",
    "section": "Motivation: depth-damage",
    "text": "Motivation: depth-damage\nIn lab 3, we studied the distribution of flood losses in a neighborhood.\n\n\nWhat if we wanted to condition this distribution on variables describing flood characteristics and/or household risk management practices (as in Rözer et al., 2019)?\nRegression lets us condition estimates on covariates\nBut we can’t use linear regression here, because the response variable has support \\((0, 1)\\)"
  },
  {
    "objectID": "slides/2023-10-04-glm.html#today",
    "href": "slides/2023-10-04-glm.html#today",
    "title": "Generalized Linear Models",
    "section": "Today",
    "text": "Today\nGeneralized Linear Models extend the concept of regression to other distributions – specifically when the conditional likelihood is not Normal."
  },
  {
    "objectID": "slides/2023-10-04-glm.html#motivation",
    "href": "slides/2023-10-04-glm.html#motivation",
    "title": "Generalized Linear Models",
    "section": "Motivation",
    "text": "Motivation\nConsider a forest patch where we have recorded the occurrence of forest fires over several years. For each year, we have also noted the average summertime temperature. We want to investigate if there’s a relationship between the average summertime temperature and the likelihood of a forest fire occurring."
  },
  {
    "objectID": "slides/2023-10-04-glm.html#data",
    "href": "slides/2023-10-04-glm.html#data",
    "title": "Generalized Linear Models",
    "section": "Data",
    "text": "Data\n\n\nCode\navg_temp = [\n    20.5,\n    21.3,\n    22.7,\n    23.4,\n    21.8,\n    24.1,\n    20.9,\n    22.5,\n    23.8,\n    24.6,\n    21.1,\n    22.3,\n    23.5,\n    24.0,\n    22.8,\n    23.9,\n    21.4,\n    20.7,\n    23.2,\n    22.9,\n]\nforest_fire_occurred = [1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1]\nscatter(\n    avg_temp,\n    forest_fire_occurred;\n    label=false,\n    xlabel=\"Average summertime temperature (deg C)\",\n    ylabel=\"Forest fire occurred\",\n    legend=:topleft\n)"
  },
  {
    "objectID": "slides/2023-10-04-glm.html#likelihood",
    "href": "slides/2023-10-04-glm.html#likelihood",
    "title": "Generalized Linear Models",
    "section": "Likelihood",
    "text": "Likelihood\nFor each data point, we can use a Bernoulli distribution to model the occurrence of a forest fire \\[\ny_i \\sim \\mathrm{Bernoulli}(p_i)\n\\] where the Bernoulli PDF is \\[\nf(x | p) = p^x (1 - p)^{1 - x}\n\\]"
  },
  {
    "objectID": "slides/2023-10-04-glm.html#data-dependence",
    "href": "slides/2023-10-04-glm.html#data-dependence",
    "title": "Generalized Linear Models",
    "section": "Data dependence",
    "text": "Data dependence\nWe have \\(y_i \\sim \\mathrm{Bernoulli}(p_i)\\). We want to model \\(p_i\\) as some function of the average summertime temperature \\(x_i\\): \\[\nf(p_i)= \\alpha + \\beta x_i\n\\]\n\nThis looks a lot like the linear regression model from before, except:\n\n\nThe likelihood is Binomial rather than Normal. No big deal – we’ve seen that we can use arbitrary probability distributions to model processes we’re interested in\nWe have \\(f(p_i)\\) rather than just \\(\\mu_i\\). This is called a link function. GLMs need a link function to map the linear space of \\(\\alpha + \\beta x_i \\in (-\\infty, \\infty)\\) onto the allowed space of the parameter."
  },
  {
    "objectID": "slides/2023-10-04-glm.html#link-function",
    "href": "slides/2023-10-04-glm.html#link-function",
    "title": "Generalized Linear Models",
    "section": "Link function",
    "text": "Link function\n\n\nFor our Binomial model, \\(p_i \\in (0, 1)\\)\n\nwe need something to that maps \\((-\\infty, \\infty) \\rightarrow (0, 1)\\).\n\nMany possible options\nThe “canonical” one is a logit link:\n\n\n\n\\[\n\\begin{align}\n\\textrm{logit}(p_i) &= \\alpha + \\beta x_i \\\\\n\\log \\frac{p_i}{1 - p_i} &= \\alpha + \\beta x_i \\\\\np_i &= \\frac{\\exp(\\alpha + \\beta x_i)}{1 + \\exp(\\alpha + \\beta x_i)}\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/2023-10-04-glm.html#probit-link",
    "href": "slides/2023-10-04-glm.html#probit-link",
    "title": "Generalized Linear Models",
    "section": "Probit link",
    "text": "Probit link\nThis isn’t the only possible link function, though. For example, economists like to use the probit link – the Probit is the inverse of a standard Nomal distribution. These can be subtly different:\n\n\nCode\ninv_probit(x) = cdf(Normal(0, 1), x)\n\nplot(\n    logistic,\n    -5,\n    5;\n    label=\"Logit\",\n    xlabel=L\"$\\alpha + \\beta x_i$\",\n    ylabel=L\"$p_i = f(\\alpha + \\beta x_i)$\",\n    legend=:topleft\n)\nplot!(inv_probit; label=\"Probit\")"
  },
  {
    "objectID": "slides/2023-10-04-glm.html#inference-i",
    "href": "slides/2023-10-04-glm.html#inference-i",
    "title": "Generalized Linear Models",
    "section": "Inference I",
    "text": "Inference I\n\n1@model function logistic_regression(y::AbstractVector, x::AbstractVector)\n    α ~ Normal(0, 1)\n    β ~ Normal(0, 1)\n2    for i in eachindex(y)\n        p = logistic(α + β * x[i])\n3        y[i] ~ Bernoulli(p)\n    end\nend\n\n\n1\n\nHere we are saying that x and y have to be vectors, but we don’t care what kind of vector (e.g. Vector{Float64}, Vector{Int}, etc.)\n\n2\n\nThis is a more robust way to write for i in 1:length(y)\n\n3\n\ninv_logit is defined above."
  },
  {
    "objectID": "slides/2023-10-04-glm.html#inference-ii",
    "href": "slides/2023-10-04-glm.html#inference-ii",
    "title": "Generalized Linear Models",
    "section": "Inference II",
    "text": "Inference II\n\n\nCode\nlogistic_chn = let\n    model = logistic_regression(forest_fire_occurred, avg_temp)\n    sampler = externalsampler(DynamicHMC.NUTS())\n    nsamples = 10_000\n1    rng = Random.MersenneTwister(1041)\n    sample(rng, model, sampler, nsamples; drop_warmup=true)\nend\nplot(logistic_chn)\n\n\n\n1\n\nThis sets the random number generator seed so that we get the same results every time we run this code. This is useful for reproducibility, but you don’t need to do this in your own code!"
  },
  {
    "objectID": "slides/2023-10-04-glm.html#motivation-1",
    "href": "slides/2023-10-04-glm.html#motivation-1",
    "title": "Generalized Linear Models",
    "section": "Motivation",
    "text": "Motivation\nImagine a national park where we’ve recorded the number of wildlife sightings over several months. For each month, we also have the average number of visitors. We want to investigate if there’s a relationship between the average number of visitors and the number of wildlife sightings."
  },
  {
    "objectID": "slides/2023-10-04-glm.html#data-1",
    "href": "slides/2023-10-04-glm.html#data-1",
    "title": "Generalized Linear Models",
    "section": "Data",
    "text": "Data\n\n\nCode\navg_visitors = [\n    50, 55, 52, 58, 60, 53, 57, 59, 54, 56, 51, 61, 62, 63, 64, 65, 66, 67, 68, 69\n]\nwildlife_sightings = [4, 2, 4, 4, 9, 10, 4, 4, 5, 6, 6, 8, 6, 12, 13, 8, 19, 15, 13, 10]\nscatter(\n    avg_visitors,\n    wildlife_sightings;\n    label=false,\n    xlabel=\"Average number of visitors\",\n    ylabel=\"Wildlife sightings\",\n    legend=:topleft\n)"
  },
  {
    "objectID": "slides/2023-10-04-glm.html#likelihood-1",
    "href": "slides/2023-10-04-glm.html#likelihood-1",
    "title": "Generalized Linear Models",
    "section": "Likelihood",
    "text": "Likelihood\nFor each data point, we can use a Poisson distribution to model the number of wildlife sightings: \\[\ny_i \\sim \\mathrm{Poisson}(\\lambda_i)\n\\] where the Poisson PMF is \\[\nf(k | \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}.\n\\]"
  },
  {
    "objectID": "slides/2023-10-04-glm.html#data-dependence-1",
    "href": "slides/2023-10-04-glm.html#data-dependence-1",
    "title": "Generalized Linear Models",
    "section": "Data dependence",
    "text": "Data dependence\nWe have \\(y_i \\sim \\mathrm{Poisson}(\\lambda_i)\\). We want to model \\(\\lambda_i\\) as some function of the average number of visitors \\(x_i\\): \\[\nf(\\lambda_i) = \\alpha + \\beta x_i\n\\] We need \\(\\lambda_i &gt; 0\\) for the Poisson distribution. The canonical link function is \\(\\log\\).​"
  },
  {
    "objectID": "slides/2023-10-04-glm.html#inference-i-1",
    "href": "slides/2023-10-04-glm.html#inference-i-1",
    "title": "Generalized Linear Models",
    "section": "Inference I",
    "text": "Inference I\n\n@model function poisson_regression(y::AbstractVector, x::AbstractVector)\n    # priors\n    α ~ Normal(0, 5)\n    β ~ Normal(0, 5)\n\n    # likelihood\n1    λ = @. exp(α + β * x)\n    return y .~ Poisson.(λ)\nend\n\n\n1\n\n@. means all the operations to the right use dot syntax – this is equivalent to exp.(α .+ β .* x)."
  },
  {
    "objectID": "slides/2023-10-04-glm.html#inference-ii-1",
    "href": "slides/2023-10-04-glm.html#inference-ii-1",
    "title": "Generalized Linear Models",
    "section": "Inference II",
    "text": "Inference II\n\n\nCode\npoiss_chn = let\n    model = poisson_regression(wildlife_sightings, avg_visitors)\n    sampler = externalsampler(DynamicHMC.NUTS())\n    nsamples = 10_000\n    rng = Random.MersenneTwister(1112)\n    sample(rng, model, sampler, nsamples; drop_warmup=true)\nend\nplot(poiss_chn)"
  },
  {
    "objectID": "slides/2023-10-04-glm.html#posterior-check",
    "href": "slides/2023-10-04-glm.html#posterior-check",
    "title": "Generalized Linear Models",
    "section": "Posterior check",
    "text": "Posterior check\nTo our scatter plot, we can add the posterior predictive distribution, which we will visualize as percentiles\n\n\nRecall: we want \\(\\mathbb{E}[f(\\lambda_i)]\\)\nHere: \\(f\\) will be some percentile of the Poisson distribution with parameter \\(\\lambda_i\\)\nFor each value we want to predict at, we need to compute this percentile for each posterior sample and then average\n\nWhy can’t we average the values of \\(\\lambda\\) and then compute the percentile?"
  },
  {
    "objectID": "slides/2023-10-04-glm.html#posterior-check-ii",
    "href": "slides/2023-10-04-glm.html#posterior-check-ii",
    "title": "Generalized Linear Models",
    "section": "Posterior check II",
    "text": "Posterior check II\n\n\nCode\n# define the values of \"avg number of visitors\" we want to predict at\nx_pred = range(40, 75; length=100)\n\n# posterior distributions\nα = poiss_chn[:α]\nβ = poiss_chn[:β]\n\n# get a Matrix of Poisson distributions indexed [MCMC sample index, x index]\nλ = hcat([exp.(α .+ β .* xi) for xi in x_pred]...)\ndists = Poisson.(λ)\n\n# select the percentiles to plot\npercentiles = [5, 25, 50, 75, 95]\n\n# create the base plot\np = plot(;\n    xlabel=\"Average number of visitors\", ylabel=\"Wildlife sightings\", legend=:topleft\n)\n\n# add percentile lines\nfor pct in percentiles\n    line = vec(mean(quantile.(dists, pct / 100); dims=1))\n    plot!(p, x_pred, line; label=\"$pct%\", color=:gray, linestyle=:dash)\nend\n\n# add the observations on top\nscatter!(p, avg_visitors, wildlife_sightings; label=\"Observed\", color=:blue)"
  },
  {
    "objectID": "slides/2023-10-04-glm.html#other-common-models",
    "href": "slides/2023-10-04-glm.html#other-common-models",
    "title": "Generalized Linear Models",
    "section": "Other common models",
    "text": "Other common models\n\nNegative Binomial regression: \\(y_i \\sim \\textrm{NegBin}(\\mu_i, r)\\) with \\(\\log(\\mu_i)= \\alpha + \\beta x_i\\)\n“Robust” regression: \\(y_i = \\alpha + \\beta x_i + \\epsilon_i\\) with \\(\\epsilon_i \\sim \\textrm{T}^\\nu(\\sigma)\\) (\\(\\nu=4\\) is common)\nLots more"
  },
  {
    "objectID": "slides/2023-10-04-glm.html#simple-interfaces",
    "href": "slides/2023-10-04-glm.html#simple-interfaces",
    "title": "Generalized Linear Models",
    "section": "Simple interfaces",
    "text": "Simple interfaces\n\nTuringGLM\n\nstill a work in progress\nconvert a formula @formula(y ~ x1 + x2 + x3) into a Turing model\n\nInspired by\n\nBRMS in R, using Stan backend\nbambi in Python, using PyMC3 backend\n\n\nThese tools can be useful for fast model building, but when you write final versions of your results you should make sure you know what priors, data re-scaling, etc. you have used!"
  },
  {
    "objectID": "slides/2023-10-04-glm.html#summary",
    "href": "slides/2023-10-04-glm.html#summary",
    "title": "Generalized Linear Models",
    "section": "Summary",
    "text": "Summary\nWorking with non-Gaussian likelihoods is pretty straightforward\n\n\nNeed a “link function”\nCan use our same software tools to get MLE or Bayesian estimates\n\n\n\nMcElreath (2020) offers useful workflow suggestions:\n\nUse sensitivity analysis\nDo prior predictive checks (see chapter 11 for an example with Poisson regression)"
  },
  {
    "objectID": "slides/2023-10-04-glm.html#logistics",
    "href": "slides/2023-10-04-glm.html#logistics",
    "title": "Generalized Linear Models",
    "section": "Logistics",
    "text": "Logistics\n\nLab 5 due next Monday\nProject 1 to be posted soon (builds on lab 5)\nExam 1 revisions due Friday in class"
  },
  {
    "objectID": "slides/2023-10-04-glm.html#read-more",
    "href": "slides/2023-10-04-glm.html#read-more",
    "title": "Generalized Linear Models",
    "section": "Read more",
    "text": "Read more\nSome optional reading:\n\nChapters 10 and 11 of McElreath (2020) (in particular section 10.2)\n\nThis book spends a lot of time talking about Maximum Entropy likelihoods, which we won’t worry about much.\n\nChapter 16 of Gelman et al. (2014)\n\nBecause this comes towards the end of the book, it builds on a fair bit of content we haven’t yet seen"
  },
  {
    "objectID": "slides/2023-10-04-glm.html#references",
    "href": "slides/2023-10-04-glm.html#references",
    "title": "Generalized Linear Models",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nGelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (2014). Bayesian Data Analysis (3rd ed.). Chapman & Hall/CRC Boca Raton, FL, USA.\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second edition.). Boca Raton ; CRC Press, Taylor & Francis Group.\n\n\nRözer, V., Kreibich, H., Schröter, K., Müller, M., Sairam, N., Doss-Gollin, J., et al. (2019). Probabilistic models significantly reduce uncertainty in Hurricane Harvey pluvial flood loss estimates. Earth’s Future, 7(4). https://doi.org/10.1029/2018ef001074"
  },
  {
    "objectID": "slides/2023-08-30-marginal-joint-conditional.html#pdf-and-cdf",
    "href": "slides/2023-08-30-marginal-joint-conditional.html#pdf-and-cdf",
    "title": "Marginal, conditional, and joint distributions",
    "section": "PDF and CDF",
    "text": "PDF and CDF\nIf \\(F_X\\) is the cumulative distribution function (CDF) of \\(X\\) and \\(f_X\\) is the probability density function (PDF) of \\(X\\), then: \\[\nF_X ( x ) = \\int_{-\\infty}^x f_X(u) \\, du,\n\\] and (if \\(f_X\\) is continuous at \\(x\\) which it typically will be) \\[\nf_{X}(x)={\\frac {d}{dx}}F_{X}(x).\n\\] A useful property is \\[\n\\Pr[a\\leq X\\leq b]=\\int _{a}^{b}f_{X}(x)\\,dx\n\\]\n\n\n\n\n\n\nImportant\n\n\nWe can only talk about the probability that \\(y\\) is in some interval \\([a, b]\\), which is given by the integral of the PDF over that interval. The probability that \\(y\\) takes on the value \\(y^*\\), written \\(p(y=y^*)\\), is zero."
  },
  {
    "objectID": "slides/2023-08-30-marginal-joint-conditional.html#pdf-example",
    "href": "slides/2023-08-30-marginal-joint-conditional.html#pdf-example",
    "title": "Marginal, conditional, and joint distributions",
    "section": "PDF example",
    "text": "PDF example\nSimple example to illustrate that \\[\nF_X(2) = \\int_{-\\infty}^2 f_X(u) \\, du\n\\]\nWe will use a standard Normal distribution as an example\n\n\n(0.9771562639858903, 0.9772498680518208)\n\n\n\nMean 0 and standard deviation 1 by default\npdf(d, x) tells us the probability density function of distribution d evaluated at x\nquad_trap is a trapezoidal approximation of the integral with arguments: function, lower bound, upper bound, and number of points"
  },
  {
    "objectID": "slides/2023-08-30-marginal-joint-conditional.html#pmfs",
    "href": "slides/2023-08-30-marginal-joint-conditional.html#pmfs",
    "title": "Marginal, conditional, and joint distributions",
    "section": "PMFs",
    "text": "PMFs\n\nDiscrete distributions (like the Poisson) have a probability mass function (PMF) instead of a PDF\nFor PMFs, \\(p(y=y^*)\\) is the probability that \\(y\\) takes on the value \\(y^*\\), and is defined\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the Distributions package, both PDFs and PMFs are called pdf"
  },
  {
    "objectID": "slides/2023-08-30-marginal-joint-conditional.html#bayes-rule",
    "href": "slides/2023-08-30-marginal-joint-conditional.html#bayes-rule",
    "title": "Marginal, conditional, and joint distributions",
    "section": "Bayes’ Rule",
    "text": "Bayes’ Rule\n\\[\np(\\theta, y) = p(\\theta) p(y | \\theta)\n\\] and thus \\[\np(\\theta | y) = \\frac{p(\\theta, y)}{p(y)} = \\frac{p(\\theta) p(y | \\theta)}{p(y)}\n\\] generally: \\[\np(\\theta | y) \\propto p(\\theta) p(y | \\theta)\n\\]"
  },
  {
    "objectID": "slides/2023-08-30-marginal-joint-conditional.html#marginal-probability",
    "href": "slides/2023-08-30-marginal-joint-conditional.html#marginal-probability",
    "title": "Marginal, conditional, and joint distributions",
    "section": "Marginal probability",
    "text": "Marginal probability\nProbability of event \\(A\\): \\(\\Pr(A)\\)\n\nWe will write the marginal probability density function as \\[\np(\\theta) \\quad \\text{or} \\quad p(y)\n\\]"
  },
  {
    "objectID": "slides/2023-08-30-marginal-joint-conditional.html#joint-probability",
    "href": "slides/2023-08-30-marginal-joint-conditional.html#joint-probability",
    "title": "Marginal, conditional, and joint distributions",
    "section": "Joint probability",
    "text": "Joint probability\nProbability of events \\(A\\) and \\(B\\): \\(\\Pr(A \\& B)\\)\n\n\\[\np(\\theta, y)\n\\]"
  },
  {
    "objectID": "slides/2023-08-30-marginal-joint-conditional.html#conditional-probability",
    "href": "slides/2023-08-30-marginal-joint-conditional.html#conditional-probability",
    "title": "Marginal, conditional, and joint distributions",
    "section": "Conditional probability",
    "text": "Conditional probability\nProbability of event \\(A\\) given event \\(B\\): \\(\\Pr(A | B)\\)\n\n\\[\np(\\theta | y) \\quad \\text{or} \\quad p(y | \\theta)\n\\]"
  },
  {
    "objectID": "slides/2023-08-30-marginal-joint-conditional.html#example-two-dice-wager",
    "href": "slides/2023-08-30-marginal-joint-conditional.html#example-two-dice-wager",
    "title": "Marginal, conditional, and joint distributions",
    "section": "Example: two-dice wager",
    "text": "Example: two-dice wager\n\nA gambler presents you with an even-money wager. You will roll two dice, and if the highest number showing is one, two, three or four, then you win. If the highest number on either die is five or six, then she wins. Should you take the bet?"
  },
  {
    "objectID": "slides/2023-08-30-marginal-joint-conditional.html#overview",
    "href": "slides/2023-08-30-marginal-joint-conditional.html#overview",
    "title": "Marginal, conditional, and joint distributions",
    "section": "Overview",
    "text": "Overview\nStandard linear regression model, let’s assume \\(x \\in \\mathbb{R}\\) for simplicity (1 predictor): \\[\ny_i = ax_i + b + \\epsilon_i\n\\] where \\(\\epsilon_i \\sim N(0, \\sigma^2)\\)."
  },
  {
    "objectID": "slides/2023-08-30-marginal-joint-conditional.html#conditional-distribution-of-y_i",
    "href": "slides/2023-08-30-marginal-joint-conditional.html#conditional-distribution-of-y_i",
    "title": "Marginal, conditional, and joint distributions",
    "section": "Conditional distribution of \\(y_i\\)",
    "text": "Conditional distribution of \\(y_i\\)\nThe conditional probability density of \\(y_i\\) given \\(x_i\\) is \\[\np(y_i | x_i, a, b, \\sigma) = N(ax_i + b, \\sigma^2)\n\\] which is a shorthand for writing out the full equation for the Normal PDF. We can (and often will) write this as \\[\ny_i \\sim \\mathcal{N}(ax_i + b, \\sigma^2)\n\\] Finally, we will sometimes write \\(p(y_i | x_i)\\) as a shorthand for \\(p(y_i | x_i, a, b, \\sigma)\\). While fine in many circumstances, we should take care to make sure we are extremely clear about what parameters we are conditioning on."
  },
  {
    "objectID": "slides/2023-08-30-marginal-joint-conditional.html#marginal-distribution-of-y_i",
    "href": "slides/2023-08-30-marginal-joint-conditional.html#marginal-distribution-of-y_i",
    "title": "Marginal, conditional, and joint distributions",
    "section": "Marginal distribution of \\(y_i\\)",
    "text": "Marginal distribution of \\(y_i\\)\nThe marginal probability density of \\(y_i\\) is \\[\np(y_i | a, b, \\sigma) = \\int p(y_i | x_i, a, b, \\sigma) p(x_i) \\, dx_i\n\\] where \\(p(x_i)\\) is the probability density of \\(x_i\\)."
  },
  {
    "objectID": "slides/2023-08-30-marginal-joint-conditional.html#joint-distribution-of-y_i-and-x_i",
    "href": "slides/2023-08-30-marginal-joint-conditional.html#joint-distribution-of-y_i-and-x_i",
    "title": "Marginal, conditional, and joint distributions",
    "section": "Joint distribution of \\(y_i\\) and \\(x_i\\)",
    "text": "Joint distribution of \\(y_i\\) and \\(x_i\\)\nThe joint probability density of \\(y_i\\) and \\(x_i\\) is \\[\np(y_i, x_i | a, b, \\sigma) = p(y_i | x_i, a, b, \\sigma) p(x_i)\n\\] where \\(p(x_i)\\) is the probability density of \\(x_i\\)."
  },
  {
    "objectID": "slides/2023-08-30-marginal-joint-conditional.html#simulation",
    "href": "slides/2023-08-30-marginal-joint-conditional.html#simulation",
    "title": "Marginal, conditional, and joint distributions",
    "section": "Simulation",
    "text": "Simulation\n\nIf \\(x=2\\), we can simulate from the conditional distribution of \\(y\\):\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf \\(x \\sim N(0, 1)\\), then we can simulate from the joint distribution of \\(x\\) and \\(y\\):\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA list comprehension here is less elegant than writing rand.(Normal.(m .* x .+ b, σ)) but it is easy to read. The results are the same.\n\n\n\nFinally, assuming the same distribution, we can simulate from the marginal distribution of \\(y\\):"
  },
  {
    "objectID": "slides/2023-08-30-marginal-joint-conditional.html#overview-1",
    "href": "slides/2023-08-30-marginal-joint-conditional.html#overview-1",
    "title": "Marginal, conditional, and joint distributions",
    "section": "Overview",
    "text": "Overview\nThe Negative Binomial distribution (see last lecture) can be interpreted as a Gamma-Poisson mixture:\n\\[\n\\begin{align}\ny &\\sim \\textrm{Poisson}(\\lambda) \\\\\n\\lambda &\\sim \\textrm{Gamma}\\left(r, \\frac{p}{1-p} \\right)\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/2023-08-30-marginal-joint-conditional.html#mathematical-derivation",
    "href": "slides/2023-08-30-marginal-joint-conditional.html#mathematical-derivation",
    "title": "Marginal, conditional, and joint distributions",
    "section": "Mathematical derivation",
    "text": "Mathematical derivation\nWe can show mathematically that if \\(y ~ \\textrm{Negative Binomial}(r, p)\\), that is equivalent to the mixture model \\(y ~ \\textrm{Poisson}(\\lambda)\\) and \\(\\lambda ~ \\textrm{Gamma}(r, p / (1 - p))\\). \\[\n\\begin{align}\n& \\int_0^{\\infty} f_{\\text {Poisson }(\\lambda)}(y) \\times f_{\\operatorname{Gamma}\\left(r, \\frac{p}{1-p}\\right)}(\\lambda) \\mathrm{d} \\lambda \\\\\n& = \\int_0^{\\infty} \\frac{\\lambda^y}{y !} e^{-\\lambda} \\times \\frac{1}{\\Gamma(r)}\\left(\\frac{p}{1-p} \\lambda\\right)^{r-1} e^{-\\frac{p}{1-p} \\lambda}\\left(\\frac{p}{1-p} \\mathrm{~d} \\lambda\\right) \\\\\n\\ldots \\\\\n&= f_{\\text {Negative Binomial }(r, p)}(y)\n\\end{align}\n\\] For all the steps see Wikipedia."
  },
  {
    "objectID": "slides/2023-08-30-marginal-joint-conditional.html#simulation-example",
    "href": "slides/2023-08-30-marginal-joint-conditional.html#simulation-example",
    "title": "Marginal, conditional, and joint distributions",
    "section": "Simulation example",
    "text": "Simulation example\nWe can see this with simulation. First we define a function to simulate from the Gamma-Poisson mixture:\n\n\ngamma_poisson (generic function with 1 method)\n\n\n\nThen we can simulate from the mixture and compare to the Negative Binomial distribution:"
  },
  {
    "objectID": "slides/2023-08-30-marginal-joint-conditional.html#so-what",
    "href": "slides/2023-08-30-marginal-joint-conditional.html#so-what",
    "title": "Marginal, conditional, and joint distributions",
    "section": "So what?",
    "text": "So what?\nI don’t need you to know all the details of this particular mixture model. What I do want you to understand is:\n\n\nWe can model data using combinations of simpler distributions\nWe can use simple simulation approaches to approximate more complex relationships\n\nFor example, if we wanted to know \\(\\Pr(y &gt; 10)\\) when \\(y \\sim \\text{Negative Binomial}(r, p)\\) but we didn’t have a Negative Binomial distribution in our software package we could estimate our quantity of interest\nThis isn’t very interesting for this model (there is an analytic solution!) but lots of models we might want to write down don’t have analytic solutions"
  },
  {
    "objectID": "slides/2023-08-30-marginal-joint-conditional.html#key-ideas",
    "href": "slides/2023-08-30-marginal-joint-conditional.html#key-ideas",
    "title": "Marginal, conditional, and joint distributions",
    "section": "Key ideas",
    "text": "Key ideas\n\nConditional probability\nJoint probability\nMarginal probability\nBayes’ Rule\nLikelihood\nPosterior\nSimulation methods"
  },
  {
    "objectID": "slides/2023-09-25-downscaling.html#we-would-like-to-accurately-model-precipitation-at-high-spatial-and-temporal-resolution",
    "href": "slides/2023-09-25-downscaling.html#we-would-like-to-accurately-model-precipitation-at-high-spatial-and-temporal-resolution",
    "title": "Module 2 motivation: downscaling",
    "section": "We would like to accurately model precipitation at high spatial and temporal resolution",
    "text": "We would like to accurately model precipitation at high spatial and temporal resolution\n\nStormwater management (long-term desigm)\nWater resources management (subseasonal to multi-year planning)\nFire propagation (hourly to weekly)"
  },
  {
    "objectID": "slides/2023-09-25-downscaling.html#objectives",
    "href": "slides/2023-09-25-downscaling.html#objectives",
    "title": "Module 2 motivation: downscaling",
    "section": "Objectives",
    "text": "Objectives\n\nenhanced spatial detail\nmitigation of systematic ESM1 biases\ngeneration of variables not explicitly rendered by GCMs\n\n(Lanzante et al., 2018)\nEarth System Model"
  },
  {
    "objectID": "slides/2023-09-25-downscaling.html#earth-system-models",
    "href": "slides/2023-09-25-downscaling.html#earth-system-models",
    "title": "Module 2 motivation: downscaling",
    "section": "Earth System Models",
    "text": "Earth System Models"
  },
  {
    "objectID": "slides/2023-09-25-downscaling.html#dreary",
    "href": "slides/2023-09-25-downscaling.html#dreary",
    "title": "Module 2 motivation: downscaling",
    "section": "Dreary",
    "text": "Dreary"
  },
  {
    "objectID": "slides/2023-09-25-downscaling.html#drizzling",
    "href": "slides/2023-09-25-downscaling.html#drizzling",
    "title": "Module 2 motivation: downscaling",
    "section": "Drizzling",
    "text": "Drizzling"
  },
  {
    "objectID": "slides/2023-09-25-downscaling.html#challenges-summarized",
    "href": "slides/2023-09-25-downscaling.html#challenges-summarized",
    "title": "Module 2 motivation: downscaling",
    "section": "Challenges, summarized",
    "text": "Challenges, summarized\n\n\nESMs are tuned to get energy balance and large-scale circulation right, not local extremes\nESMs average over space and time\nLocal-scale precipitation can be tricky to model well"
  },
  {
    "objectID": "slides/2023-09-25-downscaling.html#supervised-downscaling",
    "href": "slides/2023-09-25-downscaling.html#supervised-downscaling",
    "title": "Module 2 motivation: downscaling",
    "section": "Supervised downscaling",
    "text": "Supervised downscaling\n\n\nInput: pairs \\((X_i, y_i)\\)\n\n\\(X_i\\): predictors (e.g., gridded rainfall)\n\\(y_i\\): predictand (e.g., gauge rainfall)\n\nGoal: learn a function \\(f\\) such that \\(f(X_i) \\approx y_i\\)\n\nMeasure quality of approximation through a loss function (more later)\n\nKey point: the \\(X_i\\) and \\(y_i\\) are observed at the same time\n\nExample: map satellite to radar data"
  },
  {
    "objectID": "slides/2023-09-25-downscaling.html#distributional-downscaling",
    "href": "slides/2023-09-25-downscaling.html#distributional-downscaling",
    "title": "Module 2 motivation: downscaling",
    "section": "Distributional downscaling",
    "text": "Distributional downscaling\n\n\nESMs simulate from the distribution of weather, given climate boundary conditions. For example:\n\nRun 100 ESM ensemble members over 20th century conditions\nStudy December 1, 1980 in all draws\nSome will be rainy, some will be dry; some cool, some warm\nStatistically meaningful, but not a forecast!\n\nNo pairs \\((X_i, y_i)\\). Instead we have samples \\(\\left\\{X_1, \\ldots, X_N \\right\\}\\) and \\(\\left\\{y_1, \\ldots, y_K \\right\\}\\)"
  },
  {
    "objectID": "slides/2023-09-25-downscaling.html#common-datasets",
    "href": "slides/2023-09-25-downscaling.html#common-datasets",
    "title": "Module 2 motivation: downscaling",
    "section": "Common datasets",
    "text": "Common datasets\n\n\nGauge data\nGridded observational products\n\nFor example: radar measurments are processed to produce gridded rainfall estimates\n\nReanalysis products\n\nUse assimilation to “digest” observations using a model\nGridded reconstructions of past weather\nState of the art is ERA5\n\nESM outputs\n\nHistorical runs\nCMIP: compare multiple models on standardized scenarios (e.g., RCP 2.6, 4.5, 8.5)\nSimulate from weather, conditional on boundary conditions"
  },
  {
    "objectID": "slides/2023-09-25-downscaling.html#bias-correction",
    "href": "slides/2023-09-25-downscaling.html#bias-correction",
    "title": "Module 2 motivation: downscaling",
    "section": "Bias correction",
    "text": "Bias correction\nSimplest form of downscaling. Usually \\(X\\) are samples from a climate model and \\(y\\) are observations. \\[\n\\begin{aligned}\n\\text{bias} &= \\mathbb{E}[X] - \\mathbb{E}[y] \\\\\n\\hat{y} &= X - \\text{bias}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nNote\n\n\nIs this a distributional or supervised method?"
  },
  {
    "objectID": "slides/2023-09-25-downscaling.html#quantile-quantile-mapping",
    "href": "slides/2023-09-25-downscaling.html#quantile-quantile-mapping",
    "title": "Module 2 motivation: downscaling",
    "section": "Quantile-quantile mapping",
    "text": "Quantile-quantile mapping"
  },
  {
    "objectID": "slides/2023-09-25-downscaling.html#correctorgan",
    "href": "slides/2023-09-25-downscaling.html#correctorgan",
    "title": "Module 2 motivation: downscaling",
    "section": "CorrectorGAN",
    "text": "CorrectorGAN\n\n\n (Price & Rasp, 2022)"
  },
  {
    "objectID": "slides/2023-09-25-downscaling.html#stationarity",
    "href": "slides/2023-09-25-downscaling.html#stationarity",
    "title": "Module 2 motivation: downscaling",
    "section": "Stationarity",
    "text": "Stationarity\nStationarity means the relationship between \\(X\\) and \\(y\\) does not change over time\n\nSupervised: \\(p(y | X)\\) or \\(y = f(X)\\) does not change over time\nDistributional: Corrections to the distribution do not change over time\n\nThis is never a perfect assumption"
  },
  {
    "objectID": "slides/2023-09-25-downscaling.html#your-task",
    "href": "slides/2023-09-25-downscaling.html#your-task",
    "title": "Module 2 motivation: downscaling",
    "section": "Your task",
    "text": "Your task\n\n\nGiven:\n\nHourly gridded rainfall data (small area)\nHourly large-scale pressure and temperature fields\nHourly gauge rainfall data at a single station\n\nDevelop a model to predict hourly gauge rainfall from the available datasets"
  },
  {
    "objectID": "slides/2023-09-25-downscaling.html#expectations",
    "href": "slides/2023-09-25-downscaling.html#expectations",
    "title": "Module 2 motivation: downscaling",
    "section": "Expectations",
    "text": "Expectations\n\nTry and compare at least two different approaaches\nQuantitative and qualitative evaluation of the models\nYou can use any methods you like:\n\nThose we cover in class\nThose you already know## References\n\n\n\n\n\n\n\n\nLanzante, J. R., Dixon, K. W., Nath, M. J., Whitlock, C. E., & Adams-Smith, D. (2018). Some Pitfalls in Statistical Downscaling of Future Climate. Bulletin of the American Meteorological Society, 99(4), 791–803. https://doi.org/10.1175/bams-d-17-0046.1\n\n\nPrice, I., & Rasp, S. (2022, March 23). Increasing the accuracy and resolution of precipitation forecasts using deep generative models. https://doi.org/10.48550/arXiv.2203.12297"
  },
  {
    "objectID": "slides/2023-09-29-module-1-wrapup.html#exam-1",
    "href": "slides/2023-09-29-module-1-wrapup.html#exam-1",
    "title": "Module 1 reflection",
    "section": "Exam 1",
    "text": "Exam 1\n\nAverage: 47.5\nStandard deviation: 15.9\nMaximum: 70\nMedian: 49.5\nEasiest: maximum likelihood\nHardest: computation, conditional probability"
  },
  {
    "objectID": "slides/2023-09-29-module-1-wrapup.html#how-ill-support-your-learning",
    "href": "slides/2023-09-29-module-1-wrapup.html#how-ill-support-your-learning",
    "title": "Module 1 reflection",
    "section": "How I’ll support your learning",
    "text": "How I’ll support your learning\n\nReorganized schedule – two modules not three\nPractice problems at start of class\nMore links to suggested reading for each lecture"
  },
  {
    "objectID": "slides/2023-09-29-module-1-wrapup.html#recommended-reading",
    "href": "slides/2023-09-29-module-1-wrapup.html#recommended-reading",
    "title": "Module 1 reflection",
    "section": "Recommended reading",
    "text": "Recommended reading\n\nThinkBayes: simple and conceptual online textbook on Bayesian statistics\nRethinking Statistics: a more serious text on Bayesian thinking and estimation\n\nCode examples on the website, including implementations in Turing\n\nAn Introduction to Statistical Learning: a machine learning perspective\n\nCode examples in R and Python\n\n\nThere are many other resources available online; use them critically."
  },
  {
    "objectID": "slides/2023-09-29-module-1-wrapup.html#how-you-need-to-support-your-learning",
    "href": "slides/2023-09-29-module-1-wrapup.html#how-you-need-to-support-your-learning",
    "title": "Module 1 reflection",
    "section": "How you need to support your learning",
    "text": "How you need to support your learning\n\nCome to office hours\nReview lecture notes\n\nDon’t just read\nWork through examples\nUnderstand why\nAsk questions\n\n\nI have encouraged you to use tools (Copilot, GPT, etc) to help you with computing syntax so you can spend more time on understanding concepts."
  },
  {
    "objectID": "slides/2023-09-29-module-1-wrapup.html#revisions-1",
    "href": "slides/2023-09-29-module-1-wrapup.html#revisions-1",
    "title": "Module 1 reflection",
    "section": "Revisions",
    "text": "Revisions\n\nExam I is graded\nFinal grades will be curved, not each exam\nRevisions will be allowed"
  },
  {
    "objectID": "slides/2023-09-29-module-1-wrapup.html#due-date",
    "href": "slides/2023-09-29-module-1-wrapup.html#due-date",
    "title": "Module 1 reflection",
    "section": "Due Date",
    "text": "Due Date\n\nFriday, October 6th at 11:00AM.\nHand in to me in class\n\nLegible and clear handwritten work OR\nType up your work1\nHand in your original exam with your revisions\n\n\nIf you type, disable tools such as GitHub Copilot"
  },
  {
    "objectID": "slides/2023-09-29-module-1-wrapup.html#academic-integrity",
    "href": "slides/2023-09-29-module-1-wrapup.html#academic-integrity",
    "title": "Module 1 reflection",
    "section": "Academic integrity",
    "text": "Academic integrity\n\nSubject to Rice Honor Code\nDO: consult class notes, textbooks, linked resources, or write your own code\nDO NOT: Consult with a classmate, search the internet, usie AI chat tools, or otherwise collaborate is not permitted\nAsk if you’re not sure\n\nIf you have questions about what is permitted, please ask."
  },
  {
    "objectID": "slides/2023-09-29-module-1-wrapup.html#instructions",
    "href": "slides/2023-09-29-module-1-wrapup.html#instructions",
    "title": "Module 1 reflection",
    "section": "Instructions",
    "text": "Instructions\nFor each problem:\n\nState how many points you earned on the original exam.\nDerive the correct answer. Your answer should be clearly written and easy to follow.\nExplain why your original answer was incorrect and what confused you (no revisions are needed if your original answer was correct!)"
  },
  {
    "objectID": "slides/2023-09-29-module-1-wrapup.html#grading",
    "href": "slides/2023-09-29-module-1-wrapup.html#grading",
    "title": "Module 1 reflection",
    "section": "Grading",
    "text": "Grading\n\nOn each problem, you will earn up to 60% of the points you missed\n\nGrading on the revision will be more strict than on the original exam\n\nAdditional 10% for clear and insightful explanation of your original mistake (where applicable)\nT/F questions: up to 50% for a clear and correct explanation of why the statement is true or false\nIf your revision is worse than the original, your score will not be lowered."
  },
  {
    "objectID": "slides/2023-09-29-module-1-wrapup.html#suggestions",
    "href": "slides/2023-09-29-module-1-wrapup.html#suggestions",
    "title": "Module 1 reflection",
    "section": "Suggestions",
    "text": "Suggestions\n\n\nPrecipitation:\n\nCDF: \\(F(x) = P(X \\leq x)\\)\nCan assume that \\(F(0)=0.6\\)\n\nConditional probability: think hard about how to define \\(A\\) and \\(B\\)\nReturn period: read the wording carefully (“according to the distribution shown…”, “…true return period”)\nRecording of review session"
  },
  {
    "objectID": "slides/2023-09-29-module-1-wrapup.html#bayes-rule",
    "href": "slides/2023-09-29-module-1-wrapup.html#bayes-rule",
    "title": "Module 1 reflection",
    "section": "Bayes Rule",
    "text": "Bayes Rule\nA doctor is called to see a sick child. The doctor has prior information that 90% of sick children in that neighborhood have the flu, while the other 10% are sick with measles. Assume for simplicity that there are no other illnesses and that no children have both the flu and measles.\nA well-known symptom of measles is a rash. The probability of having a rash if one has measles is 0.95. However, occasionally children with flu also develop rash, and the probability of having a rash if one has flu is 0.08. Upon examining the child, the doctor finds a rash. What is the probability that the child has measles?"
  },
  {
    "objectID": "slides/2023-09-29-module-1-wrapup.html#analytic-posterior",
    "href": "slides/2023-09-29-module-1-wrapup.html#analytic-posterior",
    "title": "Module 1 reflection",
    "section": "Analytic posterior",
    "text": "Analytic posterior\nWe collect some count data and model it using a Poisson likelihood. The Poisson likelihood is given by: \\[\np(y_i | \\lambda) = \\frac{\\lambda^{y_i} e^{-\\lambda}}{y_i!}\n\\] where \\(y_i\\) is the number of counts and \\(\\lambda\\) is the rate parameter of the Poisson distribution. We want to do inference on \\(\\lambda\\). We have a prior belief that \\(\\lambda\\) is distributed as an Exponential distribution with the modified parameterization: \\[\np(\\lambda | \\theta) = \\frac{1}{\\theta} e^{-\\frac{\\lambda}{\\theta}}\n\\] After collecting data \\(y_1, y_2, \\ldots, y_n\\), what is the posterior distribution of \\(\\lambda\\) given our prior parameter \\(\\theta\\)?## Next week:\n\nMonday: Gridded climate data lab\nWednesday: Generalized Linear Models\nFriday: Loss functions"
  },
  {
    "objectID": "slides/2023-10-20-hyperparameters.html#parametric-function-approximation",
    "href": "slides/2023-10-20-hyperparameters.html#parametric-function-approximation",
    "title": "Hyperparameter tuning",
    "section": "Parametric function approximation",
    "text": "Parametric function approximation\n\nToday, we continue our “regression” example which is a supervised learning problem.\nRecall: we model an unknown function \\(f\\) using some parameters \\(\\theta\\). Thus, finding \\(\\hat{f}\\) is equivalent to choosing appropriate \\(\\theta\\).\nIdeally, we want to maximize performance on new data."
  },
  {
    "objectID": "slides/2023-10-20-hyperparameters.html#overfitting",
    "href": "slides/2023-10-20-hyperparameters.html#overfitting",
    "title": "Hyperparameter tuning",
    "section": "Overfitting",
    "text": "Overfitting\n\nMATLAB/Simulink"
  },
  {
    "objectID": "slides/2023-10-20-hyperparameters.html#hyperparameters",
    "href": "slides/2023-10-20-hyperparameters.html#hyperparameters",
    "title": "Hyperparameter tuning",
    "section": "Hyperparameters",
    "text": "Hyperparameters\nMany ML models (like a Random Forest) have nested parameters, sometimes called “hyperparameters”\n\n\nWhen we “fit” a model, we are finding the best parameters\n\nwhere to partition the region\ni.e., the best tree\n\nBut, we also have to choose the number of trees\n\nthis is a hyperparameter\n\nHyperparameters are not optimized during model training"
  },
  {
    "objectID": "slides/2023-10-20-hyperparameters.html#grid-search",
    "href": "slides/2023-10-20-hyperparameters.html#grid-search",
    "title": "Hyperparameter tuning",
    "section": "Grid search",
    "text": "Grid search\nVery simple idea\n\n\nPredefine a set of \\(S\\) hyperparameter sets\nFor \\(s=1, \\ldots, S\\) fit the model with the \\(k\\)th hyperparameter set\nChoose the best model"
  },
  {
    "objectID": "slides/2023-10-20-hyperparameters.html#cross-validation",
    "href": "slides/2023-10-20-hyperparameters.html#cross-validation",
    "title": "Hyperparameter tuning",
    "section": "Cross-validation",
    "text": "Cross-validation\n\nKey idea: we want to evaluate the model on data that was not used to fit the model (out of sample)\n\n\n\n\nSplit the data into \\(K\\) folds\nFor \\(k=1, \\ldots, K\\)\n\nFit the model on all folds except the \\(k\\)th\nEvaluate the model on the \\(k\\)th fold\n\nAverage the performance across all folds\n\n\n\n\nCross-validation helps to reduce the variance of estimated model performance. However, cross-validated estimates of model performance are still biased – essentially, you can overfit hyperparameters"
  },
  {
    "objectID": "slides/2023-10-20-hyperparameters.html#train-test-split",
    "href": "slides/2023-10-20-hyperparameters.html#train-test-split",
    "title": "Hyperparameter tuning",
    "section": "Train-test split",
    "text": "Train-test split\nWe want to know how well the model performs on new data!\n\n\nSplit the data into a training set and a test set\n\nOften 80-20 or 70-30\nFor spatially or temporally structured data, structured splits essential\n\nFit the model on the training set\n\nIncluding any cross-validation\n\nEvaluate the model on the test set as a final step"
  },
  {
    "objectID": "slides/2023-10-20-hyperparameters.html#data",
    "href": "slides/2023-10-20-hyperparameters.html#data",
    "title": "Hyperparameter tuning",
    "section": "Data",
    "text": "Data\n\ndata = dataset(\"ISLR\", \"Hitters\")\ndropmissing!(data, :Salary)\nnumerical_cols = [col for col in names(data) if eltype(data[!, col]) &lt;: Number]\ndata = data[:, numerical_cols]\ndescribe(data)\n\n17×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nFloat64\nReal\nFloat64\nReal\nInt64\nDataType\n\n\n\n\n1\nAtBat\n403.643\n19\n413.0\n687\n0\nInt32\n\n\n2\nHits\n107.829\n1\n103.0\n238\n0\nInt32\n\n\n3\nHmRun\n11.6198\n0\n9.0\n40\n0\nInt32\n\n\n4\nRuns\n54.7452\n0\n52.0\n130\n0\nInt32\n\n\n5\nRBI\n51.4867\n0\n47.0\n121\n0\nInt32\n\n\n6\nWalks\n41.1141\n0\n37.0\n105\n0\nInt32\n\n\n7\nYears\n7.31179\n1\n6.0\n24\n0\nInt32\n\n\n8\nCAtBat\n2657.54\n19\n1931.0\n14053\n0\nInt32\n\n\n9\nCHits\n722.186\n4\n516.0\n4256\n0\nInt32\n\n\n10\nCHmRun\n69.2395\n0\n40.0\n548\n0\nInt32\n\n\n11\nCRuns\n361.221\n2\n250.0\n2165\n0\nInt32\n\n\n12\nCRBI\n330.418\n3\n230.0\n1659\n0\nInt32\n\n\n13\nCWalks\n260.266\n1\n174.0\n1566\n0\nInt32\n\n\n14\nPutOuts\n290.711\n0\n224.0\n1377\n0\nInt32\n\n\n15\nAssists\n118.76\n0\n45.0\n492\n0\nInt32\n\n\n16\nErrors\n8.59316\n0\n7.0\n32\n0\nInt32\n\n\n17\nSalary\n535.926\n67.5\n425.0\n2460.0\n0\nFloat64"
  },
  {
    "objectID": "slides/2023-10-20-hyperparameters.html#partition-data",
    "href": "slides/2023-10-20-hyperparameters.html#partition-data",
    "title": "Hyperparameter tuning",
    "section": "Partition data",
    "text": "Partition data\nR is the target, everything else is a feature\n\ny, X = unpack(data, ==(:Salary); rng=123)\n\n\n70% training, 30% testing"
  },
  {
    "objectID": "slides/2023-10-20-hyperparameters.html#load-the-model",
    "href": "slides/2023-10-20-hyperparameters.html#load-the-model",
    "title": "Hyperparameter tuning",
    "section": "Load the model",
    "text": "Load the model\n\nRandomForestRegressor = @load RandomForestRegressor pkg = DecisionTree\nmodel = RandomForestRegressor()\n\nimport MLJDecisionTreeInterface ✔\n\n\nRandomForestRegressor(\n  max_depth = -1, \n  min_samples_leaf = 1, \n  min_samples_split = 2, \n  min_purity_increase = 0.0, \n  n_subfeatures = -1, \n  n_trees = 100, \n  sampling_fraction = 0.7, \n  feature_importance = :impurity, \n  rng = Random._GLOBAL_RNG())"
  },
  {
    "objectID": "slides/2023-10-20-hyperparameters.html#train-the-model",
    "href": "slides/2023-10-20-hyperparameters.html#train-the-model",
    "title": "Hyperparameter tuning",
    "section": "Train the model",
    "text": "Train the model\n\nmach = machine(model, X, y)\ntrain, test = partition(eachindex(y), 0.7; shuffle=true, rng=123)\nfit!(mach; rows=train)\n\ntrained Machine; caches model-specific representations of data\n  model: RandomForestRegressor(max_depth = -1, …)\n  args: \n    1:  Source @204 ⏎ Table{AbstractVector{Count}}\n    2:  Source @693 ⏎ AbstractVector{Continuous}"
  },
  {
    "objectID": "slides/2023-10-20-hyperparameters.html#predict-on-the-test-set",
    "href": "slides/2023-10-20-hyperparameters.html#predict-on-the-test-set",
    "title": "Hyperparameter tuning",
    "section": "Predict on the test set",
    "text": "Predict on the test set\n\ny_pred_test = predict(mach, X[test, :])\nrms_test = root_mean_squared_error(y_pred_test, y[test])\n\ny_pred_train = predict(mach, X[train, :])\nrms_train = root_mean_squared_error(y_pred_train, y[train])\n\nrms_train, rms_test\n\n(160.31411979262353, 188.18990826667437)"
  },
  {
    "objectID": "slides/2023-10-20-hyperparameters.html#visualize",
    "href": "slides/2023-10-20-hyperparameters.html#visualize",
    "title": "Hyperparameter tuning",
    "section": "Visualize",
    "text": "Visualize\n\n\nCode\nps = map(zip([train, test], [\"Train\", \"Test\"])) do (idx, name)\n    scatter(\n        y[idx],\n        predict(mach, X[idx, :]);\n        xlabel=\"Actual\",\n        ylabel=\"Predicted\",\n        label=\"Model\",\n        title=name,\n        legend=:bottomright\n    )\n    Plots.abline!(1, 0; label=\"1:1 line\")\nend\nplot(ps...; link=:both, size=(1000, 500))"
  },
  {
    "objectID": "slides/2023-10-20-hyperparameters.html#tuning",
    "href": "slides/2023-10-20-hyperparameters.html#tuning",
    "title": "Hyperparameter tuning",
    "section": "Tuning",
    "text": "Tuning\n\nn_trees_range = range(model, :n_trees; lower=10, upper=150, scale=:log10)\nn_subfeatures_range = range(model, :n_subfeatures; lower=1, upper=size(X, 2))\n\ntuning = TunedModel(;\n    model=model,\n    tuning=Grid(; goal=25),  # Using a grid search with 25 points in total\n    resampling=CV(; nfolds=5, rng=123),  # 5-fold cross-validation\n    measure=root_mean_squared_error,  # Evaluation metric\n    ranges=[n_trees_range, n_subfeatures_range]\n)"
  },
  {
    "objectID": "slides/2023-10-20-hyperparameters.html#fit-and-id-best-model",
    "href": "slides/2023-10-20-hyperparameters.html#fit-and-id-best-model",
    "title": "Hyperparameter tuning",
    "section": "Fit and ID best model",
    "text": "Fit and ID best model\n\ntuned_mach = machine(tuning, X, y)\nfit!(tuned_mach; rows=train)\nbest_model = fitted_params(tuned_mach).best_model\n\nprintln(\"Best n_trees: \", best_model.n_trees)\nprintln(\"Best n_subfeatures: \", best_model.n_subfeatures)\n\nBest n_trees: 10\nBest n_subfeatures: 12"
  },
  {
    "objectID": "slides/2023-10-20-hyperparameters.html#fit-the-best-model-and-make-predictions",
    "href": "slides/2023-10-20-hyperparameters.html#fit-the-best-model-and-make-predictions",
    "title": "Hyperparameter tuning",
    "section": "Fit the best model and make predictions",
    "text": "Fit the best model and make predictions\n\nbest_mach = machine(best_model, X, y)\nfit!(best_mach; rows=train)\n\ny_pred_test2 = predict(best_mach, X[test, :])\nrms_test2 = root_mean_squared_error(y_pred_test2, y[test])\n\ny_pred_train2 = predict(best_mach, X[train, :])\nrms_train2 = root_mean_squared_error(y_pred_train2, y[train])\n\nrms_train2, rms_test2\n\n(186.9403933790913, 193.79639432645558)\n\n\n\n\n\n\n\n\nImportant\n\n\nWe have achieved better performance on the training set, but worse performance on the test set!"
  },
  {
    "objectID": "slides/2023-10-20-hyperparameters.html#more-resources",
    "href": "slides/2023-10-20-hyperparameters.html#more-resources",
    "title": "Hyperparameter tuning",
    "section": "More resources",
    "text": "More resources\n\nMLJ Docs – maintained by Julia AI organization / Turing Institute\nScikitLearn Docs – uses Scikit-Learn (Python package) interface and wraps models"
  },
  {
    "objectID": "slides/2023-10-20-hyperparameters.html#project",
    "href": "slides/2023-10-20-hyperparameters.html#project",
    "title": "Hyperparameter tuning",
    "section": "Project",
    "text": "Project\nPosted on Canvas.\nIf you’re not sure what problem to tackle, some ideas are:\n\n“Downscaling”: Map coarse resolution data to fine resolution\n“Forecasting”: Predict future values of a variable\n\nQuestions?"
  },
  {
    "objectID": "slides/2023-10-20-hyperparameters.html#exams",
    "href": "slides/2023-10-20-hyperparameters.html#exams",
    "title": "Hyperparameter tuning",
    "section": "Exams",
    "text": "Exams\nExam 1: I’m working through your corrections\nExam 2: next Friday!"
  },
  {
    "objectID": "slides/2023-10-20-hyperparameters.html#means-with-missing-values",
    "href": "slides/2023-10-20-hyperparameters.html#means-with-missing-values",
    "title": "Hyperparameter tuning",
    "section": "Means with missing values",
    "text": "Means with missing values\nLet’s say you have an array with some missing values, and you want to take the mean across a particular dimension. How can you do this?\n\nX = convert(Array{Union{Float64,Missing},3}, rand(11, 10, 9))\n\nK = 10\nX[shuffle(1:length(X))[1:K]] .= missing\n\n10-element view(reshape(::Array{Union{Missing, Float64}, 3}, 990), [12, 668, 692, 312, 248, 801, 427, 782, 247, 346]) with eltype Union{Missing, Float64}:\n missing\n missing\n missing\n missing\n missing\n missing\n missing\n missing\n missing\n missing\n\n\nWe’d like to do this, but we’ll get msising values\n\nX_mean = mean(X; dims=3)\nsum(ismissing.(X_mean))\n\n9\n\n\nInstead, we can do:\n\nX_mean = [mean(skipmissing(X[i, j, :])) for i in axes(X, 1), j in axes(X, 2)]\n@assert size(X_mean) == (11, 10)"
  },
  {
    "objectID": "slides/2023-11-08-gev-nonstationary.html#independent-and-identically-distributed",
    "href": "slides/2023-11-08-gev-nonstationary.html#independent-and-identically-distributed",
    "title": "Nonstationary GEV",
    "section": "Independent and identically distributed",
    "text": "Independent and identically distributed\n\n\nExtreme value theory is based on the assumption that the data are independent and identically distributed (iid).\n\nEach draw comes from the same distribution\n\nThis is violated by\n\nClimate change\nLow-frequency variability\nMemory processes"
  },
  {
    "objectID": "slides/2023-11-08-gev-nonstationary.html#stationarity",
    "href": "slides/2023-11-08-gev-nonstationary.html#stationarity",
    "title": "Nonstationary GEV",
    "section": "Stationarity",
    "text": "Stationarity\n\n\nA stationary process is a stochastic process whose unconditional joint probability distribution does not change when shifted in time\n\nA stochastic process is a model for a sequence of random variables (e.g.: random walk, MCMC)\n\n“Stationarity is dead” (Milly et al., 2008)"
  },
  {
    "objectID": "slides/2023-11-08-gev-nonstationary.html#climate-change-impacts-overview",
    "href": "slides/2023-11-08-gev-nonstationary.html#climate-change-impacts-overview",
    "title": "Nonstationary GEV",
    "section": "Climate change impacts overview",
    "text": "Climate change impacts overview\n\n\nThermodynamic effects\n\nClausius-Clapeyron relation: \\(e_s(T) = e_0 \\exp\\left(\\frac{L_v}{R_v T}\\right)\\)\n\nAbout 7% per degree K\n\n\nDynamic effects\n\nLonger, hotter summers\n“Tropics” expand poleward\nStorm structure and intensity\n\n\n\nThe rest of this section draws from Seneviratne et al. (2021) executive summary"
  },
  {
    "objectID": "slides/2023-11-08-gev-nonstationary.html#climate-change-impacts-on-precipitation",
    "href": "slides/2023-11-08-gev-nonstationary.html#climate-change-impacts-on-precipitation",
    "title": "Nonstationary GEV",
    "section": "Climate change impacts on precipitation",
    "text": "Climate change impacts on precipitation\n\nThe frequency and intensity of heavy precipitation events have likely increased at the global scale over a  majority of land regions with good observational coverage. Heavy precipitation has likely increased on the continental scale over three continents: North America, Europe, and Asia.\nHeavy precipitation will generally become more frequent and more intense with additional global warming. At a global warming level of 4°C relative to the pre-industrial level, very rare (e.g., one in 10 or more years) heavy precipitation events would become more frequent and more intense than in the recent past, on the global scale (virtually certain) and in all continents and AR6 regions. The increase in frequency and intensity is extremely likely for most continents and very likely for most AR6 regions.\nThe projected increase in the intensity of extreme precipitation translates to an increase in the frequency and magnitude of pluvial floods  – surface water and flash floods  – (high confidence), as pluvial flooding results from precipitation intensity exceeding the capacity of natural and artificial drainage systems."
  },
  {
    "objectID": "slides/2023-11-08-gev-nonstationary.html#climate-change-impacts-on-river-floods",
    "href": "slides/2023-11-08-gev-nonstationary.html#climate-change-impacts-on-river-floods",
    "title": "Nonstationary GEV",
    "section": "Climate change impacts on river floods",
    "text": "Climate change impacts on river floods\n\nSignificant trends in peak streamflow have been observed in some regions over the past decades (high confidence).\n\nThe  seasonality of river floods has changed in cold regions where snow-melt is involved, with an earlier occurrence of peak streamflow (high confidence).\n\nGlobal hydrological models project a  larger fraction of land areas to be affected by an increase in river floods than by a  decrease in river floods (medium confidence)."
  },
  {
    "objectID": "slides/2023-11-08-gev-nonstationary.html#climate-change-impacts-on-extreme-temperatures",
    "href": "slides/2023-11-08-gev-nonstationary.html#climate-change-impacts-on-extreme-temperatures",
    "title": "Nonstationary GEV",
    "section": "Climate change impacts on extreme temperatures",
    "text": "Climate change impacts on extreme temperatures\n\nThe frequency and intensity of hot extremes (including heatwaves) have increased, and those of cold extremes have decreased on the global scale since 1950 (virtually certain). This also applies at regional scale, with more than 80% of AR6 regions1 showing similar changes assessed to be at least likely.\nHuman-induced greenhouse gas forcing is the main driver of the observed changes in hot and cold extremes on the global scale (virtually certain) and on most continents (very likely).\nThe frequency and intensity of hot extremes will continue to increase and those of cold extremes will continue to decrease, at global and continental scales and in nearly all inhabited regions1 with increasing global warming levels."
  },
  {
    "objectID": "slides/2023-11-08-gev-nonstationary.html#climate-change-impacts-on-tropical-cyclones",
    "href": "slides/2023-11-08-gev-nonstationary.html#climate-change-impacts-on-tropical-cyclones",
    "title": "Nonstationary GEV",
    "section": "Climate change impacts on tropical cyclones",
    "text": "Climate change impacts on tropical cyclones\n\nThe average and maximum rain rates associated with tropical cyclones (TCs), extratropical cyclones and atmospheric rivers across the globe, and severe convective storms in some regions, increase  in a  warming world (high confidence).\nIt is likely that the global proportion of Category 3–5 tropical cyclone instances2 has increased over the past four decades.\nThe proportion of intense TCs, average peak TC wind speeds, and peak wind speeds of the most intense TCs will increase on the global scale with increasing global warming (high confidence).\nThere is low confidence in past changes of maximum wind speeds and other measures of dynamical intensity of extratropical cyclones. Future wind speed changes are expected to be small, although poleward shifts in the storm tracks could lead to substantial changes in extreme wind speeds in some regions (medium confidence)."
  },
  {
    "objectID": "slides/2023-11-08-gev-nonstationary.html#el-niño-southern-oscillation",
    "href": "slides/2023-11-08-gev-nonstationary.html#el-niño-southern-oscillation",
    "title": "Nonstationary GEV",
    "section": "El Niño-Southern Oscillation",
    "text": "El Niño-Southern Oscillation"
  },
  {
    "objectID": "slides/2023-11-08-gev-nonstationary.html#motivation",
    "href": "slides/2023-11-08-gev-nonstationary.html#motivation",
    "title": "Nonstationary GEV",
    "section": "Motivation",
    "text": "Motivation\n\nFagnant et al. (2020)"
  },
  {
    "objectID": "slides/2023-11-08-gev-nonstationary.html#get-data",
    "href": "slides/2023-11-08-gev-nonstationary.html#get-data",
    "title": "Nonstationary GEV",
    "section": "Get data",
    "text": "Get data\nData is from NOAA GHCND\n\nannmax_prcp = CSV.read(\"data/hobby-annmax.csv\", DataFrame)\nannmax_prcp[!, :date] = Dates.Date.(annmax_prcp[!, :date], \"mm/dd/yyyy\")\nannmax_prcp[!, :year] = Dates.year.(annmax_prcp[!, :date])\nfirst(annmax_prcp, 5)\n\n5×3 DataFrame\n\n\n\nRow\ndate\nprcp_in\nyear\n\n\n\nDate\nFloat64\nInt64\n\n\n\n\n1\n1931-11-24\n2.53\n1931\n\n\n2\n1932-08-14\n4.17\n1932\n\n\n3\n1933-07-23\n2.9\n1933\n\n\n4\n1934-04-06\n3.68\n1934\n\n\n5\n1935-12-08\n2.99\n1935\n\n\n\n\n\n\n\nKeep only years where there are at least 350 days of data"
  },
  {
    "objectID": "slides/2023-11-08-gev-nonstationary.html#exploratory-visualization",
    "href": "slides/2023-11-08-gev-nonstationary.html#exploratory-visualization",
    "title": "Nonstationary GEV",
    "section": "Exploratory visualization",
    "text": "Exploratory visualization\n\n\nCode\nplot(\n    annmax_prcp.year,\n    annmax_prcp.prcp_in;\n    marker=:circ,\n    label=false,\n    xlabel=\"Year\",\n    ylabel=\"Ann. Max. Daily Precipitation [in]\",\n    title=\"Houston Hobby Airport\"\n)"
  },
  {
    "objectID": "slides/2023-11-08-gev-nonstationary.html#stationary-analysis",
    "href": "slides/2023-11-08-gev-nonstationary.html#stationary-analysis",
    "title": "Nonstationary GEV",
    "section": "Stationary analysis",
    "text": "Stationary analysis\n\nhobby_mle = gevfit(annmax_prcp.prcp_in)\np0 = plot_rl_extremes(hobby_mle, annmax_prcp.prcp_in)\ntitle!(p0, \"Stationary Model\")"
  },
  {
    "objectID": "slides/2023-11-08-gev-nonstationary.html#rank-trend",
    "href": "slides/2023-11-08-gev-nonstationary.html#rank-trend",
    "title": "Nonstationary GEV",
    "section": "Rank Trend",
    "text": "Rank Trend\n\n\n\n\n\n\nTip\n\n\nThe Mann-Kendall test is commonly used to assess the presence of a trend in time series data.\n\n\n\n\nprcp_rank = invperm(sortperm(annmax_prcp.prcp_in))\nrank_cor = round(cor(prcp_rank, annmax_prcp.year); digits=2)\nscatter(\n    annmax_prcp.year,\n    prcp_rank;\n    label=false,\n    xlabel=\"Year\",\n    ylabel=\"Rank\",\n    title=\"Rank Correlation: $rank_cor\"\n)"
  },
  {
    "objectID": "slides/2023-11-08-gev-nonstationary.html#rolling-window",
    "href": "slides/2023-11-08-gev-nonstationary.html#rolling-window",
    "title": "Nonstationary GEV",
    "section": "Rolling window",
    "text": "Rolling window\n\nAs in Fagnant et al. (2020)\nPro:\n\nSimple\nInterpretable\n\nCon:\n\nNoisy\nYou lose extremes\n\n\n\nLess bias, more variance"
  },
  {
    "objectID": "slides/2023-11-08-gev-nonstationary.html#regression-models",
    "href": "slides/2023-11-08-gev-nonstationary.html#regression-models",
    "title": "Nonstationary GEV",
    "section": "Regression models",
    "text": "Regression models\n\nIn linear regression and GLMs, every data point is drawn from its own distribution. This distribution depends on some parameters and some covariates.\n\n\nEach data point’s likelihood can be expressed relative to that particular distribution.\n\n\nWe can apply this idea to extreme value models (e.g., the GEV)"
  },
  {
    "objectID": "slides/2023-11-08-gev-nonstationary.html#types-of-regression-models",
    "href": "slides/2023-11-08-gev-nonstationary.html#types-of-regression-models",
    "title": "Nonstationary GEV",
    "section": "Types of regression models",
    "text": "Types of regression models\nWe have limitless flexibility!\n\n\nWhat varies? ::: {.incremental} 1. Location parameter: \\(\\mu(t) = f(X(t))\\) 1. Scale parameter: \\(\\sigma(t) = f(X(t))\\) 1. Both location and scale 1. Scale and coefficient of variation: \\(\\mu(t) = \\phi \\sigma(t)\\) 1. Varying shape is impractical but allowed\n\nHow does it vary? ::: {.incremental} 1. Linear: \\(\\theta(t) = \\alpha + \\beta_1 X_1(t) + \\beta_2 X_2(t) + \\cdots\\) 1. Occasionally more (splines, GAMS, etc) 1. Anything is allowed, not everything is practical\n\n\n:::"
  },
  {
    "objectID": "slides/2023-11-08-gev-nonstationary.html#choosing-covariates",
    "href": "slides/2023-11-08-gev-nonstationary.html#choosing-covariates",
    "title": "Nonstationary GEV",
    "section": "Choosing covariates",
    "text": "Choosing covariates\nSome general guidance:\n\n\nTheory / domain knowledge is helpful\nFor precip, log of CO2 is a good variable\n\nisolates global warming from ENSO"
  },
  {
    "objectID": "slides/2023-11-08-gev-nonstationary.html#read-in-covariates",
    "href": "slides/2023-11-08-gev-nonstationary.html#read-in-covariates",
    "title": "Nonstationary GEV",
    "section": "Read in covariates",
    "text": "Read in covariates\n\nco2 = CSV.read(\"data/global_mean_CO2.csv\", DataFrame)\nco2[!, :log_CO2] = log.(co2[!, :CO2_ppm])\nmrg = innerjoin(co2, annmax_prcp; on=:year)\nplot(mrg[!, :year], mrg[!, :CO2_ppm]; label=false, xlabel=\"Year\", ylabel=\"CO2 (ppm)\")"
  },
  {
    "objectID": "slides/2023-11-08-gev-nonstationary.html#location-trend",
    "href": "slides/2023-11-08-gev-nonstationary.html#location-trend",
    "title": "Nonstationary GEV",
    "section": "Location trend",
    "text": "Location trend\n\\[\n\\begin{aligned}\ny_t &\\sim \\text{GEV} \\left( \\mu_t, \\sigma, \\xi \\right) \\\\\n\\mu_t &= \\alpha + \\beta X_t\n\\end{aligned}\n\\]\nIf we are OK with default uniform priors (🫤) we can use Extremes.jl\n\nfit1 = gevfitbayes(mrg, :prcp_in; locationcovid=[:log_CO2])\np1 = plot_rl_extremes(fit1, annmax_prcp.prcp_in)\ntitle!(p1, \"Location Trend: 2017\")"
  },
  {
    "objectID": "slides/2023-11-08-gev-nonstationary.html#scale-trend",
    "href": "slides/2023-11-08-gev-nonstationary.html#scale-trend",
    "title": "Nonstationary GEV",
    "section": "Scale trend",
    "text": "Scale trend\n\nfit2 = gevfitbayes(mrg, :prcp_in; logscalecovid=[:log_CO2])\np2 = plot_rl_extremes(fit2, annmax_prcp.prcp_in)\ntitle!(p2, \"Scale Trend: 2017\")"
  },
  {
    "objectID": "slides/2023-11-08-gev-nonstationary.html#both",
    "href": "slides/2023-11-08-gev-nonstationary.html#both",
    "title": "Nonstationary GEV",
    "section": "Both",
    "text": "Both\n\nfit3 = gevfitbayes(mrg, :prcp_in; locationcovid=[:log_CO2], logscalecovid=[:log_CO2])\np3 = plot_rl_extremes(fit3, annmax_prcp.prcp_in)\ntitle!(p3, \"Location and Scale Trend: 2017\")"
  },
  {
    "objectID": "slides/2023-11-08-gev-nonstationary.html#comparison",
    "href": "slides/2023-11-08-gev-nonstationary.html#comparison",
    "title": "Nonstationary GEV",
    "section": "Comparison",
    "text": "Comparison\n\nplot(p0, p1, p2, p3; link=:all, layout=(2, 2), size=(900, 700), ylims=(0, 25))"
  },
  {
    "objectID": "slides/2023-11-08-gev-nonstationary.html#key-ideas",
    "href": "slides/2023-11-08-gev-nonstationary.html#key-ideas",
    "title": "Nonstationary GEV",
    "section": "Key ideas",
    "text": "Key ideas\n\nNonstationary models reduce bias, increase variance\n\nPhysical process knowledge is valuable\nLarge parametric uncertainty\n\nModel comparison is challenging\n\n\n\n\n\n\n\nFagnant, C., Gori, A., Sebastian, A., Bedient, P. B., & Ensor, K. B. (2020). Characterizing spatiotemporal trends in extreme precipitation in Southeast Texas. Natural Hazards, 104(2), 1597–1621. https://doi.org/10.1007/s11069-020-04235-x\n\n\nMilly, P. C. D., Betancourt, J., Falkenmark, M., Hirsch, R. M., Kundzewicz, Z. W., Lettenmaier, D. P., & Stouffer, R. J. (2008). Stationarity is dead: Whither water management? Science, 319(5863), 573–574. https://doi.org/10.1126/science.1151915\n\n\nSeneviratne, S. I., Zhang, X., Adnan, M., Badi, W., Dereczynski, C., Di Luca, A., et al. (2021). Weather and climate extreme events in a changing climate. In V. Masson-Delmotte, P. Zhai, A. Pirani, S. L. Connors, C. Péan, S. Berger, et al. (Eds.), Climate change 2021: The physical science basis. Contribution of working group I to the sixth assessment report of the intergovernmental panel on climate change. Book section, Cambridge, UK and New York, NY, USA: Cambridge University Press. https://doi.org/10.1017/9781009157896.013"
  },
  {
    "objectID": "slides/2023-10-25-review.html#tbd",
    "href": "slides/2023-10-25-review.html#tbd",
    "title": "Module II Review",
    "section": "TBD",
    "text": "TBD\nThis is a placeholder for a planned project workshop time. The schedule will be modified as needed."
  },
  {
    "objectID": "slides/2023-10-23-workshop.html#tbd",
    "href": "slides/2023-10-23-workshop.html#tbd",
    "title": "Project Workshop",
    "section": "TBD",
    "text": "TBD\nThis is a placeholder for a planned project workshop time. The schedule will be modified as needed."
  },
  {
    "objectID": "slides/2023-10-06-loss-functions.html#practice-problem",
    "href": "slides/2023-10-06-loss-functions.html#practice-problem",
    "title": "Intro to Machine Learning",
    "section": "Practice Problem",
    "text": "Practice Problem\nWe recorded the number of accidents and average vehicle speed in a city over several months:\n\n\n\nMonth\nAverage Speed (km/h)\nNumber of Accidents\n\n\n\n\nJan\n45\n10\n\n\nFeb\n50\n8\n\n\nMar\n55\n12\n\n\nApr\n60\n15\n\n\n\n\nIdentify a suitable distribution for modeling the number of accidents.\nWrite down the likelihood for the chosen distribution.\nSuggest a link function to relate the average speed to the number of accidents.\nBriefly explain why the chosen link function is appropriate."
  },
  {
    "objectID": "slides/2023-10-06-loss-functions.html#logistics",
    "href": "slides/2023-10-06-loss-functions.html#logistics",
    "title": "Intro to Machine Learning",
    "section": "Logistics",
    "text": "Logistics\n\nTurn in exam revisions now\nRevised course schedule\nList of recommended readings"
  },
  {
    "objectID": "slides/2023-10-06-loss-functions.html#example-data",
    "href": "slides/2023-10-06-loss-functions.html#example-data",
    "title": "Intro to Machine Learning",
    "section": "Example data",
    "text": "Example data\nWe want to make predictions about the value of \\(y\\) given some \\(x\\).\n\nThe true function will be \\[\nf(x) = 2x + x \\sin(2 \\pi x)\n\\] but let’s assume we don’t know it."
  },
  {
    "objectID": "slides/2023-10-06-loss-functions.html#viz",
    "href": "slides/2023-10-06-loss-functions.html#viz",
    "title": "Intro to Machine Learning",
    "section": "Viz",
    "text": "Viz\n\n\nCode\nf(x) = 2x + x * sin(2pi * x)\nN = 100\nRandom.seed!(1017)\nx = rand(Uniform(0, 2), N)\nX = hcat(ones(N), x)\ny = f.(x) .+ rand(Normal(0, 1), N)\np_base = scatter(x, y; label=\"Obs\", xlabel=L\"$x$\", ylabel=L\"$y$\")\np1 = plot(p_base)\nplot!(p1, f; label=L\"$f(x)$\")"
  },
  {
    "objectID": "slides/2023-10-06-loss-functions.html#linear-regression",
    "href": "slides/2023-10-06-loss-functions.html#linear-regression",
    "title": "Intro to Machine Learning",
    "section": "Linear regression",
    "text": "Linear regression\n\\[\ny | X \\sim \\mathcal{N}(X^T \\beta, \\sigma^2 I)\n\\]\n\n\n\n\n\n\nNotation\n\n\nWe will frequently use this linear algebra notation, which is equivalent to writing \\[\ny_i | X_i \\sim \\mathcal{N} \\left(\\sum_{j=1}^J X_{ij} \\beta_j, \\sigma^2 \\right).\n\\]"
  },
  {
    "objectID": "slides/2023-10-06-loss-functions.html#bayesian-inference",
    "href": "slides/2023-10-06-loss-functions.html#bayesian-inference",
    "title": "Intro to Machine Learning",
    "section": "Bayesian inference",
    "text": "Bayesian inference\nWe’ve seen this before:\n\n@model function lin_reg(X, y)\n    N, P = size(X)\n    β ~ MvNormal(zeros(P), 10 * ones(P)) # prior: βₚ ~ N(0, 10)\n    σ ~ InverseGamma(1, 5)\n    return y ~ MvNormal(X * β, σ)\nend\n\nlin_reg (generic function with 2 methods)\n\n\n\n\nCode\nchn_lin_reg = let\n    model = lin_reg(X, y)\n    sampler = externalsampler(DynamicHMC.NUTS())\n    n_per_chain = 5000\n    nchains = 4\n    sample(model, sampler, MCMCThreads(), n_per_chain, nchains; drop_warmup=true)\nend\n\n\n\nChains MCMC chain (5000×4×4 Array{Float64, 3}):\nIterations        = 1:1:5000\nNumber of chains  = 4\nSamples per chain = 5000\nWall duration     = 7.19 seconds\nCompute duration  = 5.81 seconds\nparameters        = β[1], β[2], σ\ninternals         = lp\nSummary Statistics\n  parameters      mean       std      mcse     ess_bulk     ess_tail      rhat ⋯\n      Symbol   Float64   Float64   Float64      Float64      Float64   Float64 ⋯\n        β[1]    0.0015    0.2740    0.0033    6826.1387    8822.3325    1.0005 ⋯\n        β[2]    1.7479    0.2267    0.0027    6885.6942    8616.7500    1.0005 ⋯\n           σ    1.3674    0.0994    0.0010   10957.0709   11254.4262    1.0005 ⋯\n                                                                1 column omitted\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n        β[1]   -0.5366   -0.1865    0.0046    0.1854    0.5389\n        β[2]    1.3019    1.5954    1.7459    1.9002    2.1950\n           σ    1.1908    1.2977    1.3601    1.4306    1.5790"
  },
  {
    "objectID": "slides/2023-10-06-loss-functions.html#point-estimate",
    "href": "slides/2023-10-06-loss-functions.html#point-estimate",
    "title": "Intro to Machine Learning",
    "section": "Point estimate",
    "text": "Point estimate\nWe can calcualte a point estimate instead of sampling from the posterior. This might make sense if:\n\n\nJust need a plausible value of the parameters\nDon’t need to carefully quantify parametric uncertainty\n(Caveat: for linear regression, analytic approximations of posterior are available)"
  },
  {
    "objectID": "slides/2023-10-06-loss-functions.html#computation",
    "href": "slides/2023-10-06-loss-functions.html#computation",
    "title": "Intro to Machine Learning",
    "section": "Computation",
    "text": "Computation\nWe consider two point estimates:\n\nMaximum likelihood estimate (MLE): \\(\\arg \\max_\\theta p(y | \\theta)\\)\nMaximum a posteriori estimate (MAP): \\(\\arg \\max_\\theta p(\\theta | y)\\)\n\n\n\n\nCode\nreg_model = lin_reg(X, y)\n1θ̂_map = optimize(reg_model, MAP()).values.array\nθ̂_mle = optimize(reg_model, MLE()).values.array\n\nf̂_mle(x) = θ̂_mle[1] + θ̂_mle[2] * x\nf̂_map(x) = θ̂_map[1] + θ̂_map[2] * x\n\np2 = plot(p_base)\nx_plot = range(minimum(x), maximum(x); length=250)\nplot!(p2, x_plot, f̂_mle.(x_plot); label=\"MLE\", linewidth=3)\nplot!(p2, x_plot, f̂_map.(x_plot); label=\"MAP\", alpha=0.5, linewidth=2)\n\n\n\n1\n\nTuring provides special methods for optimize so we can plug in our model (with data) and MAP() or MLE() and it will figure out the loss function and upper/lower bounds for us."
  },
  {
    "objectID": "slides/2023-10-06-loss-functions.html#critiquing-the-fit",
    "href": "slides/2023-10-06-loss-functions.html#critiquing-the-fit",
    "title": "Intro to Machine Learning",
    "section": "Critiquing the fit",
    "text": "Critiquing the fit\nOne way we can tell that our fit is terrible is by plotting the residuals. We have assumed that the residuals are IID. However, we can see that the residuals are correlated with our predictor!\n\nϵ_regression = y .- f̂_mle.(x)\np3 = scatter(x, ϵ_regression; label=\"Residuals\", xlabel=L\"$x$\", ylabel=L\"$\\epsilon$\")"
  },
  {
    "objectID": "slides/2023-10-06-loss-functions.html#expanding-regression",
    "href": "slides/2023-10-06-loss-functions.html#expanding-regression",
    "title": "Intro to Machine Learning",
    "section": "Expanding regression",
    "text": "Expanding regression\nWithout changing our data generating process, we can expand our regression model to include more features. For example: \\[\n\\mu_i = \\beta_0 + \\sum_{p=1}^P \\beta_p x_{p}^ p\n\\] but we still have \\[\ny_i | X_i \\sim \\mathcal{N}(\\mu_i, \\sigma^2)\n\\]\n\n\n\nCode\nfunction log_lik_poly(β, σ, x, y)\n    μ = [sum([βi * xi^(i - 1) for (i, βi) in enumerate(β)]) for xi in x]\n    return sum(logpdf.(Normal.(μ, σ), y))\nend\norder = 7\nloss_poly(θ) = -log_lik_poly(θ[1:(order+1)], θ[order+2], x, y)\n\nlower = -Inf * ones(order + 2)\nlower[end] = 0\nupper = Inf * ones(order + 2)\nguess = 0.5 * ones(order + 2)\nres = optimize(loss_poly, lower, upper, guess)\nθ̂_poly = res.minimizer\nf̂_poly(x) = θ̂_poly[1] + sum(θ̂_poly[1+k] * x^k for k in 1:order)\n\np4 = plot(p_base)\nplot!(p4, x_plot, f.(x_plot); label=\"True Function\")\nplot!(p4, x_plot, f̂_poly.(x_plot); label=\"Polynomial Fit K=$order\")\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat are some possible problems with this approach?"
  },
  {
    "objectID": "slides/2023-10-06-loss-functions.html#overview",
    "href": "slides/2023-10-06-loss-functions.html#overview",
    "title": "Intro to Machine Learning",
    "section": "Overview",
    "text": "Overview\nWe are interested generally in estimating some function \\(f\\) that maps inputs \\(X\\) to outputs \\(y\\). \\[\ny = f(X) + \\epsilon\n\\]"
  },
  {
    "objectID": "slides/2023-10-06-loss-functions.html#nonparametric-methods",
    "href": "slides/2023-10-06-loss-functions.html#nonparametric-methods",
    "title": "Intro to Machine Learning",
    "section": "Nonparametric methods",
    "text": "Nonparametric methods\n\\(K\\) nearest neighbors (KNN): find the \\(K\\) training examples that are closest to a given input and returns the average output.\n\n\n\n\n\n\nNote\n\n\nNonparametric does not mean that there are no parameters. For example, \\(K\\) is a parameter!\n\n\n\n\n\n\nCode\n# data structure to make nearest neighbor search fast -- don't worry about this\nkdtree = KDTree(transpose(x))\n\nfunction knn_predict(xi, k)\n    # find the k nearest neighbors\n    idxs, _ = knn(kdtree, [xi], k, true)\n    # return the average of the y values\n    return mean(y[idxs])\nend\n\nŷ_3 = knn_predict.(x_plot, 3)\nŷ_10 = knn_predict.(x_plot, 10)\n\np5 = plot(p_base)\nplot!(p5, x_plot, ŷ_3; label=\"K=3\")\nplot!(p5, x_plot, ŷ_10; label=\"K=10\")\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat are some problems with this \\(K\\)NN model?"
  },
  {
    "objectID": "slides/2023-10-06-loss-functions.html#parametric-methods",
    "href": "slides/2023-10-06-loss-functions.html#parametric-methods",
    "title": "Intro to Machine Learning",
    "section": "Parametric methods",
    "text": "Parametric methods\nParametric methods model the function \\(f\\) using some parameters \\(\\theta\\). Thus, finding \\(\\hat{f}\\) is equivalent to choosing appropriate \\(\\theta\\).\nThe linear regression example we’ve been working with is a parametric function approximation."
  },
  {
    "objectID": "slides/2023-10-06-loss-functions.html#loss-functions-1",
    "href": "slides/2023-10-06-loss-functions.html#loss-functions-1",
    "title": "Intro to Machine Learning",
    "section": "Loss functions",
    "text": "Loss functions\nWe need to define what we mean by a “best” approximation.\n\n\nWhat? Measures difference between predicted and actual values.\nWhy? Guide optimization towards best model parameters.\nTypes: Vary by algorithm and task (e.g., regression vs. classification).\nImpact: Choice can significantly affect model performance.\nKey: Align loss function with task objectives for best results."
  },
  {
    "objectID": "slides/2023-10-06-loss-functions.html#common-loss-functions",
    "href": "slides/2023-10-06-loss-functions.html#common-loss-functions",
    "title": "Intro to Machine Learning",
    "section": "Common Loss Functions",
    "text": "Common Loss Functions\n\n\nMSE (Mean Squared Error): \\(L(y, \\hat{y}) = (y - \\hat{y})^2\\). Emphasizes larger errors but is sensitive to outliers.\nMAE (Mean Absolute Error): \\(L(y, \\hat{y}) = |y - \\hat{y}|\\). Less sensitive to outliers and non-differentiable at zero.\nHuber Loss: \\[L_\\delta(y, \\hat{y}) = \\begin{cases}\n\\frac{1}{2}(y - \\hat{y})^2 & \\text{for } |y - \\hat{y}| \\leq \\delta \\\\\n\\delta \\( |y - \\hat{y}| - \\frac{1}{2}\\delta^2 \\) & \\text{otherwise}\n\\end{cases}\\] Combines MSE and MAE. Requires threshold parameter \\(\\delta\\).\nQuantile Loss: \\(L_\\tau(y, \\hat{y}) = \\tau(y - \\hat{y})\\) if \\((y - \\hat{y}) &gt; 0\\) else \\((\\tau - 1)(y - \\hat{y})\\). Tailored to specific quantiles (\\(\\tau\\)). Useful for asymmetric errors."
  },
  {
    "objectID": "slides/2023-10-06-loss-functions.html#visualization",
    "href": "slides/2023-10-06-loss-functions.html#visualization",
    "title": "Intro to Machine Learning",
    "section": "Visualization",
    "text": "Visualization\n\n\nCode\n# Sample residuals\nresiduals = -2:0.01:2\n\n# Loss functions\nmse_loss = residuals .^ 2\nmae_loss = abs.(residuals)\nhuber_loss = [abs(r) &lt;= 1 ? 0.5 * r^2 : abs(r) - 0.5 for r in residuals]\nquantile_tau = 0.5  # example quantile value\nquantile_loss = [r &gt; 0 ? quantile_tau * r : (1 - quantile_tau) * r for r in residuals]\n\n# Plot\nplot(residuals, mse_loss; label=\"MSE\", lw=2)\nplot!(residuals, mae_loss; label=\"MAE\", lw=2)\nplot!(residuals, huber_loss; label=\"Huber\", lw=2)\nplot!(residuals, quantile_loss; label=\"Quantile (τ=0.5)\", lw=2)\nxlabel!(L\"Residual ($y - \\hat{y}$)\")\nylabel!(\"Loss\")\ntitle!(\"Loss Functions Visualization\")"
  },
  {
    "objectID": "slides/2023-10-06-loss-functions.html#motivation-1",
    "href": "slides/2023-10-06-loss-functions.html#motivation-1",
    "title": "Intro to Machine Learning",
    "section": "Motivation",
    "text": "Motivation\n\n\nModel Performance: Why does our model make errors? Can we reduce them?\nOverfitting vs Underfitting: How do we balance fitting our data well and ensuring our model generalizes to new data?\nModel Complexity: As we add more features or increase model complexity, how does it impact our model’s errors?\nOptimal Model: How do we find the sweet spot where our model has the best predictive performance?\nInterpretability: Understanding bias and variance can help in making informed decisions about model selection and complexity.\n\n\n\nKey Insight: Every model error can be decomposed into bias, variance, and irreducible error. Balancing bias and variance is crucial for creating models that perform well on both training and unseen data."
  },
  {
    "objectID": "slides/2023-10-06-loss-functions.html#bias-and-variance",
    "href": "slides/2023-10-06-loss-functions.html#bias-and-variance",
    "title": "Intro to Machine Learning",
    "section": "Bias and variance",
    "text": "Bias and variance\n\n\n\n\n\n\n\nHigh bias, low variance\n\n\n\n\n\n\n\nHigh bias, high variance\n\n\n\n\n\n\n\n\n\nLow bias, low variance\n\n\n\n\n\n\n\nLow bias, high variance\n\n\n\n\nFigure 1: Bias-variance illustration (Wikipedia)"
  },
  {
    "objectID": "slides/2023-10-06-loss-functions.html#mean-squared-error-mse",
    "href": "slides/2023-10-06-loss-functions.html#mean-squared-error-mse",
    "title": "Intro to Machine Learning",
    "section": "Mean Squared Error (MSE)",
    "text": "Mean Squared Error (MSE)\nThe expected prediction error for any machine learning algorithm can be broken down as:\n\\[\n\\text{MSE} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}\n\\]\n\n\nBias: How much on average are the predicted values different from the actual values? Represents errors due to overly simplistic assumptions in the learning algorithm. \\[\n\\text{Bias}(\\hat{f}(x)) = E[\\hat{f}(x) - f(x)]\n\\]\nVariance: How much does the prediction for a given point vary between different realizations of the model? Represents errors due to the model’s sensitivity to small fluctuations in the training set. \\[\n\\text{Variance}(\\hat{f}(x)) = E[\\hat{f}(x)^2] - E[\\hat{f}(x)]^2\n\\]\nIrreducible Error: Noise inherent in any real-world data that we cannot remove, no matter how good our model is."
  },
  {
    "objectID": "slides/2023-10-06-loss-functions.html#bayesian-methods",
    "href": "slides/2023-10-06-loss-functions.html#bayesian-methods",
    "title": "Intro to Machine Learning",
    "section": "Bayesian methods",
    "text": "Bayesian methods\nWe can think about Bayesian methods through the lens of the bias-variance trade-off\n\n\nPriors add bias\nPriors often reduce variance"
  },
  {
    "objectID": "slides/2023-10-06-loss-functions.html#ridge-regression",
    "href": "slides/2023-10-06-loss-functions.html#ridge-regression",
    "title": "Intro to Machine Learning",
    "section": "Ridge Regression",
    "text": "Ridge Regression\nRidge Regression, modifies linear regression to include a regularization term. The regularization term discourages overly complex models which can overfit the training data.\n\\[\nL(\\beta) = \\| Y - X^T \\beta \\|_2^2 + \\lambda \\| \\beta \\|_2^2\n\\] where \\(\\lambda\\) is the regularization parameter and where \\[\n\\| x \\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}\n\\] is the L2 norm."
  },
  {
    "objectID": "slides/2023-10-06-loss-functions.html#lasso-regression",
    "href": "slides/2023-10-06-loss-functions.html#lasso-regression",
    "title": "Intro to Machine Learning",
    "section": "Lasso Regression",
    "text": "Lasso Regression\nLasso Regression (Least Absolute Shrinkage and Selection Operator) includes an L1 penalty.\n\n\nStronger penalty near zero will set some coefficients to almost exactly zero\nSmaller penalty far from zero\n\n\n\n\\[\nL(\\beta) = \\| Y - X^T \\beta \\|_2^2 + \\lambda \\| \\beta \\|_1\n\\]"
  },
  {
    "objectID": "slides/2023-10-06-loss-functions.html#key-ideas",
    "href": "slides/2023-10-06-loss-functions.html#key-ideas",
    "title": "Intro to Machine Learning",
    "section": "Key ideas",
    "text": "Key ideas\n\n\nPoint estimates rather than quantifying posterior\n\nFocus on more complex functions\nAppropriate when “a good model” is more important than “the full distribution of the paramameters”\n\nMeasure quality of predictions using loss functions, which we can optimize\nBias-Variance Trade-Off\nRegularization"
  },
  {
    "objectID": "slides/2023-10-06-loss-functions.html#suggested-reading",
    "href": "slides/2023-10-06-loss-functions.html#suggested-reading",
    "title": "Intro to Machine Learning",
    "section": "Suggested reading",
    "text": "Suggested reading\nChapter 2 of Friedman et al. (2001)\n\n\n\n\n\n\n\n\n\nFriedman, J., Hastie, T., & Tibshirani, R. (2001). The Elements of Statistical Learning (Vol. 1). Springer series in statistics Springer, Berlin."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CEVE 543: Data Science for Climate Risk Assessment",
    "section": "",
    "text": "This is the course website for the Fall 2023 edition of CEVE 543, Data Science for Climate Risk Assessment, taught at Rice University by James Doss-Gollin."
  },
  {
    "objectID": "index.html#course-information",
    "href": "index.html#course-information",
    "title": "CEVE 543: Data Science for Climate Risk Assessment",
    "section": "Course Information",
    "text": "Course Information\n\nDetails on the class and course policies are provided in the syllabus.\nTopics, slides, assignments, and other materials can be found in the schedule."
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "CEVE 543: Data Science for Climate Risk Assessment",
    "section": "Instructor",
    "text": "Instructor\nDr. James Doss-Gollin is an assistant professor of Civil and Environmental Engineering at Rice University. His research integrates Earth science, data science, and decision science to address challenges in climate risk management, water resources, and energy system resilience. He also teaches CEVE 421/521 (Climate Risk Management)."
  },
  {
    "objectID": "index.html#software-tools",
    "href": "index.html#software-tools",
    "title": "CEVE 543: Data Science for Climate Risk Assessment",
    "section": "Software Tools",
    "text": "Software Tools\n\nThis course will use the Julia programming language. Julia is a modern, free, open source language designed for scientific computing.\nNo prior knowledge of Julia (or other programming languages) is required. We will cover all required material in labs.\nAssignments will be distributed using GitHub Classroom."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "CEVE 543: Data Science for Climate Risk Assessment",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe layout for this site was inspired by and draws from Vivek Srikrishnan’s Environmental Systems Analysis course at Cornell, STA 210 at Duke University, and Andrew Heiss’s course materials at Georgia State."
  },
  {
    "objectID": "resources/julia-plots.html",
    "href": "resources/julia-plots.html",
    "title": "Tutorial: Making Plots with Julia",
    "section": "",
    "text": "This tutorial will give some examples of plotting and plotting features in Julia, as well as providing references to some relevant resources. The main plotting library is Plots.jl, but there are some others that provide useful features."
  },
  {
    "objectID": "resources/julia-plots.html#overview",
    "href": "resources/julia-plots.html#overview",
    "title": "Tutorial: Making Plots with Julia",
    "section": "",
    "text": "This tutorial will give some examples of plotting and plotting features in Julia, as well as providing references to some relevant resources. The main plotting library is Plots.jl, but there are some others that provide useful features."
  },
  {
    "objectID": "resources/julia-plots.html#some-resources",
    "href": "resources/julia-plots.html#some-resources",
    "title": "Tutorial: Making Plots with Julia",
    "section": "Some Resources",
    "text": "Some Resources\n\nPlots.jl useful tips\nPlots.jl examples\nPlot attributes\nAxis attributes\nColor names## Demos\n\n\nusing Plots\nusing Random\nRandom.seed!(1);\n\n\nLine Plots\nTo generate a basic line plot, use plot.\ny = rand(5) plot(y; label=“original data”, legend=:topright) ```There’s a lot of customization here that can occur, a lot of which is discussed in the docs or can be found with some Googling.\n\n\nAdding Plot Elements\nNow we can add some other lines and point markers.\n\ny2 = rand(5)\ny3 = rand(5)\nplot!(y2; label=\"new data\")\nscatter!(y3; label=\"even more data\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemember that an exclamation mark (!) at the end of a function name means that function modifies an object in-place, so plot! and scatter! modify the current plotting object, they don’t create a new plot.\n\n\nRemoving Plot Elements\nSometimes we want to remove legends, axes, grid lines, and ticks.\n\nplot!(; legend=false, axis=false, grid=false, ticks=false)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\nAspect Ratio\nIf we want to have a square aspect ratio, use ratio = 1.\nv = rand(5) plot(v; ratio=1, legend=false) scatter!(v) ```### Heatmaps\nA heatmap is effectively a plotted matrix with colors chosen according to the values. Use clim to specify a fixed range for the color limits.\nA = rand(10, 10) heatmap(A; clim=(0, 1), ratio=1, legend=false, axis=false, ticks=false)\n\nM = [0 1 0; 0 0 0; 1 0 0]\nwhiteblack = [RGBA(1, 1, 1, 0), RGB(0, 0, 0)]\nheatmap(;\n    c=whiteblack,\n    M,\n    aspect_ratio=1,\n    ticks=0.5:3.5,\n    lims=(0.5, 3.5),\n    gridalpha=1,\n    legend=false,\n    axis=false,\n    ylabel=\"i\",\n    xlabel=\"j\",\n)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nCustom Colors\n\nusing Colors\n\nmycolors = [colorant\"lightslateblue\", colorant\"limegreen\", colorant\"red\"]\nA = [i for i in 50:300, j in 1:100]\nheatmap(A; c=mycolors, clim=(1, 300))\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\nPlotting Areas Under Curves\n\ny = rand(10)\nplot(y; fillrange=y .* 0 .+ 0.5, label=\"above/below 1/2\", legend=:top)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx = LinRange(0, 2, 100)\ny1 = exp.(x)\ny2 = exp.(1.3 .* x)\nplot(x, y1; fillrange=y2, fillalpha=0.35, c=1, label=\"Confidence band\", legend=:topleft)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx = -3:0.01:3\nareaplot(x, exp.(-x .^ 2 / 2) / √(2π); alpha=0.25, legend=false)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nM = [1 2 3; 7 8 9; 4 5 6; 0 0.5 1.5]\nareaplot(1:3, M; seriescolor=[:red :green :blue], fillalpha=[0.2 0.3 0.4])\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nusing SpecialFunctions f = x -&gt; exp(-x^2 / 2) / √(2π) δ = 0.01 plot() x = √2 .* erfinv.(2 .* ((δ / 2):δ:1) .- 1) areaplot(x, f.(x); seriescolor=[:red, :blue], legend=false) plot!(x, f.(x); c=:black) ```### Plotting Shapes\n\nrectangle(w, h, x, y) = Shape(x .+ [0, w, w, 0], y .+ [0, 0, h, h])\ncircle(r, x, y) = (θ = LinRange(0, 2π, 500);\n(x .+ r .* cos.(θ), y .+ r .* sin.(θ)))\nplot(circle(5, 0, 0); ratio=1, c=:red, fill=true)\nplot!(rectangle(5 * √2, 5 * √2, -2.5 * √2, -2.5 * √2); c=:white, fill=true, legend=false)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting Distributions\nThe StatsPlots.jl package is very useful for making various plots of probability distributions.\n\nusing Distributions, StatsPlots\nplot(Normal(2, 5))\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nscatter(LogNormal(0.8, 1.5))\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also use this functionality to plot distributions of data in tabular data structures like DataFrames.\n\nusing DataFrames\ndat = DataFrame(; a=1:10, b=10 .+ rand(10), c=10 .* rand(10))\n@df dat density([:b :c], color=[:black :red])\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEditing Plots Manually\n\npl = plot(1:4, [1, 4, 9, 16])\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npl.attr\n\nRecipesPipeline.DefaultsDict with 30 entries:\n  :dpi                      =&gt; 96\n  :background_color_outside =&gt; :match\n  :plot_titlefontvalign     =&gt; :vcenter\n  :warn_on_unsupported      =&gt; true\n  :background_color         =&gt; RGBA{Float64}(1.0,1.0,1.0,1.0)\n  :inset_subplots           =&gt; nothing\n  :size                     =&gt; (672, 480)\n  :display_type             =&gt; :auto\n  :overwrite_figure         =&gt; true\n  :html_output_format       =&gt; :svg\n  :plot_titlefontfamily     =&gt; :match\n  :plot_titleindex          =&gt; 0\n  :foreground_color         =&gt; RGB{N0f8}(0.0,0.0,0.0)\n  :window_title             =&gt; \"Plots.jl\"\n  :plot_titlefontrotation   =&gt; 0.0\n  :extra_plot_kwargs        =&gt; Dict{Any, Any}()\n  :pos                      =&gt; (0, 0)\n  :plot_titlefonthalign     =&gt; :hcenter\n  :tex_output_standalone    =&gt; false\n  :extra_kwargs             =&gt; :series\n  :thickness_scaling        =&gt; 1\n  :layout                   =&gt; 1\n  :plot_titlelocation       =&gt; :center\n  :plot_titlefontsize       =&gt; 16\n  :plot_title               =&gt; \"\"\n  ⋮                         =&gt; ⋮\n\n\n\npl.series_list[1]\n\nPlots.Series(RecipesPipeline.DefaultsDict(:plot_object =&gt; Plot{Plots.GRBackend() n=1}, :subplot =&gt; Subplot{1}, :label =&gt; \"y1\", :fillalpha =&gt; nothing, :linealpha =&gt; nothing, :linecolor =&gt; RGBA{Float64}(0.0,0.6056031611752245,0.9786801175696073,1.0), :x_extrema =&gt; (NaN, NaN), :series_index =&gt; 1, :markerstrokealpha =&gt; nothing, :markeralpha =&gt; nothing…))\n\n\n\npl[:size] = (300, 200)\n\n(300, 200)\n\n\n\npl\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog-Scaled Axes\n\nxx = 0.1:0.1:10\nplot(xx .^ 2; xaxis=:log, yaxis=:log)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot(exp.(x); yaxis=:log)"
  },
  {
    "objectID": "resources/julia-basics.html",
    "href": "resources/julia-basics.html",
    "title": "Tutorial: Julia Basics",
    "section": "",
    "text": "This tutorial will give some examples of basic Julia commands and syntax."
  },
  {
    "objectID": "resources/julia-basics.html#overview",
    "href": "resources/julia-basics.html#overview",
    "title": "Tutorial: Julia Basics",
    "section": "",
    "text": "This tutorial will give some examples of basic Julia commands and syntax."
  },
  {
    "objectID": "resources/julia-basics.html#getting-help",
    "href": "resources/julia-basics.html#getting-help",
    "title": "Tutorial: Julia Basics",
    "section": "Getting Help",
    "text": "Getting Help\n\nCheck out the official documentation for Julia: https://docs.julialang.org/en/v1/.\nStack Overflow is a commonly-used resource for programming assistance.\nAt a code prompt or in the REPL, you can always type ?functionname to get help."
  },
  {
    "objectID": "resources/julia-basics.html#further-resources",
    "href": "resources/julia-basics.html#further-resources",
    "title": "Tutorial: Julia Basics",
    "section": "Further Resources",
    "text": "Further Resources\nThere are lots of great resources on programming and Julia. Here is a curated list of some particularly helpful tools.\n\n\n\n\n\n\nNote\n\n\n\nSome of these tutorials provide their own instructions on how to install Julia. Please follow the instructions provided in this course\n\n\n\nJulia for Nervous Begineers: A free course on JuliaAcademy for people who are hesitant but curious about learning to write code in Julia.\nFastTrack to Julia cheatsheet\nPlotting cheatsheet\nIntroduction to Computational Thinking: a great Julia-based course at MIT covering applied mathematics and… computational thinking\nComprehensive Julia Tutorials: YouTube playlist covering a variety of Julia topics, starting with an introduciton to the language.## Comments Comments hide statements from the interpreter or compiler. It’s a good idea to liberally comment your code so readers (including yourself!) know why your code is structured and written the way it is. Single-line comments in Julia are preceded with a #. Multi-line comments are preceded with #= and ended with =#"
  },
  {
    "objectID": "resources/julia-basics.html#suppressing-output",
    "href": "resources/julia-basics.html#suppressing-output",
    "title": "Tutorial: Julia Basics",
    "section": "Suppressing Output",
    "text": "Suppressing Output\nYou can suppress output using a semi-colon (;).\n\n4 + 8;\n\nThat didn’t show anything, as opposed to:\n\n4 + 8\n\n12"
  },
  {
    "objectID": "resources/julia-basics.html#variables",
    "href": "resources/julia-basics.html#variables",
    "title": "Tutorial: Julia Basics",
    "section": "Variables",
    "text": "Variables\nVariables are names which correspond to some type of object. These names are bound to objects (and hence their values) using the = operator.\n\nx = 5\n\n5\n\n\nVariables can be manipulated with standard arithmetic operators.\n\n4 + x\n\n9\n\n\nAnother advantage of Julia is the ability to use Greek letters (or other Unicode characters) as variable names. For example, type a backslash followed by the name of the Greek letter (i.e. \\alpha) followed by TAB.\n\nα = 3\n\n3\n\n\nYou can also include subscripts or superscripts in variable names using \\_ and \\^, respectively, followed by TAB. If using a Greek letter followed by a sub- or super-script, make sure you TAB following the name of the letter before the sub- or super-script. Effectively, TAB after you finish typing the name of each \\character.\n\nβ₁ = 10 # The name of this variable was entered with \\beta + TAB + \\_1 + TAB\n\n10\n\n\nHowever, try not to overwrite predefined names! For example, you might not want to use π as a variable name…\n\nπ\n\nπ = 3.1415926535897...\n\n\nIn the grand scheme of things, overwriting π is not a huge deal unless you want to do some trigonometry. However, there are more important predefined functions and variables that you may want to be aware of. Always check that a variable or function name is not predefined!"
  },
  {
    "objectID": "resources/julia-basics.html#data-types",
    "href": "resources/julia-basics.html#data-types",
    "title": "Tutorial: Julia Basics",
    "section": "Data Types",
    "text": "Data Types\nEach datum (importantly, not the variable which is bound to it) has a data type. Julia types are similar to C types, in that they require not only the type of data (Int, Float, String, etc), but also the precision (which is related to the amount of memory allocated to the variable). Issues with precision won’t be a big deal in this class, though they matter when you’re concerned about performance vs. decimal accuracy of code.\nYou can identify the type of a variable or expression with the typeof() function.\n\ntypeof(\"This is a string.\")\n\nString\n\n\n\ntypeof(x)\n\nInt64\n\n\n\nNumeric types\nA key distinction is between an integer type (or Int) and a floating-point number type (or float). Integers only hold whole numbers, while floating-point numbers correspond to numbers with fractional (or decimal) parts. For example, 9 is an integer, while 9.25 is a floating point number. The difference between the two has to do with the way the number is stored in memory. 9, an integer, is handled differently in memory than 9.0, which is a floating-point number, even though they’re mathematically the same value.\n\ntypeof(9)\n\nInt64\n\n\n\ntypeof(9.25)\n\nFloat64\n\n\nSometimes certain function specifications will require you to use a Float variable instead of an Int. One way to force an Int variable to be a Float is to add a decimal point at the end of the integer.\n\ntypeof(9.)\n\nFloat64\n\n\n\n\nStrings\nStrings hold characters, rather than numeric values. Even if a string contains what seems like a number, it is actually stored as the character representation of the digits. As a result, you cannot use arithmetic operators (for example) on this datum.\n\n\"5\" + 5\n\nLoadError: MethodError: no method matching +(::String, ::Int64)\n\n\u001b[0mClosest candidates are:\n\u001b[0m  +(::Any, ::Any, \u001b[91m::Any\u001b[39m, \u001b[91m::Any...\u001b[39m)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m\u001b[4moperators.jl:578\u001b[24m\u001b[39m\n\u001b[0m  +(\u001b[91m::T\u001b[39m, ::T) where T&lt;:Union{Int128, Int16, Int32, Int64, Int8, UInt128, UInt16, UInt32, UInt64, UInt8}\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m\u001b[4mint.jl:87\u001b[24m\u001b[39m\n\u001b[0m  +(\u001b[91m::Rational\u001b[39m, ::Integer)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m\u001b[4mrational.jl:327\u001b[24m\u001b[39m\n\u001b[0m  ...\n\n\nHowever, you can try to tell Julia to interpret a string encoding a numeric character as a numeric value using the parse() function. This can also be used to encode a numeric data as a string.\n\nparse(Int64, \"5\") + 5\n\n10\n\n\nTwo strings can be concatenated using *:\n\n\"Hello\" * \" \" * \"there\"\n\n\"Hello there\"\n\n\n\n\nBooleans\nBoolean variables (or Bools) are logical variables, that can have true or false as values.\n\nb = true\n\ntrue\n\n\nNumerical comparisons, such as ==, !=, or &lt;, return a Bool.\n\nc = 9 &gt; 11\n\nfalse\n\n\nBools are important for logical flows, such as if-then-else blocks or certain types of loops."
  },
  {
    "objectID": "resources/julia-basics.html#mathematical-operations",
    "href": "resources/julia-basics.html#mathematical-operations",
    "title": "Tutorial: Julia Basics",
    "section": "Mathematical operations",
    "text": "Mathematical operations\nAddition, subtraction, multiplication, and division work as you would expect. Just pay attention to types! The type of the output is influenced by the type of the inputs: adding or multiplying an Int by a Float will always result in a Float, even if the Float is mathematically an integer. Division is a little special: dividing an Int by another Int will still return a float, because Julia doesn’t know ahead of time if the denominator is a factor of the numerator.\n\n3 + 5\n\n8\n\n\n\n3 * 2\n\n6\n\n\n\n3 * 2.\n\n6.0\n\n\n\n6 - 2\n\n4\n\n\n\n9 / 3\n\n3.0\n\n\nRaising a base to an exponent uses ^, not **.\n\n3^2\n\n9\n\n\nJulia allows the use of updating operators to simplify updating a variable in place (in other words, using x += 5 instead of x = x + 5.\n\nBoolean algebra\nLogical operations can be used on variables of type Bool. Typical operators are && (and), || (or), and ! (not).\n\ntrue && true\n\ntrue\n\n\n\ntrue && false\n\nfalse\n\n\n\ntrue || false\n\ntrue\n\n\n\n!true\n\nfalse\n\n\nComparisons can be chained together.\n\n3 &lt; 4 || 8 == 12\n\ntrue\n\n\nWe didn’t do this above, since Julia doesn’t require it, but it’s easier to understand these types of compound expressions if you use parentheses to signal the order of operations. This helps with debugging!\n\n(3 &lt; 4) || (8 == 12)\n\ntrue"
  },
  {
    "objectID": "resources/julia-basics.html#data-structures",
    "href": "resources/julia-basics.html#data-structures",
    "title": "Tutorial: Julia Basics",
    "section": "Data Structures",
    "text": "Data Structures\nData structures are containers which hold multiple values in a convenient fashion. Julia has several built-in data structures, and there are many extensions provided in additional packages.\n\nTuples\nTuples are collections of values. Julia will pay attention to the types of these values, but they can be mixed. Tuples are also immutable: their values cannot be changed once they are defined.\nTuples can be defined by just separating values with commas.\n\ntest_tuple = 4, 5, 6\n\n(4, 5, 6)\n\n\nTo access a value, use square brackets and the desired index. Note: Julia indexing starts at 1, not 0!\n\ntest_tuple[1]\n\n4\n\n\nAs mentioned above, tuples are immutable. What happens if we try to change the value of the first element of test_tuple?\n\ntest_tuple[1] = 5\n\nLoadError: MethodError: no method matching setindex!(::Tuple{Int64, Int64, Int64}, ::Int64, ::Int64)\n\n\nTuples also do not have to hold the same types of values.\n\ntest_tuple_2 = 4, 5., 'h'\ntypeof(test_tuple_2)\n\nTuple{Int64, Float64, Char}\n\n\nTuples can also be defined by enclosing the values in parentheses.\ntest_tuple_3 = (4, 5., 'h')\ntypeof(test_tuple_3)\n\n\nArrays\nArrays also hold multiple values, which can be accessed based on their index position. Arrays are commonly defined using square brackets.\n\ntest_array = [1, 4, 7, 8]\ntest_array[2]\n\n4\n\n\nUnlike tuples, arrays are mutable, and their contained values can be changed later.\n\ntest_array[1] = 6\ntest_array\n\n4-element Vector{Int64}:\n 6\n 4\n 7\n 8\n\n\nArrays also can hold multiple types. Unlike tuples, this causes the array to no longer care about types at all.\n\ntest_array_2 = [6, 5., 'h']\ntypeof(test_array_2)\n\n\nVector{Any} (alias for Array{Any, 1})\n\n\n\nCompare this with test_array:\n\ntypeof(test_array)\n\n\nVector{Int64} (alias for Array{Int64, 1})\n\n\n\n\n\nDictionaries\nInstead of using integer indices based on position, dictionaries are indexed by keys. They are specified by passing key-value pairs to the Dict() method.\n\ntest_dict = Dict(\"A\"=&gt;1, \"B\"=&gt;2)\ntest_dict[\"B\"]\n\n2\n\n\n\n\nComprehensions\nCreating a data structure with more than a handful of elements can be tedious to do by hand. If your desired array follows a certain pattern, you can create structures using a comprehension. Comprehensions iterate over some other data structure (such as an array) implicitly and populate the new data structure based on the specified instructions.\n\n[i^2 for i in 0:1:5]\n\n6-element Vector{Int64}:\n  0\n  1\n  4\n  9\n 16\n 25\n\n\nFor dictionaries, make sure that you also specify the keys.\n\nDict(string(i) =&gt; i^2 for i in 0:1:5)\n\nDict{String, Int64} with 6 entries:\n  \"4\" =&gt; 16\n  \"1\" =&gt; 1\n  \"5\" =&gt; 25\n  \"0\" =&gt; 0\n  \"2\" =&gt; 4\n  \"3\" =&gt; 9"
  },
  {
    "objectID": "resources/julia-basics.html#functions",
    "href": "resources/julia-basics.html#functions",
    "title": "Tutorial: Julia Basics",
    "section": "Functions",
    "text": "Functions\nA function is an object which accepts a tuple of arguments and maps them to a return value. In Julia, functions are defined using the following syntax.\n\nfunction my_actual_function(x, y)\n    return x + y\nend\nmy_actual_function(3, 5)\n\n8\n\n\nFunctions in Julia do not require explicit use of a return statement. They will return the last expression evaluated in their definition. However, it’s good style to explicitly return function outputs. This improves readability and debugging, especially when functions can return multiple expressions based on logical control flows (if-then-else blocks).\nFunctions in Julia are objects, and can be treated like other objects. They can be assigned to new variables or passed as arguments to other functions.\n\ng = my_actual_function\ng(3, 5)\n\n8\n\n\n\nfunction function_of_functions(f, x, y)\n    return f(x, y)\nend\nfunction_of_functions(g, 3, 5)\n\n8\n\n\n\nShort and Anonymous Functions\nIn addition to the long form of the function definition shown above, simple functions can be specified in more compact forms when helpful.\nThis is the short form:\n\nh₁(x) = x^2 # make the subscript using \\_1 + &lt;TAB&gt;\nh₁(4)\n\n16\n\n\nThis is the anonymous form:\n\nx-&gt;sin(x)\n(x-&gt;sin(x))(π/4)\n\n0.7071067811865475\n\n\n\n\nMutating Functions\nThe convention in Julia is that functions should not modify (or mutate) their input data. The reason for this is to ensure that the data is preserved. Mutating functions are mainly appropriate for applications where performance needs to be optimized, and making a copy of the input data would be too memory-intensive.\nIf you do write a mutating function in Julia, the convention is to add a ! to its name, like my_mutating_function!(x).\n\n\nOptional arguments\nThere are two extremes with regard to function parameters which do not always need to be changed. The first is to hard-code them into the function body, which has a clear downside: when you do want to change them, the function needs to be edited directly. The other extreme is to treat them as regular arguments, passing them every time the function is called. This has the downside of potentially creating bloated function calls, particularly when there is a standard default value that makes sense for most function evaluations.\nMost modern languages, including Julia, allow an alternate solution, which is to make these arguments optional. This involves setting a default value, which is used unless the argument is explicitly defined in a function call.\n\nfunction setting_optional_arguments(x, y, c=0.5)\n    return c * (x + y)\nend\n\nsetting_optional_arguments (generic function with 2 methods)\n\n\nIf we want to stick with the fixed value \\(c=0.5\\), all we have to do is call setting_optional_arguments with the x and y arguments.\n\nsetting_optional_arguments(3, 5)\n\n4.0\n\n\nOtherwise, we can pass a new value for c.\n\nsetting_optional_arguments(3, 5, 2)\n\n16\n\n\n\n\nPassing data structures as arguments\nInstead of passing variables individually, it may make sense to pass a data structure, such as an array or a tuple, and then unpacking within the function definition. This is straightforward in long form: access the appropriate elements using their index.\nIn short or anonymous form, there is a trick which allows the use of readable variables within the function definition.\n\nh₂((x,y)) = x*y # enclose the input arguments in parentheses to tell Julia to expect and unpack a tuple\n\nh₂ (generic function with 1 method)\n\n\n\nh₂((2, 3)) # this works perfectly, as we passed in a tuple\n\n6\n\n\n\nh₂(2, 3) # this gives an error, as h₂ expects a single tuple, not two different numeric values\n\nLoadError: MethodError: no method matching h₂(::Int64, ::Int64)\n\n\u001b[0mClosest candidates are:\n\u001b[0m  h₂(::Any)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[32mMain\u001b[39m \u001b[90m\u001b[4mIn[50]:1\u001b[24m\u001b[39m\n\n\n\nh₂([3, 10]) # this also works with arrays instead of tuples\n\n30\n\n\n\n\nVectorized operations\nJulia uses dot syntax to vectorize an operation and apply it element-wise across an array.\nFor example, to calculate the square root of 3:\n\nsqrt(3)\n\n1.7320508075688772\n\n\nTo calculate the square roots of every integer between 1 and 5:\n\nsqrt.([1, 2, 3, 4, 5])\n\n5-element Vector{Float64}:\n 1.0\n 1.4142135623730951\n 1.7320508075688772\n 2.0\n 2.23606797749979\n\n\nThe same dot syntax is used for arithmetic operations over arrays, since these operations are really functions.\n\n[1, 2, 3, 4] .* 2\n\n4-element Vector{Int64}:\n 2\n 4\n 6\n 8\n\n\nVectorization can be faster and is more concise to write and read than applying the same function to multiple variables or objects explicitly, so take advantage!\n\n\nReturning multiple values\nYou can return multiple values by separating them with a comma. This implicitly causes the function to return a tuple of values.\n\nfunction return_multiple_values(x, y)\n    return x + y, x * y\nend\nreturn_multiple_values(3, 5)\n\n(8, 15)\n\n\nThese values can be unpacked into multiple variables.\n\nn, ν = return_multiple_values(3, 5)\nn\n\n8\n\n\n\nν\n\n15\n\n\n\n\nReturning nothing\nSometimes you don’t want a function to return any values at all. For example, you might want a function that only prints a string to the console.\n\nfunction print_some_string(x)\n    println(\"x: $x\")\n    return nothing\nend\nprint_some_string(42)\n\nx: 42"
  },
  {
    "objectID": "resources/julia-basics.html#printing-text-output",
    "href": "resources/julia-basics.html#printing-text-output",
    "title": "Tutorial: Julia Basics",
    "section": "Printing Text Output",
    "text": "Printing Text Output\nThe Text() function returns its argument as a plain text string. Notice how this is different from evaluating a string!\n\nText(\"I'm printing a string.\")\n\nI'm printing a string.\n\n\nText() is used in this tutorial as it returns the string passed to it. To print directly to the console, use println().\n\nprintln(\"I'm writing a string to the console.\")\n\nI'm writing a string to the console.\n\n\n\nPrinting Variables In a String\nWhat if we want to include the value of a variable inside of a string? We do this using string interpolation, using $variablename inside of the string.\n\nbar = 42\nText(\"Now I'm printing a variable: $bar\")\n\nNow I'm printing a variable: 42"
  },
  {
    "objectID": "resources/julia-basics.html#control-flows",
    "href": "resources/julia-basics.html#control-flows",
    "title": "Tutorial: Julia Basics",
    "section": "Control Flows",
    "text": "Control Flows\nOne of the tricky things about learning a new programming language can be getting used to the specifics of control flow syntax. These types of flows include conditional if-then-else statements or loops.\n\nConditional Blocks\nConditional blocks allow different pieces of code to be evaluated depending on the value of a boolean expression or variable. For example, if we wanted to compute the absolute value of a number, rather than using abs():\n\nfunction our_abs(x)\n    if x &gt;= 0\n        return x\n    else\n        return -x\n    end\nend\n\nour_abs (generic function with 1 method)\n\n\n\nour_abs(4)\n\n4\n\n\n\nour_abs(-4)\n\n4\n\n\nTo nest conditional statements, use elseif.\n\nfunction test_sign(x)\n    if x &gt; 0\n        return Text(\"x is positive.\")\n    elseif x &lt; 0\n        return Text(\"x is negative.\")\n    else\n        return Text(\"x is zero.\")\n    end\nend\n\ntest_sign (generic function with 1 method)\n\n\n\ntest_sign(-5)\n\nx is negative.\n\n\n\ntest_sign(0)\n\nx is zero.\n\n\n\n\nLoops\nLoops allow expressions to be evaluated repeatedly until they are terminated. The two main types of loops are while loops and for loops.\n\nWhile loops\nwhile loops continue to evaluate an expression so long as a specified boolean condition is true. This is useful when you don’t know how many iterations it will take for the desired goal to be reached.\n\nfunction compute_factorial(x)\n    factorial = 1\n    while (x &gt; 1)\n        factorial *= x\n        x -= 1\n    end\n    return factorial\nend\ncompute_factorial(5)\n\n120\n\n\n\nWhile loops can easily turn into infinite loops if the condition is never meaningfully updated. Be careful, and look there if your programs are getting stuck. Also, If the expression in a while loop is false when the loop is reached, the loop will never be evaluated.\n\n\n\nFor loops\nfor loops run for a finite number of iterations, based on some defined index variable.\n\nfunction add_some_numbers(x)\n    total_sum = 0 # initialize at zero since we're adding\n    for i in 1:x # the counter i is updated every iteration\n        total_sum += i\n    end\n    return total_sum\nend\nadd_some_numbers(4)\n\n10\n\n\nfor loops can also iterate over explicitly passed containers, rather than iterating over an incrementally-updated index sequence. Use the in keyword when defining the loop.\n\nfunction add_passed_numbers(set)\n    total_sum = 0\n    for i in set # this is the syntax we use when we want i to correspond to different container values\n        total_sum += i\n    end\n    return total_sum\nend\nadd_passed_numbers([1, 3, 5])\n\n9"
  },
  {
    "objectID": "resources/julia-basics.html#linear-algebra",
    "href": "resources/julia-basics.html#linear-algebra",
    "title": "Tutorial: Julia Basics",
    "section": "Linear algebra",
    "text": "Linear algebra\nMatrices are defined in Julia as 2d arrays. Unlike basic arrays, matrices need to contain the same data type so Julia knows what operations are allowed. When defining a matrix, use semicolons to separate rows. Row elements should not be separated by commas.\n\ntest_matrix = [1 2 3; 4 5 6]\n\n2×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n\n\nYou can also specify matrices using spaces and newlines.\n\ntest_matrix_2 = [\n    1 2 3\n    4 5 6\n]\n\n2×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n\n\nFinally, matrices can be created using comprehensions by separating the inputs by a comma.\n\n[i*j for i in 1:1:5, j in 1:1:5]\n\n5×5 Matrix{Int64}:\n 1   2   3   4   5\n 2   4   6   8  10\n 3   6   9  12  15\n 4   8  12  16  20\n 5  10  15  20  25\n\n\nVectors are treated as 1d matrices.\n\ntest_row_vector = [1 2 3]\n\n1×3 Matrix{Int64}:\n 1  2  3\n\n\n\ntest_col_vector = [1; 2; 3]\n\n3-element Vector{Int64}:\n 1\n 2\n 3\n\n\nMany linear algebra operations on vectors and matrices can be loaded using the LinearAlgebra package."
  },
  {
    "objectID": "resources/julia-basics.html#package-management",
    "href": "resources/julia-basics.html#package-management",
    "title": "Tutorial: Julia Basics",
    "section": "Package management",
    "text": "Package management\nSometimes you might need functionality that does not exist in base Julia. Julia handles packages using the Pkg package manager. After finding a package which has the functions that you need, you have two options:\n\nUse the package management prompt in the Julia REPL (the standard Julia interface; what you get when you type julia in your terminal). Enter this by typing ] at the standard green Julia prompt julia&gt;. This will become a blue pkg&gt;. You can then add new packages using add packagename.\nFrom the standard prompt, enter import Pkg; Pkg.add(packagename). The packagename package can then be used by adding using packagename to the start of the script."
  },
  {
    "objectID": "labs/lab01/solution.html",
    "href": "labs/lab01/solution.html",
    "title": "Lab 01",
    "section": "",
    "text": "Do not modify this section!"
  },
  {
    "objectID": "labs/lab01/solution.html#setup",
    "href": "labs/lab01/solution.html#setup",
    "title": "Lab 01",
    "section": "",
    "text": "Do not modify this section!"
  },
  {
    "objectID": "labs/lab01/solution.html#running-code",
    "href": "labs/lab01/solution.html#running-code",
    "title": "Lab 01",
    "section": "Running Code",
    "text": "Running Code\nWe can use Quarto to run Julia code in-line\n\nprintln(\"I'm using Julia!\")\n\nI'm using Julia!"
  },
  {
    "objectID": "labs/lab01/solution.html#rendering-the-document",
    "href": "labs/lab01/solution.html#rendering-the-document",
    "title": "Lab 01",
    "section": "Rendering the Document",
    "text": "Rendering the Document\nNow, verify that you can render the document in HTML:\n\nOpen the command palette (Cmd+Shift+P on macOS, Ctrl+Shift+P on Windows/Linux)\nType “Quarto: Render Document”\nA web browser should open with the rendered document\n\nCheck the box (replace the space with an x) to confirm that this worked\n\nthis worked for me\nthis did not work for me (specify the error below)"
  },
  {
    "objectID": "labs/lab08/template.html",
    "href": "labs/lab08/template.html",
    "title": "Lab 08: \\(K\\)-Means Clustering",
    "section": "",
    "text": "\\(K\\)-Means Clustering is a widely-used unsupervised machine learning algorithm, ideal for partitioning datasets into distinct, non-overlapping groups or ‘clusters’. We’ve seen it in the context of regional frequency analysis.\n\n\n\nInputs: \\(k\\) (number of clusters), \\(\\vb{x} = \\{x_1, x_2, x_n\\}\\) (data points)\nOutputs: \\(\\vb{c} = \\{\\mu_1, \\mu_2, \\ldots, \\mu_n\\}\\) (cluster assignments), \\(\\vb{\\mu} = \\{\\mu_1, \\mu_2, \\ldots, \\mu_k \\}\\) (cluster centroids)\nSteps:\n\nRandomly initialize \\(K\\) cluster centers: \\(\\vb{\\mu} = \\mu_1^{(0)}, \\mu_2^{(0)}, \\ldots, \\mu_k^{(0)} \\in \\mathbb{R}^d\\)\nIterate until convergence:\n\nAssign each observation \\(x_i\\) to the closest (in Euclidean distance) mean: \\[c_i^{(j)} = \\arg_{k=1, \\ldots, K} \\min \\|x_i - \\mu_k^{(j)} \\|\\]\nRecompute each \\(\\mu_k^{(j)}\\) as the mean of all points assigned to it\nTerminate when the total change of the cluster centroids satisfies \\[ \\sum_{k=1}^K \\| \\mu_k^{(j)} - \\mu_k^{(j-1)} \\| &lt; \\tau\\]"
  },
  {
    "objectID": "labs/lab08/template.html#algorithm",
    "href": "labs/lab08/template.html#algorithm",
    "title": "Lab 08: \\(K\\)-Means Clustering",
    "section": "",
    "text": "Inputs: \\(k\\) (number of clusters), \\(\\vb{x} = \\{x_1, x_2, x_n\\}\\) (data points)\nOutputs: \\(\\vb{c} = \\{\\mu_1, \\mu_2, \\ldots, \\mu_n\\}\\) (cluster assignments), \\(\\vb{\\mu} = \\{\\mu_1, \\mu_2, \\ldots, \\mu_k \\}\\) (cluster centroids)\nSteps:\n\nRandomly initialize \\(K\\) cluster centers: \\(\\vb{\\mu} = \\mu_1^{(0)}, \\mu_2^{(0)}, \\ldots, \\mu_k^{(0)} \\in \\mathbb{R}^d\\)\nIterate until convergence:\n\nAssign each observation \\(x_i\\) to the closest (in Euclidean distance) mean: \\[c_i^{(j)} = \\arg_{k=1, \\ldots, K} \\min \\|x_i - \\mu_k^{(j)} \\|\\]\nRecompute each \\(\\mu_k^{(j)}\\) as the mean of all points assigned to it\nTerminate when the total change of the cluster centroids satisfies \\[ \\sum_{k=1}^K \\| \\mu_k^{(j)} - \\mu_k^{(j-1)} \\| &lt; \\tau\\]"
  },
  {
    "objectID": "labs/lab08/template.html#initialize-centroids",
    "href": "labs/lab08/template.html#initialize-centroids",
    "title": "Lab 08: \\(K\\)-Means Clustering",
    "section": "2.1 Initialize Centroids",
    "text": "2.1 Initialize Centroids\nFirst, edit the init_centroids function. It takes in a matrix \\(X_{n \\times d}\\) indexed by \\(n\\) observations and \\(d\\) features, and returns a matrix with \\(K\\) rows (one for each centroid) and \\(d\\) columns (one for each feature) where \\(d\\) is the number of features of \\(X\\). The code provided initializes each centroid to a random value.\nYou can change this to whatever you like – be sure to explain your reasoning. One common approach is to choose \\(k\\) random observations from the dataset as your initial centroids. Be sure to make sure that your centroids are distinct!"
  },
  {
    "objectID": "labs/lab08/template.html#euclidean-distance",
    "href": "labs/lab08/template.html#euclidean-distance",
    "title": "Lab 08: \\(K\\)-Means Clustering",
    "section": "2.2 Euclidean Distance",
    "text": "2.2 Euclidean Distance\nIn order to assign observations to clusters, we need to be able to compute the distance between between an observation and a centroid. We will use the Euclidean distance, which is defined above. This function should take in two generic vectors and return a scalar."
  },
  {
    "objectID": "labs/lab08/template.html#assign-clusters",
    "href": "labs/lab08/template.html#assign-clusters",
    "title": "Lab 08: \\(K\\)-Means Clustering",
    "section": "2.3 Assign Clusters",
    "text": "2.3 Assign Clusters\nThere is just one line of code to edit here.\n\nThe argmin function may be your friend."
  },
  {
    "objectID": "labs/lab08/template.html#update-centroids",
    "href": "labs/lab08/template.html#update-centroids",
    "title": "Lab 08: \\(K\\)-Means Clustering",
    "section": "2.4 Update Centroids",
    "text": "2.4 Update Centroids\nAs you loop through the algorithm, you will need to update the centroids. This function takes in the data matrix \\(X\\), the cluster assignments \\(\\vb{c}\\), which is a vector of integers, and the number of clusters \\(k\\). It returns a matrix with \\(K\\) rows (one for each centroid) and \\(d\\) columns (one for each feature) where \\(d\\) is the number of features of \\(X\\)."
  },
  {
    "objectID": "labs/lab08/template.html#k-means-algorithm",
    "href": "labs/lab08/template.html#k-means-algorithm",
    "title": "Lab 08: \\(K\\)-Means Clustering",
    "section": "2.5 \\(K\\)-means algorithm",
    "text": "2.5 \\(K\\)-means algorithm\nThis function is provided for you. You do not need to edit it. You simply need to define all the functions it calls.\n\nfunction kmeans(X::AbstractMatrix, k::Int; τ=1e-5, maxiter=500)\n    n, d = size(X) # get the number of observations and features\n\n    # initialize the cluster centroids (μ)\n    μ = init_centroids(X, k)\n    μ_history = [μ]\n\n    is_converged = false # initialize the flag\n    j = 1 # initialize the counter\n\n    # go through the loop until convergence is reached\n    while !is_converged\n        cluster_assignments = assign_clusters(X, μ)\n        cluster_centroids = update_centroids(X, cluster_assignments, k) # update the centroids\n\n        # add the current centroids to the history\n        push!(μ_history, cluster_centroids)\n\n        # check for convergence\n        is_converged = check_convergence(μ_history, τ) # check for convergence\n\n        # if we have run too many iterations, throw an error (avoid infinite loops)\n        if j &gt; maxiter\n            @warn \"Failed to converge after $j iterations\"\n            return cluster_assignments, μ_history\n        end\n\n        # increase the counter\n        j += 1\n    end\n\n    cluster_assignments = assign_clusters(X, μ)\n\n    return cluster_assignments, μ_history\nend"
  },
  {
    "objectID": "labs/lab02/template.html",
    "href": "labs/lab02/template.html",
    "title": "Lab 02",
    "section": "",
    "text": "In this lab we will learn how to work with tabular data in Julia. Specifically, you will get some experience using:\n\nDataFrames.jl to store tabular data as a DataFrame\nCSV.jl to read CSV files and convert them to DataFrames\nDataFramesMeta.jl to manipulate DataFrames\nPlots.jl and StatsPlots.jl to create visualizations\n\n\n\nTechnically, what we are doing today is called exploratory modeling or exploratory data analysis. The latter is more common, but the former makes clear that all visualizations include some (usually implicit) conceptual model of the data. You will find more resources if you search for “exploratory data analysis”, however.\n\n\n\nI have provided you with a lot of template code.\nAny code block that starts with\n\n\n```{julia}\n\n\nis a “live” code block and will execute (run).\n\nThese blocks will automatically run when you render the document\nYou can click “Run Cell” to run the cell\nMake sure you understand what the code does and ask questions (in-person or on Canvas) if you don’t\nDo not change these blocks unless instructed\n\nAny code block that starts with\n\n\n```julia\n\n\nis source code and will not run.\n\nYou should change these blocks to complete the lab, filling in blanks and adding code as needed.\nTo make it a cell that you can run, add the brackets\n\nChanges you are expected to make are marked as follows:\n\n\n\n\n\n\nThis is an instruction.\n\n\n\n\n\n\nYou may find the following Quarto documentation pages helpful:\n\nUsing Julia\nMarkdown Basic\n\nAsk questions in class or post them on the Lab02 discussion on Canvas. The sections of this lab are numbered, so refer to the number when asking questions on Canvas to make it easier for us to help you."
  },
  {
    "objectID": "labs/lab02/template.html#exploratory-modeling",
    "href": "labs/lab02/template.html#exploratory-modeling",
    "title": "Lab 02",
    "section": "",
    "text": "Technically, what we are doing today is called exploratory modeling or exploratory data analysis. The latter is more common, but the former makes clear that all visualizations include some (usually implicit) conceptual model of the data. You will find more resources if you search for “exploratory data analysis”, however."
  },
  {
    "objectID": "labs/lab02/template.html#how-to-use-this-template",
    "href": "labs/lab02/template.html#how-to-use-this-template",
    "title": "Lab 02",
    "section": "",
    "text": "I have provided you with a lot of template code.\nAny code block that starts with\n\n\n```{julia}\n\n\nis a “live” code block and will execute (run).\n\nThese blocks will automatically run when you render the document\nYou can click “Run Cell” to run the cell\nMake sure you understand what the code does and ask questions (in-person or on Canvas) if you don’t\nDo not change these blocks unless instructed\n\nAny code block that starts with\n\n\n```julia\n\n\nis source code and will not run.\n\nYou should change these blocks to complete the lab, filling in blanks and adding code as needed.\nTo make it a cell that you can run, add the brackets\n\nChanges you are expected to make are marked as follows:\n\n\n\n\n\n\nThis is an instruction."
  },
  {
    "objectID": "labs/lab02/template.html#getting-help",
    "href": "labs/lab02/template.html#getting-help",
    "title": "Lab 02",
    "section": "",
    "text": "You may find the following Quarto documentation pages helpful:\n\nUsing Julia\nMarkdown Basic\n\nAsk questions in class or post them on the Lab02 discussion on Canvas. The sections of this lab are numbered, so refer to the number when asking questions on Canvas to make it easier for us to help you."
  },
  {
    "objectID": "labs/lab02/template.html#clone-the-respository",
    "href": "labs/lab02/template.html#clone-the-respository",
    "title": "Lab 02",
    "section": "2.1 Clone the respository",
    "text": "2.1 Clone the respository\nFirst, you’ll need to clone this repository to your computer. As with Lab 01, I recommend to use GitHub Desktop or the built-in Git support in VS Code. Remember to use the link from Canvas (classroom.github.com/...)."
  },
  {
    "objectID": "labs/lab02/template.html#install-required-packages",
    "href": "labs/lab02/template.html#install-required-packages",
    "title": "Lab 02",
    "section": "2.2 Install required packages",
    "text": "2.2 Install required packages\n\nIn VS Code, open the command palette (Windows: Ctrl+Shift+P, Mac: Cmd+Shift+P) and select Julia: Start REPL.\nIn the Julia REPL, type ] to enter the package manager.\nType activate . to activate the project environment.\nType instantiate to install the required packages. This may take a moment.1"
  },
  {
    "objectID": "labs/lab02/template.html#using-statement",
    "href": "labs/lab02/template.html#using-statement",
    "title": "Lab 02",
    "section": "2.3 Using statement",
    "text": "2.3 Using statement\nIn Julia we say using to import a package. Typically we want to do this as early as possible in a script or notebook.\n\nusing CSV\nusing DataFrames\nusing DataFramesMeta\nusing Dates\nusing Plots\nusing StatsBase: mean\nusing StatsPlots\nusing Unitful\n\n\n\nThat this is a code block code and will run. If you have not yet installed the packages, you will see an error message. Don’t change this block – see instructions to install and instantiate."
  },
  {
    "objectID": "labs/lab02/template.html#check",
    "href": "labs/lab02/template.html#check",
    "title": "Lab 02",
    "section": "2.4 Check",
    "text": "2.4 Check\nTo make sure everything is working, you should open the command palette and select Quarto: Render HTML. This will generate a HTML file from this notebook. This is a good way to check that everything is working before you start editing code."
  },
  {
    "objectID": "labs/lab02/template.html#dates",
    "href": "labs/lab02/template.html#dates",
    "title": "Lab 02",
    "section": "3.1 Dates",
    "text": "3.1 Dates\nWe can see that our DataFrame has five columns, the first of which is “Date Time”. However, the “Date Time” column is being parsed as a string. We want it to be a DateTime object from the Dates package. To do that, we need to tell Julia how the dates are formatted. We could then manually convert, but CSV.read has a kewyord argument that we can use\n\n1date_format = \"yyyy-mm-dd HH:MM\"\n2df = CSV.read(fname, DataFrame; dateformat=date_format)\nfirst(df, 3)\n\n\n1\n\nThis is a string that tells Julia how the dates are formatted. For example, 1928-01-01 00:00. See the documentation for more information.\n\n2\n\ndateformat is a keyword argument while date_format is a variable whose value is \"yyyy-mm-dd HH:MM\". We could equivalently write dateformat=\"yyyy-mm-dd HH:MM\".\n\n\n\n\n3×5 DataFrame\n\n\n\nRow\nDate Time\nWater Level\nSigma\nI\nL\n\n\n\nDateTime\nFloat64\nFloat64\nInt64\nInt64\n\n\n\n\n1\n1928-01-01T00:00:00\n-0.547\n0.0\n0\n0\n\n\n2\n1928-01-01T01:00:00\n-0.699\n0.0\n0\n0\n\n\n3\n1928-01-01T02:00:00\n-0.73\n0.0\n0\n0"
  },
  {
    "objectID": "labs/lab02/template.html#water-levels",
    "href": "labs/lab02/template.html#water-levels",
    "title": "Lab 02",
    "section": "3.2 Water levels",
    "text": "3.2 Water levels\nThe next column is “Water Level”, which is the height of the water above the reference point (NAVD) in meters. We can see that this is being parsed as a float, which is what we want 👍. However, you have to know that the data is in meters rather than inches or feet or something else. To explicitly add information about the units, we can use the Unitful package.\n\ndf[!, \" Water Level\"] .*= 1u\"m\"\nfirst(df, 3)\n\n3×5 DataFrame\n\n\n\nRow\nDate Time\nWater Level\nSigma\nI\nL\n\n\n\nDateTime\nQuantity…\nFloat64\nInt64\nInt64\n\n\n\n\n1\n1928-01-01T00:00:00\n-0.547 m\n0.0\n0\n0\n\n\n2\n1928-01-01T01:00:00\n-0.699 m\n0.0\n0\n0\n\n\n3\n1928-01-01T02:00:00\n-0.73 m\n0.0\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n\nCode explanation\n\n\n\n\nWe select the column with water levels using its name. The ! means “all rows”. Thus, df[!, \" Water Level\"] is a vector of all the water levels stored.\n*= means to multiply in place. For example, if x=2 then x *= 2 is equivalent to x = x * 2. .*= is a vector syntax, meaning do the multiplication to each element of the vector individually.\n1u\"m\" is a Unitful object that represents 1 meter. We multiply the water levels by this to convert them to meters."
  },
  {
    "objectID": "labs/lab02/template.html#subsetting-and-renaming",
    "href": "labs/lab02/template.html#subsetting-and-renaming",
    "title": "Lab 02",
    "section": "3.3 Subsetting and renaming",
    "text": "3.3 Subsetting and renaming\nWe want to only keep the first two (for more on the other three, see here). We can also rename the columns to make them easier to work with (spaces in variable names are annoying). To do this, we use the @rename function:\n\n1df = @rename(df, :datetime = $\"Date Time\", :lsl = $\" Water Level\");\n\n\n1\n\nThe $ is needed here because the right hand side is a string, not a symbol.\n\n\n\n\nThen, we can use the @select function to do select the columns we want. Notice how the first argument to select is the DataFrame and the subsequent arguments are column names. Notice also that our column names were strings (\"Date Time\"), but we can also use symbols (:datetime).\n\ndf = @select(df, :datetime, :lsl)\nfirst(df, 3)\n\n3×2 DataFrame\n\n\n\nRow\ndatetime\nlsl\n\n\n\nDateTime\nQuantity…\n\n\n\n\n1\n1928-01-01T00:00:00\n-0.547 m\n\n\n2\n1928-01-01T01:00:00\n-0.699 m\n\n\n3\n1928-01-01T02:00:00\n-0.73 m\n\n\n\n\n\n\nFor more on what DataFramesMeta can do, see this Tweet."
  },
  {
    "objectID": "labs/lab02/template.html#writing-a-function",
    "href": "labs/lab02/template.html#writing-a-function",
    "title": "Lab 02",
    "section": "3.4 Writing a function",
    "text": "3.4 Writing a function\nWe have just done a lot of work to read in our data. However, this just gives us data for the year 1928. In fact, we have a CSV file for each year 1928-2021. To make sure we can read them each in excatly the same way, we want to write a function.\nfunction read_tides(year::Int)\n    fname = \"data/tidesandcurrents-8638610-$(year)-NAVD-GMT-metric.csv\" # don't change this\n    date_format = \"yyyy-mm-dd HH:MM\" # don't change this\n    # your code here\n    # 1. read in the CSV file and save as a dataframe\n    # 2. convert the \"Date Time\" column to a DateTime object\n    # 3. convert the \" Water Level\" column to meters\n    # 4. rename the columns to \"datetime\" and \"lsl\"\n    # 5. select the \"datetime\" and \"lsl\" columns\n    # 6. return the dataframe\nend\n\n# print out the first 10 rows of the 1928 data\nfirst(read_tides(1928), 10) \n\n\n\n\n\n\nInstructions\n\n\n\nFill out this function. Your function should implement the six steps indicated in the instructions. When it’s done, convert it to a live code block with: ```{julia}. When you run this code, it should print out the first 10 rows of the 1928 data. Make sure they look right!"
  },
  {
    "objectID": "labs/lab02/template.html#combining-files",
    "href": "labs/lab02/template.html#combining-files",
    "title": "Lab 02",
    "section": "3.5 Combining files",
    "text": "3.5 Combining files\nNow that we have the ability to read in the data corresponding to any year, we can read them all in and combine into a single DataFrame. First, let’s read in all the data.\nyears = 1928:2021 # all the years of data\nannual_data = # 1. call the read_tides function on each year\ntypeof(annual_data) # should be a vector of DataFrames\nNext, we’ll use the vcat function to combine all the data into a single DataFrame.\ndf = vcat(annual_data...) # don't change this\nfirst(df, 5)\nlast(df, 5) # check the last 5 years\n\n\n\n\n\n\nInstructions\n\n\n\n\nCall the read_tides function on each year\nTurn the two code blocks in this section into live code blocks\nRun the code and make sure the first 5 rows and last five rows look right"
  },
  {
    "objectID": "labs/lab02/template.html#time-series-plot",
    "href": "labs/lab02/template.html#time-series-plot",
    "title": "Lab 02",
    "section": "4.1 Time series plot",
    "text": "4.1 Time series plot\nLet’s start with a simple time series plot of the water levels. Our data is collected hourly, so we have a lot of data points! Still, we can plot them all.\n\nplot(\n    df.datetime,\n    df.lsl;\n    title=\"Water levels at Sewells Point, VA\",\n1    ylabel=\"Water level\",\n2    label=false,\n)\n\n\n1\n\nBecause we are using the Unitful pacakge, the y-axis label will automatically include the units!!!\n\n2\n\nWe are only plotting one “series” (data set), so we don’t need a legend."
  },
  {
    "objectID": "labs/lab02/template.html#zooming-in",
    "href": "labs/lab02/template.html#zooming-in",
    "title": "Lab 02",
    "section": "4.2 Zooming in",
    "text": "4.2 Zooming in\nFocusing on the entire time series means we can’t dig into the details. Let’s zoom in on a single month (October 1928) using the @subset function.\n\nt_start = Dates.DateTime(1928, 10, 1, 0)\nt_end = Dates.DateTime(1928, 10, 31, 23)\ndf_month = @subset(df, t_start .&lt;= :datetime .&lt;= t_end)\nfirst(df_month, 3)\n\n3×2 DataFrame\n\n\n\nRow\ndatetime\nlsl\n\n\n\nDateTime\nQuantity…\n\n\n\n\n1\n1928-10-01T00:00:00\n0.215 m\n\n\n2\n1928-10-01T01:00:00\n0.429 m\n\n\n3\n1928-10-01T02:00:00\n0.581 m\n\n\n\n\n\n\nNow we can plot it as above.\n\nplot(\n    df_month.datetime,\n    df_month.lsl;\n    title=\"Water levels at Sewells Point, VA\",\n    ylabel=\"Water level\",\n    label=false,\n)"
  },
  {
    "objectID": "labs/lab02/template.html#instructions-2",
    "href": "labs/lab02/template.html#instructions-2",
    "title": "Lab 02",
    "section": "4.3 Instructions",
    "text": "4.3 Instructions\nChange the start and end dates to plot March 2020. What do you notice? :::"
  },
  {
    "objectID": "labs/lab02/template.html#groupby",
    "href": "labs/lab02/template.html#groupby",
    "title": "Lab 02",
    "section": "4.4 Groupby",
    "text": "4.4 Groupby\nAn essential idea in working with tabular data (and other data formats) is “split-apply-combine”. Essentially: split the data into groups, apply some function to each group, and then combine the results.\nWe can use this workflow to answer an interesting question: what is the average water level for each month?2\n\n1df[!, :month] = Dates.month.(df.datetime)\n2dropmissing!(df, :lsl)\n3df_bymonth = groupby(df, :month)\n4df_climatology = combine(df_bymonth, :lsl =&gt; mean =&gt; :lsl_avg);\n\n\n1\n\nThis creates a new column called :month that is the month of each observation.\n\n2\n\nThis will discard any rows in df that have a missing value of :lsl. This is necessary because the mean function will return missing if any of the values are missing.\n\n3\n\nThis creates a GroupedDataFrame object that contains all the data grouped by month.\n\n4\n\nThis takes the grouped data and calculates the mean of the :lsl column for each month. The general syntax is combine(grouped_df, :column =&gt; function).\n\n\n\n\nWe can now plot the climatology.\n\nplot(\n1    df_climatology.month,\n    df_climatology.lsl_avg;\n2    xticks=1:12,\n    xlabel=\"Month\",\n    ylabel=\"Average Water level\",\n3    linewidth=3,\n    label=false,\n)\n\n\n1\n\nWe can use df.colname instead of df[!, :colname]. The latter is more robust but the former is easier to type.\n\n2\n\nSetting xticks will set the x-axis ticks to the values in the vector. We can use this to make sure the x-axis ticks are labeled with the months.\n\n3\n\nWe can set the line width to make the plot easier to read.\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstructions\n\n\n\n\nUse the full dataset to plot the climatology using data from all years\nNext, create a new Markdown header (## Groupby Day of Year) plot the average water level for each day of the year (Dates.dayofyear from 1 to 366).\nWhat do you notice?"
  },
  {
    "objectID": "labs/lab02/template.html#boxplot",
    "href": "labs/lab02/template.html#boxplot",
    "title": "Lab 02",
    "section": "4.5 Boxplot",
    "text": "4.5 Boxplot\nBoxplots are ways to visualize the distribution of data. They show the median (the line in the middle of the box), the interquartile range (the box), and the range of the data (the whiskers). Outliers are shown as dots. We can use the boxplot function from the StatsPlots.jl package:\n\nboxplot(\n1    df[!, :month],\n    df[!, :lsl];\n    xticks=1:12,\n    xlabel=\"Month\",\n    ylabel=\"Water level\",\n    label=false,\n2    title=\"Climatology\",\n)\n\n\n1\n\nWe are back to df[!, :colname] syntax. Both work!\n\n2\n\nWe can set the title using the title keyword argument.\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstructions\n\n\n\nRepeat this analysis grouping by year rather than month. What do you notice from the boxplots?"
  },
  {
    "objectID": "labs/lab02/template.html#submission",
    "href": "labs/lab02/template.html#submission",
    "title": "Lab 02",
    "section": "4.6 Submission",
    "text": "4.6 Submission\n\nRemove all of the instructions blocks (from the first ::: {.callout-important} to the following :::)\nRemove all of the text I have written (including this block) so that all the text is your own. This makes it much easier to grade and to follow what is yours.\n\nDo not delete any of the headers\n\nThus, the beginning of the document should have the headers for the overview, exploratory modeling, instructions, and getting help, but these should not have any text\nThis will ensure all documents have the same numbering to make it easier to troubleshoot any issues\n\nDo not delete any of the code\n\nMake sure your code runs (click the “Run All” button in the command palette)\nRender your notebook as HTML (optional)\n\nOpen comand palette\nSelect Quarto: Render HTML\nThis will create a website that you can view in your browser. The address will be in your terminal like\n\nWatching files for changes\nBrowse at http://localhost:4200/labs/lab02/instructions.html\n\nMake sure your notebook looks right in the browser.\n\nRender your hnotebook as DOCX (required)\n\nOpen comand palette\nSelect Quarto: Render DOCX\nIt may give you a link to click on in order to download the file. Click it and it will be downloaded to your computer (probably in your Downloads folder)\n\nSubmit the .docx file to Canvas.\n\nProofread before you submit!"
  },
  {
    "objectID": "labs/lab02/template.html#footnotes",
    "href": "labs/lab02/template.html#footnotes",
    "title": "Lab 02",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nJulia precompiles packages when they are installed, and (to a lesser extent) when they are first used. The first time you use a package it may take a moment to load. This is normal, nothing to worry about, and rapidly improving.↩︎\nTo do a better job, we should separate out the long-term trend from the seasonal cycle. This is called de-trending and is a common technique in climate science. We can worry more about this later.↩︎"
  },
  {
    "objectID": "labs/lab04/template.html",
    "href": "labs/lab04/template.html",
    "title": "Lab 04",
    "section": "",
    "text": "Use linear regression to relate the mean annual runoff of several streams (runoff) with nine other variables:\n\nthe precipitation falling at the gage (precip),\nthe drainage area of the basin (area),\nthe average slope of the basin (slope)\nthe length of the drainage basin (length)\nthe perimeter of the basin (perim)\nthe diameter of the largest circle which could be inscribed within the drainage basin (diameter)\nthe shape factor of the basin (shape_factor)\nthe stream frequency – the ratio of the number of streams in the basin to the basin area (stream_freq)\nthe relief ratio for the basin (relief_ratio)\n\n\n\n\n\n\n\nCredit\n\n\n\nThis is a light modification of Example 11.3 from\nHelsel, D. R., Hirsch, R. M., Ryberg, K. R., Archfield, S. A., & Gilroy, E. J. (2020). Statistical methods in water resources. Techniques and Methods. U.S. Geological Survey. https://doi.org/10.3133/tm4A3"
  },
  {
    "objectID": "labs/lab04/template.html#data",
    "href": "labs/lab04/template.html#data",
    "title": "Lab 04",
    "section": "2.1 Data",
    "text": "2.1 Data\nNext, we need to load in the data.\n\nrunoff_df = CSV.read(\"data/runoff.csv\", DataFrame)\ndisplay(size(runoff_df))\nfirst(runoff_df, 3)\n\n(13, 10)\n\n\n3×10 DataFrame\n\n\n\nRow\nrunoff\nprecip\narea\nslope\nlength\nperim\ndiameter\nshape_factor\nstream_freq\nrelief_ratio\n\n\n\nFloat64\nFloat64\nFloat64\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nInt64\n\n\n\n\n1\n17.38\n44.37\n2.21\n50\n2.38\n7.93\n0.91\n0.38\n1.36\n332\n\n\n2\n14.62\n44.09\n2.53\n7\n2.55\n7.65\n1.23\n0.48\n2.37\n55\n\n\n3\n15.48\n41.25\n5.63\n19\n3.11\n11.61\n2.11\n0.57\n2.31\n77"
  },
  {
    "objectID": "labs/lab04/template.html#model-specification",
    "href": "labs/lab04/template.html#model-specification",
    "title": "Lab 04",
    "section": "3.1 Model specification",
    "text": "3.1 Model specification\nWe are going to fit models of the form \\[\ny_i \\sim \\mathcal{N} \\left( \\beta X_i , \\sigma \\right).\n\\] We only have 13 observations of 10 variables, so we’ll need to be careful with our data. Before diving into the model, let’s look at the data."
  },
  {
    "objectID": "labs/lab04/template.html#pairwise-correlations",
    "href": "labs/lab04/template.html#pairwise-correlations",
    "title": "Lab 04",
    "section": "3.2 Pairwise correlations",
    "text": "3.2 Pairwise correlations\nOne simple way to look at the data is to look at the pairwise correlations between the variables. The following function plot_correlation will plot a heatmap of the pairwise correlations.\n\n\nCode\nfunction plot_correlation(df::AbstractDataFrame)\n    varnames = names(df)\n1    correlations = cor(Matrix(df))\n    P = size(correlations, 1)\n\n    p = heatmap(\n        varnames, # x labels\n        varnames, # y labels\n        correlations; # variable\n        aspect_ratio=1, # 1:1 aspect ratio\n        xrotation=30, # rotate x ticks\n        size=(600, 600), # specify the size of the plot\n        title=\"Pairwise Correlations\",\n        clims=(-1, 1),\n        cmap=:PuOr, # select a diverging color map\n    )\n\n    # add the exact text\n    ann = [\n        (i - 0.5, j - 0.5, text(round(correlations[i, j]; digits=2), 8, :white, :center))\n        for i in 1:P for j in 1:P\n    ]\n    annotate!(p, ann; linecolor=:white)\n    return p\nend\n\n\n\n1\n\ncor expects a Matrix input and will throw an error if we give it a DataFrame. Matrix(df) just extracts numeric values for our DataFrame. This won’t work well if we have non-numeric columns, but we don’t here.\n\n\n\n\n\nplot_correlation(runoff_df)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nDescribe what you learn from this plot in the specific context of modeling the relationship between runoff and the other variables. What can you not learn from this plot?"
  },
  {
    "objectID": "labs/lab04/template.html#define-model",
    "href": "labs/lab04/template.html#define-model",
    "title": "Lab 04",
    "section": "4.1 Define model",
    "text": "4.1 Define model\nWe can implement this in Turing as follows:\n#| output: false\n@model function linear_reg(X::AbstractVector, y::AbstractVector)\n\n    # First, define all parameters\n    # they need priors so we use a ~\n    α # the intercept\n    β # the slope\n    σ # the standard deviation\n\n    # We can calculate intermediate quantities in our function\n    # Turing knows this isn't a parameter because we use = not ~\n    μ = α .+ β .* X\n\n    # likelihood model\n    y ...\nend\n::: {.callout-important} ## Task\nImplement the model above so that it is consistent with the mathematical model written above. You will iterate between this and the next step a few times. You only need to keep your final model, but briefly describe what you did and why. :::## Prior predictive check\nBefore we fit the model, we run a prior predictive check. This lets us check whether the prior assumptions we have encoded into the model are consisten with our beliefs. First we sample from the prior:\ny = vec(runoff_df[!, :runoff])\nX = vec(runoff_df[!, :YOUR_VAR_HERE]) # which variable did you choose?\nppc_linear = let\n    model = linear_reg(X, y) # call the model\n    sampler = Prior() # we want to sample from the prior\n    nsamples = 10_000\n    sample(model, sampler, nsamples; drop_warmup=true)\nend\nWe can use the lm_predict function to generate predictions from the prior.\n\n\nCode\nfunction lm_predict(chn::Turing.MCMCChains.Chains, X::AbstractVecOrMat, N::Int)\n1    p = get_params(chn)\n2    α = vec(p[:α])\n3    β = if isa(p[:β], Tuple)\n        β_matrix = hcat(vec.(p[:β])...)\n        [β_matrix[i, :] for i in 1:size(β_matrix, 1)]\n    else\n        vec(p[:β])\n    end\n4    idx = rand(1:length(α), N)\n5    ŷ = [\n        α[i] .+ X * β[i] for i in idx\n    ]\n6    if N == 1\n        ŷ = ŷ[1]\n    end\n    return ŷ\nend\n\n\n\n1\n\nThis function extracts the parameters from the chain\n\n2\n\nIf we use multiple chains, then p[:α] is a Matrix. We convert it to a vector so that it’s “flat”.\n\n3\n\nIf we have multiple predictors (multivariate linear regression) then p[:β] is a tuple of vectors. We convert it into a vector of vectors so that β[i] is a vector of coefficients for the \\(i\\)th sample.\n\n4\n\nWe randomly select N samples from the posterior.\n\n5\n\nWe calculate the predictions for each of the N samples.\n\n6\n\nIf N is 1, then we return a vector, not a vector of vectors.\n\n\n\n\nWe leverage this function to generate predictions from the prior and plot them.\nX_pred = ... # specify the range of values for\np1 = plot(xlabel=\"Precip\", ylabel=\"Runoff\", title=\"Prior predictive check\")\nfor i in 1:250\n    y_pred = lm_predict(ppc_linear, X_pred, 1)\n    plot!(p1, X_pred, y_pred; color=:black, alpha=0.1, label=false)\nend\np1\n\n\n\n\n\n\nTask\n\n\n\nFill in the missing lines in the above blocks and run."
  },
  {
    "objectID": "labs/lab04/template.html#inference",
    "href": "labs/lab04/template.html#inference",
    "title": "Lab 04",
    "section": "4.2 Inference",
    "text": "4.2 Inference\nOnce we have a prior that we’re happy with, we can use our model to generate samples from the posterior distribution. We draw 4 chains of 5000 samples. If you get a weird error here, it may be because of threading. Try the mapreduce based approach here rather than waste time debugging.\nchn_linear = let\n    model = ... # FILL IN\n    sampler = externalsampler(DynamicHMC.NUTS())\n    n_per_chain = 5000\n    nchains = 4\n    sample(model, sampler, MCMCThreads(), n_per_chain, nchains; drop_warmup=true)\nend\nsummarystats(chn_linear)\nBased on \\(\\hat{R}\\) values close to 1, we have some evidence that our chains have converged. We can also check the trace plots:\n... # Fill in to generate the trace plot\n\n\n\n\n\n\nTask\n\n\n\nFill in the missing lines in the above blocks. Describe what you learn from the trace plot."
  },
  {
    "objectID": "labs/lab04/template.html#posterior-predictive-checks",
    "href": "labs/lab04/template.html#posterior-predictive-checks",
    "title": "Lab 04",
    "section": "4.3 Posterior predictive checks",
    "text": "4.3 Posterior predictive checks\nWe can again plot our predictions, this time using the posterior samples. We add the data to the plot as well.\np2 = plot(xlabel=\"Precip\", ylabel=\"Runoff\", title=\"Posterior\") # blank plot\npred_linear = lm_predict(chn_linear, X_pred, 500) # generate predictions\nfor y_pred in pred_linear\n    plot!(p2, X_pred, y_pred; color=:black, alpha=0.1, label=false) # add each prediction to a plot\nend\n# add the observations with scatter!\np2\n\n\n\n\n\n\nTask\n\n\n\nAdd the observations to the plot above with scatter!"
  },
  {
    "objectID": "labs/lab04/template.html#prior-predictive-check",
    "href": "labs/lab04/template.html#prior-predictive-check",
    "title": "Lab 04",
    "section": "5.1 Prior predictive check",
    "text": "5.1 Prior predictive check\nX = Matrix(runoff_df[:, Not(:runoff)]) # all columns except runoff\ny = vec(runoff_df[!, :runoff]) # runoff \n\nppc_multi = let\n    ... # FILL IN\nend\nHow should we visualize the prior predictive check? A simple way is to visualize the implied distribution of \\(y\\) given observed \\(X\\):\nhistogram(\n    vcat(lm_predict(ppc_multi, X, 500)...),\n    xlabel=L\"$y$\",\n    ylabel=L\"Implied $p(y)$\",\n    label=false,\n    normalize=:pdf,\n    yticks=false,\n    title=\"Prior predictive check\",\n)\n\n\n\n\n\n\nTask\n\n\n\nModify the above code to draw samples from the prior and save as ppc_multi."
  },
  {
    "objectID": "labs/lab04/template.html#inference-1",
    "href": "labs/lab04/template.html#inference-1",
    "title": "Lab 04",
    "section": "5.2 Inference",
    "text": "5.2 Inference\nNow that our priors seem plausible (we could a;ways do better, but that’s not the point of this lab), we can fit the model.julia chn_multi = let     ... end # summary stats\n\n\n\n\n\n\nTask\n\n\n\nDraw the samples from the posterior and save as chn_multi. Display the summary statistics. What do you learn?"
  },
  {
    "objectID": "labs/lab04/template.html#posterior-predictive-check",
    "href": "labs/lab04/template.html#posterior-predictive-check",
    "title": "Lab 04",
    "section": "5.3 Posterior predictive check",
    "text": "5.3 Posterior predictive check\nHow should we visualize these inferences? It’s hard to plot a single \\(x\\) vs \\(y\\) when we have multiple \\(x\\). There are many possible plots, but a good one is to plot the predicted vs actual values of \\(y\\). To make the plot more readable, we’ll add some jitter to the data. Here’s a function to do that:\n\n\nCode\nfunction jitter(x, y; ϵx=0.1, ϵy=0.1)\n    x_jit = x .+ rand(Normal(0, ϵx), size(x))\n    y_jit = y .+ rand(Normal(0, ϵy), size(y))\n    return x_jit, y_jit\nend\n\n\nNow we can create our scatter plot.\np4 = plot(; xlabel=\"Predicted Runoff\", ylabel=\"Actual Runoff\", aspect_ratio=1) # blank plot\nfor i in 1:500 # loop through\n    pred = lm_predict(chn_multi, X, 1)\n    xplt, yplt = jitter(pred, y; ϵx=0.1, ϵy=0.1)\n    scatter!(xplt, yplt; color=:black, alpha=0.5, label=false, markersize=0.5)\nend\nPlots.abline!(p4, 1, 0; color=:red, label=\"1:1 line\")\np4\n\n\n\n\n\n\nTask\n\n\n\nThe above block shouldn’t need to be modified. Add the curly brackets when you are ready. What do you learn from this plot?"
  },
  {
    "objectID": "labs/lab03/template.html",
    "href": "labs/lab03/template.html",
    "title": "Lab 03",
    "section": "",
    "text": "In the wake of a severe flood, an insurance company has comissioned you to study flood damage in one of the most-affected neighborhoods. This neighborhood is perfectly flat, so we can assume that all houses experienced the same flood depth. However, they are elevated to different heights, use different materials, and are built to different standards, and as a result they experienced different amounts of damage. Specifically, the insurance company has provided you with a dataset of the fraction of the value of each house that was lost in the flood and asked you to model the distribution of losses.\n\n\nOur model for the loss fraction \\(y_i\\) for the \\(i\\)th house is \\[\ny_i | \\alpha, \\beta \\sim \\text{Beta}(\\alpha, \\beta)\n\\] where \\(\\alpha\\) and \\(\\beta\\) are the shape parameters of the Beta Distribution. Because we are not including any explanatory information in our model, we are assuming that the distribution of loss fractions is the same for all houses. This is reasonable for our neighborhood, but would not be applicable to another neighborhood or a different flood event.\nThe above notation is shorthand for \\[\np(y_i | \\alpha, \\beta) =\n\\frac{x^{\\alpha-1}(1-x)^{\\beta-1}} {\\mathrm{B}(\\alpha,\\beta)}\\!\n\\] where \\(\\mathrm {B} (\\alpha ,\\beta )={\\frac {\\Gamma (\\alpha )\\Gamma (\\beta )}{\\Gamma (\\alpha +\\beta )}}\\) and \\(\\Gamma\\) is the Gamma function. We will work directly with the pdf and logpdf functions in the Distributions package, so you don’t need to memorize this formula..\n\n\n\nAs for labs 1 and 2, make sure you follow the three standard initial setup steps:\n\nOpen the lab 3 folder in VS Code. Do NOT open a “parent” directory containing lab 3. If you’re not sure what folder you’re in, open the Juila REPL and type pwd(). It should say something like /.../lab03-username....\nActivate the project environment: ] to enter package mode then activate .. Don’t forget the . at the end, it’s very important.\nInstall all required packages: ] to enter package mode, then instantiate.\n\nAt this point, check to make sure you can render the document. In VS Code, open the command palette (Windows: Ctrl+Shift+P, Mac: Cmd+Shift+P) and type Render: HTML. If this gives you trouble, try the following:\n\n] in the Julia REPL to enter Package mode\nbuild IJulia\n\nIf this still gives you trouble, ask for help on Canvas or in-person.\n\n\n\nAs usual we start by specifying the packages we are using\n\nusing Distributions # probability distributions\nusing DelimitedFiles # read data\nusing LaTeXStrings # LaTeX plot labels\nusing Optim # optimization\nusing Plots # plotting\nusing StatsPlots # plot distributions\n\n[ Info: Precompiling StatsPlots [f3b207a7-027a-5e70-b257-86293d7955fd]\n\n\n\n\n\nWe can learn something about the Beta distribution defined above by plotting it for some different values:\n\n1p = plot(; xlabel=L\"y\", ylabel=L\"p(y | \\alpha, \\beta)\", legend=:top)\n2for (α, color) in zip([1.0, 5.0, 25.0], [:orange, :purple, :black])\n3    for (β, linestyle) in zip([1.0, 5.0, 25.0], [:solid, :dash, :dot])\n4        plot!(\n            p,\n5            Beta(α, β);\n            label=\"α = $α, β = $β\",\n            color=color,\n            linestyle=linestyle,\n            linewidth=2\n        )\n    end\nend\np\n\n\n1\n\nThis defines a blank plot for us to add to\n\n2\n\nWhen we loop through our values of α, we zip it with a vector of colors so that we can plot each value of α in a different color.\n\n3\n\nSimilarly, we can attach each value of β to a different linestyle.\n\n4\n\nplot!(p, ...) will add more elements to the plot p that we defined above.\n\n5\n\nUsing the StatsPlots package, we can plot a distribution by passing it to plot! with no additional arguments."
  },
  {
    "objectID": "labs/lab03/template.html#mathematical-model",
    "href": "labs/lab03/template.html#mathematical-model",
    "title": "Lab 03",
    "section": "",
    "text": "Our model for the loss fraction \\(y_i\\) for the \\(i\\)th house is \\[\ny_i | \\alpha, \\beta \\sim \\text{Beta}(\\alpha, \\beta)\n\\] where \\(\\alpha\\) and \\(\\beta\\) are the shape parameters of the Beta Distribution. Because we are not including any explanatory information in our model, we are assuming that the distribution of loss fractions is the same for all houses. This is reasonable for our neighborhood, but would not be applicable to another neighborhood or a different flood event.\nThe above notation is shorthand for \\[\np(y_i | \\alpha, \\beta) =\n\\frac{x^{\\alpha-1}(1-x)^{\\beta-1}} {\\mathrm{B}(\\alpha,\\beta)}\\!\n\\] where \\(\\mathrm {B} (\\alpha ,\\beta )={\\frac {\\Gamma (\\alpha )\\Gamma (\\beta )}{\\Gamma (\\alpha +\\beta )}}\\) and \\(\\Gamma\\) is the Gamma function. We will work directly with the pdf and logpdf functions in the Distributions package, so you don’t need to memorize this formula.."
  },
  {
    "objectID": "labs/lab03/template.html#setup",
    "href": "labs/lab03/template.html#setup",
    "title": "Lab 03",
    "section": "",
    "text": "As for labs 1 and 2, make sure you follow the three standard initial setup steps:\n\nOpen the lab 3 folder in VS Code. Do NOT open a “parent” directory containing lab 3. If you’re not sure what folder you’re in, open the Juila REPL and type pwd(). It should say something like /.../lab03-username....\nActivate the project environment: ] to enter package mode then activate .. Don’t forget the . at the end, it’s very important.\nInstall all required packages: ] to enter package mode, then instantiate.\n\nAt this point, check to make sure you can render the document. In VS Code, open the command palette (Windows: Ctrl+Shift+P, Mac: Cmd+Shift+P) and type Render: HTML. If this gives you trouble, try the following:\n\n] in the Julia REPL to enter Package mode\nbuild IJulia\n\nIf this still gives you trouble, ask for help on Canvas or in-person."
  },
  {
    "objectID": "labs/lab03/template.html#package-imports",
    "href": "labs/lab03/template.html#package-imports",
    "title": "Lab 03",
    "section": "",
    "text": "As usual we start by specifying the packages we are using\n\nusing Distributions # probability distributions\nusing DelimitedFiles # read data\nusing LaTeXStrings # LaTeX plot labels\nusing Optim # optimization\nusing Plots # plotting\nusing StatsPlots # plot distributions\n\n[ Info: Precompiling StatsPlots [f3b207a7-027a-5e70-b257-86293d7955fd]"
  },
  {
    "objectID": "labs/lab03/template.html#about-the-beta-distribution",
    "href": "labs/lab03/template.html#about-the-beta-distribution",
    "title": "Lab 03",
    "section": "",
    "text": "We can learn something about the Beta distribution defined above by plotting it for some different values:\n\n1p = plot(; xlabel=L\"y\", ylabel=L\"p(y | \\alpha, \\beta)\", legend=:top)\n2for (α, color) in zip([1.0, 5.0, 25.0], [:orange, :purple, :black])\n3    for (β, linestyle) in zip([1.0, 5.0, 25.0], [:solid, :dash, :dot])\n4        plot!(\n            p,\n5            Beta(α, β);\n            label=\"α = $α, β = $β\",\n            color=color,\n            linestyle=linestyle,\n            linewidth=2\n        )\n    end\nend\np\n\n\n1\n\nThis defines a blank plot for us to add to\n\n2\n\nWhen we loop through our values of α, we zip it with a vector of colors so that we can plot each value of α in a different color.\n\n3\n\nSimilarly, we can attach each value of β to a different linestyle.\n\n4\n\nplot!(p, ...) will add more elements to the plot p that we defined above.\n\n5\n\nUsing the StatsPlots package, we can plot a distribution by passing it to plot! with no additional arguments."
  },
  {
    "objectID": "labs/lab03/template.html#likelihood-model",
    "href": "labs/lab03/template.html#likelihood-model",
    "title": "Lab 03",
    "section": "2.1 Likelihood model",
    "text": "2.1 Likelihood model\nfunction log_lik(y::Vector{T}, α::T, β::T) where {T&lt;:Real}\n    # fill in here\n    # refer to lecture slides for syntax and function names\n    # don't forget your `return` statement!\nend\n\n\n\n\n\n\nFill in the log_lik function where y is a vector of data, and α and β are the parameters of the Beta distribution. Then, convert it to live code by adding curly brackets, like you did in lab 02. Hint: define a function function log_lik(y::T, α::T, β::T) where {T&lt;:Real}... that takes in a single point. Then this function that takes in a vector y can calculate the log-likelihood for each data point individually and combine them. This is not the only way to solve this problem"
  },
  {
    "objectID": "labs/lab03/template.html#check-if-its-right",
    "href": "labs/lab03/template.html#check-if-its-right",
    "title": "Lab 03",
    "section": "2.2 Check if it’s right",
    "text": "2.2 Check if it’s right\nWe can check your log_lik function by comparing what you calculate using it to a known, correct value.\nyour_val = log_lik([0.2, 0.4, 0.6, 0.8], 5., 5.) # calls your implementation\ntrue_value = -0.2947032775653282 # I calculated this\n@assert isapprox(your_val, true_value) # checks if they're close\nprintln(\"😁\")\n\nConvert this to a live code block by adding curly brackets, like you did in lab 02, and run. You should see a 😄."
  },
  {
    "objectID": "labs/lab03/template.html#plot",
    "href": "labs/lab03/template.html#plot",
    "title": "Lab 03",
    "section": "2.3 Plot",
    "text": "2.3 Plot\nNow that we have the log_likelihood, we’re going to plot it for many values of \\(\\alpha\\) and \\(\\beta\\).\nFirst, let’s define a grid of values for \\(\\alpha\\) and \\(\\beta\\) to plot:\n\nα_plot = range(0.01, 50; length=500)\nβ_plot = range(0.01, 50; length=501)\n\n\nDefine them to be different lengths so that if we make an indexing mistake, we should get an error.\n\nNext, we can calculate the log likelihood at each point on the grid of α_plot and β_plot as:\nlog_lik_fax = [log_lik(fax, αi, βi) for αi in α_plot, βi in β_plot]\n\n# ensure that the dimensions are correct\n@assert size(log_lik_fax) == (length(α_plot), length(β_plot))\ndisplay(size(log_lik_fax))\nNow, we’re ready to plot. You can use the following code, which provides some helpful keyword arguments to make your plot look nice. Feel free to play around with them.\np1 = plot(\n    α_plot,\n    β_plot,\n    log_lik_fax';\n    st=:heatmap,\n    xlabel=L\"$\\alpha$\",\n    ylabel=L\"$\\beta$\",\n    legend=:topright,\n    colorbar_title=L\"$\\log p(y | \\alpha, \\beta)$\"\n)\np1\n\n\n\n\n\n\nSize and plotting\n\n\n\nNote that we plot log_lik_fax' instead of log_lik_fax. That is equivalent to transpose(log_lik_fax). This is a quirk of plots.jl syntax (see here). We avoid confusion by checking the size of log_lik_fax above.\n\n\n\n\n\n\n\n\nNote that we have assigned our plot a variable name, p1. This will let us add elements to it later.\n\n\n\n\n\n\n\n\n\nAdd curly brackets to the code blocks in this section once your log_lik function is working."
  },
  {
    "objectID": "labs/lab03/template.html#best-estimates",
    "href": "labs/lab03/template.html#best-estimates",
    "title": "Lab 03",
    "section": "2.4 Best estimates",
    "text": "2.4 Best estimates\nWhat values of α_plot and β_plot that maximize the log likelihood? We can find out in several different ways.\n\n2.4.1 Optimization on a grid\nThe easiest thing to do is to find the maximum value of the log likelihood on our grid. We can do that as follows:\nidx_fax = argmax(log_lik_fax) # the index that maximizes the log likelihood\nα_fax_best = α_plot[idx_fax[1]]\nβ_fax_best = β_plot[idx_fax[2]]\n\n\n\n\n\n\nAdd curly brackets to the code blocks in this section once your log_lik function is working.\n\n\n\n\n\n2.4.2 Optim.jl\nWe can also use the optimize function from Optim.jl to find the maximum value of the log likelihood.\n# define the function to be optimized\nloss_fax(θ) = # define the negative log-likelihood here, using your `log_lik` function and passing in `fax` as the `y` argument.\nlower = [0.001, 0.001] # lower bound -- don't need to change\nupper = [Inf, Inf] # upper bound -- don't need to edit\nguess = [1.0, 1.0] # initial guess -- leave as-is\n\nres = optimize(loss_fax, lower, upper, guess) # run the optimization\nθ_fax = Optim.minimizer(res) # get the best parameters\n\n\n\n\n\n\nAdd curly brackets to the code blocks in this section once you have implemented everything. Follow the commented instructions.\n\n\n\n\n\n2.4.3 Distributions.jl\nAs an alternative to Optim.jl, we can use the fit_mle function from Distributions.jl for most distributions. We can use this to check our work:\nDistributions.fit_mle(Beta, fax)\n\n\n\n\n\n\nAdd curly brackets to the code blocks in this section once you have implemented everything above. Compare the fit_mle result to your result using optimize."
  },
  {
    "objectID": "labs/lab03/template.html#update-the-plot",
    "href": "labs/lab03/template.html#update-the-plot",
    "title": "Lab 03",
    "section": "2.5 Update the plot",
    "text": "2.5 Update the plot\nThese best estimates should show up as points on our plot. We can add them as follows:\nscatter!(p1, [α_fax_best], [β_fax_best]; label=\"Grid Search\")\nscatter!(p1, [θ_fax[1]], [θ_fax[2]]; label=\"MLE\")\np1\n\n\nWhen we add a single point, we have to wrap it in brackets [] to make it a vector. The x and y inputs to plot (or scatter!) need to be vectors.\n\n\n\n\n\n\nAdd curly brackets to the code blocks in this section once you have implemented everything above."
  },
  {
    "objectID": "labs/lab05/template.html",
    "href": "labs/lab05/template.html",
    "title": "Lab 05",
    "section": "",
    "text": "Remember to git clone to your machine, then activate and instantiate the environment, as we have done in previous labs.\n\nusing CSV\nusing DataFrames\nusing Dates\nusing Plots\nusing NCDatasets\nusing Unitful\n\n\n\n\n\nNCDatasets.jl\nDataFramesMeta.jl"
  },
  {
    "objectID": "labs/lab05/template.html#setup",
    "href": "labs/lab05/template.html#setup",
    "title": "Lab 05",
    "section": "",
    "text": "Remember to git clone to your machine, then activate and instantiate the environment, as we have done in previous labs.\n\nusing CSV\nusing DataFrames\nusing Dates\nusing Plots\nusing NCDatasets\nusing Unitful"
  },
  {
    "objectID": "labs/lab05/template.html#resources",
    "href": "labs/lab05/template.html#resources",
    "title": "Lab 05",
    "section": "",
    "text": "NCDatasets.jl\nDataFramesMeta.jl"
  },
  {
    "objectID": "labs/lab07/template.html",
    "href": "labs/lab07/template.html",
    "title": "Lab 07",
    "section": "",
    "text": "An important idea in statistics is the Central Limit Theorem (CLT), which states (much more elegantly and precisely) that the sum of many independent random variables is approximately normally distributed, even if the underlying process is not randomly distributed (as long as it’s moderately well-behaved). This is often used to justify approximating the average of many data points, and the uncertainty in this average, as a normal distribution.\nThe CLT is a powerful tool, but intuition we develop about working with distributions that behave “nicely” can be misleading when we are working with extreme value distributions. The purpose of this lab is to explore this idea and to build some intuition about sampling uncertainty and sample statistics when working with extreme value distributions.\n\n\nRemember to git clone to your machine, then activate and instantiate the environment, as we have done in previous labs. If you are having trouble, you may want to build IJulia in the Pkg manager.\nDo not use additional packages for this assignment, though you are welcome to look at the documentation or implementation of other packages for nearest-neighbors methods.\n\nusing Distributions\nusing Plots\nusing StatsBase\nusing StatsPlots\n\nPlots.default(; margin=4Plots.mm, size=(700, 400), linewidth=2)"
  },
  {
    "objectID": "labs/lab07/template.html#setup",
    "href": "labs/lab07/template.html#setup",
    "title": "Lab 07",
    "section": "",
    "text": "Remember to git clone to your machine, then activate and instantiate the environment, as we have done in previous labs. If you are having trouble, you may want to build IJulia in the Pkg manager.\nDo not use additional packages for this assignment, though you are welcome to look at the documentation or implementation of other packages for nearest-neighbors methods.\n\nusing Distributions\nusing Plots\nusing StatsBase\nusing StatsPlots\n\nPlots.default(; margin=4Plots.mm, size=(700, 400), linewidth=2)"
  },
  {
    "objectID": "labs/lab07/template.html#running-statistics",
    "href": "labs/lab07/template.html#running-statistics",
    "title": "Lab 07",
    "section": "2.1 Running statistics",
    "text": "2.1 Running statistics\nAn important idea in statistics is that we can calculate descriptive statistics from our data. For example, things like the mean, standard deviation, skewness, or 99th percentile. An interesting question to ask is how many observations we need to get a reliable estimate of these “sample statistics”.\nLet’s start by looking at the mean with our 100,000 samples. We could do this with a loop, but it turns out to be inefficient. Instead, we’ll define some more efficient functions\n\ncumul_mean(x) = cumsum(x) ./ (1:length(x))\ncumul_std(x) = sqrt.(cumsum(x .- cum_mean(x)) .^ 2 ./ (1:length(x)))\n\nNow let’s plot the cumulative mean of our samples.\n\nplot(\n    cumul_mean(samples_normal); label=\"Cumulative Mean\", ylabel=\"Mean\", xlabel=\"Sample Size\"\n)\nhline!([mean(dist_normal)]; label=\"True Mean\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that we very rapidly converge to the “true” value. Of course, maybe we just got lucky with our unique sample. We can repeat this experiment many times and see how often we get a good estimate of the mean (note the log \\(x\\) scale!)\n\nplot(; xscale=:log10)\nfor i in 1:10\n    samples_normal = rand(dist_normal, 10_000)\n    plot!(cumul_mean(samples_normal); label=nothing, linewidth=1, color=:gray)\nend\nhline!([mean(dist_normal)]; label=\"True Mean\")"
  },
  {
    "objectID": "labs/lab07/template.html#sample-extrema",
    "href": "labs/lab07/template.html#sample-extrema",
    "title": "Lab 07",
    "section": "2.2 Sample extrema",
    "text": "2.2 Sample extrema\nAnother question we can ask is how much extreme values, like the minimum or maximum of our samples, vary from one random realization to the next.\nWe can answer this question in a very simple experiment. We’ll draw N samples from our distribution, and take the maximum. We’ll repeat this M times, and plot the distribution of the maximums.\n\nN = 100\nM = 1_000\nmaxima = [maximum(rand(dist_normal, N)) for i in 1:M]\nhistogram(\n    maxima;\n    normalize=:pdf,\n    label=\"Maxima\",\n    ylabel=\"Probability Density\",\n    title=\"Normal Distribution with $N Samples\",\n)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see from this that even though the Normal distribution is “well-behaved”, the maximum of a sample of 100 observations can vary enormously from one set of 100 draws to the next. (And of course, we are assuming that we know the true distribution!)"
  },
  {
    "objectID": "labs/lab01/template.html",
    "href": "labs/lab01/template.html",
    "title": "Lab 01",
    "section": "",
    "text": "Please see the slides for more details."
  },
  {
    "objectID": "labs/lab01/template.html#overview",
    "href": "labs/lab01/template.html#overview",
    "title": "Lab 01",
    "section": "",
    "text": "Please see the slides for more details."
  },
  {
    "objectID": "labs/lab01/template.html#instructions",
    "href": "labs/lab01/template.html#instructions",
    "title": "Lab 01",
    "section": "Instructions",
    "text": "Instructions\n\nRead and go through the Software Installation Guide for instructions on setting up your computer for this course.\nFollow the link to lab 1 assignment from Canvas (it should start with classroom.github.com). It may take a few minutes for the site to configure your repository.\nYou will get a message saying ” Your assignment repository has been created: …“. Click on the link to go to your repository.\nclone the repository for lab 01 (use the Github Classroom link from Canvas) to your computer. You can use VS Code functionality, GitHub Desktop, or your terminal.\nOpen the directory containing the repository in VS Code doing one of the following:\n\nFrom GitHub desktop: Repository &gt; Open in Visual Studio Code\nIn VS Code: File &gt; Open Folder...\n\nInstantiate the project environment as follows:\n\nOpen the command palette (Ctrl+Shift+P on Windows/Linux, Cmd+Shift+P on Mac)\nStart typing “Julia: Start REPL”. It will auto-complete; select the command as it appears.\nIn the Julia REPL, type ] to enter the package manager. It should now show something like (lab01) pkg&gt;.\nType instantiate and run it (Enter). This will install all the packages needed for this lab.\nType the backspace key to exit the package manager.\n\nEdit the solutions.qmd file to add your name and netID\nRender the document\n\nOpen the solutions.qmd file\nOpen the command palette and run “Quarto: Render”. After some activity, a preview of the rendered document should open in VS Code. If you see something like Browse at http://localhost:4200/labs/lab01/solution.html you can open that link in your web browser to see the rendered document.\nCheck the box on line 46 or 47 of the solutions.qmd file to indicate that you were able to render the document. If you were unable to render the document, check the other box and seek help. Make sure the box check renders correctly in the preview\n\nIf you’re still having trouble:\n\nTry running Pkg.build(\"IJulia\") in the Julia REPL\n\ncommit and push your changes"
  },
  {
    "objectID": "labs/lab06/template.html",
    "href": "labs/lab06/template.html",
    "title": "Lab 06",
    "section": "",
    "text": "Today we are going to build a simple model to simulate precipitation, conditional on temperature fields.\n\n\nPrecipitation is notoriously difficult to forecast at high spatial and temporal resolution. Temperature, by contrast, is much smoother and often easier to forecast. We might imagine seeking to generate high-resolution precipitation fields from climate models that have substantial biases in representing precipitation.\n\n\n\nFor your reference, here is the full algorithm we will implement today. We’ll break this into bite-sized pieces for you, and then at the end we’ll implement the whole thing as a function in Julia.\n\nInputs\n\nPast temperature fields \\(T_{\\text{past}}\\)\nCurrent temperature field \\(T_{\\text{current}}\\) for which we are making predictions\nNumber of nearest neighbors \\(K\\)\nNumber of principal components \\(n_{\\text{PC}}\\)\n\nOutput\n\nSampled precipitation field \\(P\\)\n\nSteps\n\nPreprocessing:\n\nPreprocess the temperature fields \\(T_{\\text{past}}\\) and \\(T_{\\text{current}}\\) as deemed necessary.\nDocument and explain the preprocessing steps and assumptions made.\n\nDimensionality Reduction using PCA:\n\nApply Principal Component Analysis (PCA) to the past temperature fields \\(T_{\\text{past}}\\).\nRetain \\(n_{\\text{PC}}\\) principal components. The choice of \\(n_{\\text{PC}}\\) should be made and defended by the student.\n\nFind \\(K\\) Nearest Neighbors:\n\nUsing the Euclidean distance metric, find the \\(K\\) nearest neighbors to the projected current temperature field from step 2.\n\nSample Precipitation Field:\n\nFor each of the \\(K\\) nearest neighbors, compute the inverse distance as \\(w_i = \\frac{1}{\\text{distance}_i}\\) where \\(\\text{distance}_i\\) is the distance to the \\(i^{th}\\) neighbor.\n\nThis is sometimes called the “kernel” and there are other valid choices (from simple – like distance squared – to more complex)\n\nNormalize the weights such that they sum to 1: \\(w_i = \\frac{w_i}{\\sum_{j=1}^{K} w_j}\\).\nSample a precipitation field \\(P\\) from the \\(K\\) nearest neighbors with probability proportional to the normalized inverse distances \\(w_i\\).\n\n\n\n\n\n\nWe will use the ERA5 reanalysis dataset, which is a global dataset of atmospheric variables, including temperature and precipitation, for temperature. We use the same precipitation dataset as Lab 05. This data is available to enrolled students on Canvas as precip.nc and temp.nc.\n\n\n\nRemember to git clone to your machine, then activate and instantiate the environment, as we have done in previous labs. Do not use additional packages for this assignment, though you are welcome to look at the documentation or implementation of other packages for nearest-neighbors methods.\n\nusing Dates\nusing MultivariateStats\nusing Plots\nusing NCDatasets\nusing StatsBase\nusing Unitful\n\nPlots.default(; margin=4Plots.mm, size=(700, 400), linewidth=2)"
  },
  {
    "objectID": "labs/lab06/template.html#why-might-this-be-useful",
    "href": "labs/lab06/template.html#why-might-this-be-useful",
    "title": "Lab 06",
    "section": "",
    "text": "Precipitation is notoriously difficult to forecast at high spatial and temporal resolution. Temperature, by contrast, is much smoother and often easier to forecast. We might imagine seeking to generate high-resolution precipitation fields from climate models that have substantial biases in representing precipitation."
  },
  {
    "objectID": "labs/lab06/template.html#algorithm",
    "href": "labs/lab06/template.html#algorithm",
    "title": "Lab 06",
    "section": "",
    "text": "For your reference, here is the full algorithm we will implement today. We’ll break this into bite-sized pieces for you, and then at the end we’ll implement the whole thing as a function in Julia.\n\nInputs\n\nPast temperature fields \\(T_{\\text{past}}\\)\nCurrent temperature field \\(T_{\\text{current}}\\) for which we are making predictions\nNumber of nearest neighbors \\(K\\)\nNumber of principal components \\(n_{\\text{PC}}\\)\n\nOutput\n\nSampled precipitation field \\(P\\)\n\nSteps\n\nPreprocessing:\n\nPreprocess the temperature fields \\(T_{\\text{past}}\\) and \\(T_{\\text{current}}\\) as deemed necessary.\nDocument and explain the preprocessing steps and assumptions made.\n\nDimensionality Reduction using PCA:\n\nApply Principal Component Analysis (PCA) to the past temperature fields \\(T_{\\text{past}}\\).\nRetain \\(n_{\\text{PC}}\\) principal components. The choice of \\(n_{\\text{PC}}\\) should be made and defended by the student.\n\nFind \\(K\\) Nearest Neighbors:\n\nUsing the Euclidean distance metric, find the \\(K\\) nearest neighbors to the projected current temperature field from step 2.\n\nSample Precipitation Field:\n\nFor each of the \\(K\\) nearest neighbors, compute the inverse distance as \\(w_i = \\frac{1}{\\text{distance}_i}\\) where \\(\\text{distance}_i\\) is the distance to the \\(i^{th}\\) neighbor.\n\nThis is sometimes called the “kernel” and there are other valid choices (from simple – like distance squared – to more complex)\n\nNormalize the weights such that they sum to 1: \\(w_i = \\frac{w_i}{\\sum_{j=1}^{K} w_j}\\).\nSample a precipitation field \\(P\\) from the \\(K\\) nearest neighbors with probability proportional to the normalized inverse distances \\(w_i\\)."
  },
  {
    "objectID": "labs/lab06/template.html#data",
    "href": "labs/lab06/template.html#data",
    "title": "Lab 06",
    "section": "",
    "text": "We will use the ERA5 reanalysis dataset, which is a global dataset of atmospheric variables, including temperature and precipitation, for temperature. We use the same precipitation dataset as Lab 05. This data is available to enrolled students on Canvas as precip.nc and temp.nc."
  },
  {
    "objectID": "labs/lab06/template.html#setup",
    "href": "labs/lab06/template.html#setup",
    "title": "Lab 06",
    "section": "",
    "text": "Remember to git clone to your machine, then activate and instantiate the environment, as we have done in previous labs. Do not use additional packages for this assignment, though you are welcome to look at the documentation or implementation of other packages for nearest-neighbors methods.\n\nusing Dates\nusing MultivariateStats\nusing Plots\nusing NCDatasets\nusing StatsBase\nusing Unitful\n\nPlots.default(; margin=4Plots.mm, size=(700, 400), linewidth=2)"
  },
  {
    "objectID": "labs/lab06/template.html#precipitation",
    "href": "labs/lab06/template.html#precipitation",
    "title": "Lab 06",
    "section": "2.1 Precipitation",
    "text": "2.1 Precipitation\nWe start by loading the precipitation data. Create variables called precip_time, precip_lon, precip_lat, and precip. precip should be a 3D array with appropriate units added. Remember to close the file when you have read the data\n\n\n\n\n\n\nHint\n\n\n\nReview the lab 05 solutions posted on Canvas to make sure you have the right syntax. Additionally, if the latitudes are flipped, you need to reverse them (also in lab 5 solutions!)\n\n\nprecip_ds = NCDataset(\"data/precip.nc\")\n# ..."
  },
  {
    "objectID": "labs/lab06/template.html#temperature",
    "href": "labs/lab06/template.html#temperature",
    "title": "Lab 06",
    "section": "2.2 Temperature",
    "text": "2.2 Temperature\nNext we load the temperature data\nCreate variables called temp_time, temp_lon, temp_lat, and temp. temp should have appropriate units.\nMake sure that temp_time and precip_time are the same! If so, you can create a variable called time that is equal to temp_time."
  },
  {
    "objectID": "labs/lab06/template.html#split",
    "href": "labs/lab06/template.html#split",
    "title": "Lab 06",
    "section": "2.3 Split",
    "text": "2.3 Split\nWe will split the data into training and testing sets (we will cover this idea in more detail later). The idea is to use the training set to build the model, and the testing set to evaluate the model.\nWe will use the last 10 years of data as the testing set, and the rest as the training set. Create variables called precip_train, precip_test, temp_train, and temp_test, time_train, and time_test.\n\n2.3.1 Preprocess\nReview the slides on PCA and decide whether you would like to preprocess the temperature data before applying PCA. Explain what you have done and why.\nNext, we need to convert the temperature data to a matrix. You can use the reshape function to do this.1 Call your matrix temp_mat.\nPut all these steps into a function called preprocess that takes in:\n\nthe raw field of temperature data (indexed lon, lat, time)\nthe raw field of temperature data (indexed lon, lat, time) corresponding to the “reference period” over which climatology is returned\nany other inputs you would like (e.g., a vector of times for the reference and target periods)\n\nand returns the matrix of preprocessed temperature data.\nfunction preprocess(temp::Array{T, 3}, temp_reference::Array{T, 3})::AbstractMatrix where T\n    # ...\nend"
  },
  {
    "objectID": "labs/lab06/template.html#fitting",
    "href": "labs/lab06/template.html#fitting",
    "title": "Lab 06",
    "section": "3.1 Fitting",
    "text": "3.1 Fitting\nFollowing the instructions provided in the documentation, fit the PCA model to the training data. Choose how many dimensions you are using and explain why you chose that number. Create any plots to help inform this choice."
  },
  {
    "objectID": "labs/lab06/template.html#visualizing",
    "href": "labs/lab06/template.html#visualizing",
    "title": "Lab 06",
    "section": "3.2 Visualizing",
    "text": "3.2 Visualizing\n\nPlot the spatial patterns associated with the first two principal components. You’ll need to reshape the data back to its original shape.\n\nYou do not need to add map features. The best package for mapping is GeoMakie, which has a bit of a new plotting syntax and is still a little rough around the edges, though improving rapidly and great in many ways.\n\nPlot the time series of the first two principal components. (We have the actual times – called time – so the \\(x\\) axis should show the actual time)\nMake a scatter plot of the first two principal components, where each day is a day. Color the points by the precipitation value at that time.\n\nDefine “the precipitation value” however you like – it may be the precipitation at a particular point or a spatial mean.\nUse the syntax scatter(x, y, zcolor=z) to plot the points, where x and y are the first two principal components and z is the precipitation value."
  },
  {
    "objectID": "labs/lab06/template.html#footnotes",
    "href": "labs/lab06/template.html#footnotes",
    "title": "Lab 06",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAccess documentation by typing ?reshape in the REPL.↩︎"
  },
  {
    "objectID": "resources/github.html",
    "href": "resources/github.html",
    "title": "GitHub Resources",
    "section": "",
    "text": "Git Basics from The Odin Project.\nLearn Git Branching: An interactive, visual tutorial to how git works.\nVersion Control from MIT’s “CS: Your Missing Semester” course.\nGit and GitHub for Poets: YouTube playlist covering the basics of git and GitHub."
  },
  {
    "objectID": "resources/github.html#helpful-resources",
    "href": "resources/github.html#helpful-resources",
    "title": "GitHub Resources",
    "section": "",
    "text": "Git Basics from The Odin Project.\nLearn Git Branching: An interactive, visual tutorial to how git works.\nVersion Control from MIT’s “CS: Your Missing Semester” course.\nGit and GitHub for Poets: YouTube playlist covering the basics of git and GitHub."
  },
  {
    "objectID": "resources/quarto.html",
    "href": "resources/quarto.html",
    "title": "Quarto",
    "section": "",
    "text": "In this tutorial, you will learn how to typeset mathematics and equations in Jupyter notebooks using LaTeX."
  },
  {
    "objectID": "resources/quarto.html#overview",
    "href": "resources/quarto.html#overview",
    "title": "Quarto",
    "section": "",
    "text": "In this tutorial, you will learn how to typeset mathematics and equations in Jupyter notebooks using LaTeX."
  },
  {
    "objectID": "resources/quarto.html#further-resourcesw",
    "href": "resources/quarto.html#further-resourcesw",
    "title": "Quarto",
    "section": "Further Resourcesw",
    "text": "Further Resourcesw\n\nMarkdown Cheatsheet\nLaTeX Cheatsheet\nMathpix Snpi allows you to convert images of equations to LaTeX code (there is a free tier)\nDetexify lets you draw a symbol and suggests the LaTeX code for the corresponding symbol\n\nThis tutorial was inspired and draws from Justin Bois’ tutorial."
  },
  {
    "objectID": "resources/quarto.html#inline-mathematics",
    "href": "resources/quarto.html#inline-mathematics",
    "title": "Quarto",
    "section": "Inline Mathematics",
    "text": "Inline Mathematics\nTo include mathematical notation within text, enclose the LaTeX within dollar signs $. For example, to obtain the output\n\nthe objective function is \\(4x + 7x\\),\n\nyou would enter\n\nthe objective function is $4x + 7x$.\n\nYou can enter subscripts and superscripts with _ and ^, respectively; to get\n\nthe function is \\(f(x_i) = x_i^2\\),\n\ntype\n\nthe function is $f(x_i) = x_i^2$.\n\nIf you want multiple characters to be enclosed in a subscript or superscript, enclose them in braces {}:\n\n\\(e^{i \\pi} - 1 = 0\\) is produced by\n\n\n$e^{i \\pi} - 1 = 0$.\n\nTo get special characters like \\(\\pi\\) (or other Greek letters), precede their name (or sometimes a code) with a backslash: $\\pi$. There are a number of special characters like this, which you can find in cheatsheets like this one.\nBold characters, which you might use to denote vectors, can be rendered using \\mathbf:\n\n\\(\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^n a_i \\times b_i\\)\n\n\n$\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^n a_i \\times b_i$\n\nFractions can be displayed using \\frac{}{}, where the first bracket encloses the numerator and the second the denominator, as in\n\n\\(\\frac{1}{2}\\)\n\n\n$\\frac{1}{2}$"
  },
  {
    "objectID": "resources/quarto.html#displaying-equations",
    "href": "resources/quarto.html#displaying-equations",
    "title": "Quarto",
    "section": "Displaying Equations",
    "text": "Displaying Equations\nTo place equations or other mathematics on their own line(s), enclose the entire block in two dollar signs $$. For example, the prior dot-product definition could be displayed as \\[\n\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^n a_i \\times b_i\n\\] using\n$$\n\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^n a_i \\times b_i\n$$\nDisplaying equations on their own line(s) can improve the spacing of symbols like sums (as above) or fractions: compare the inline\n\n\\(x &lt; \\frac{1}{2}\\)\n\nto \\[\nx &lt; \\frac{1}{2}.\n\\]\nTo display multiple related lines in a single block, there are two environments of note. The first will center all of the equations, and is obtained by enclosing the equations in \\begin{gather} and \\end{gather}. Each line should be separated with \\\\:\n\\[\n\\begin{gather}\nx_1 + x_2 \\leq 5 \\\\\ny \\leq \\frac{1}{2}.\n\\end{gather}\n\\]\n$$\n\\begin{gather}\nx_1 + x_2 \\leq 5 \\\\\ny \\leq \\frac{1}{2}.\n\\end{gather}\n$$\nThe second environment will let you align the equations as you wish instead of automatically centering them, and is used by enclosing the equations with \\begin{align} and \\end{align}, with an ampersand & in front of the characters which will be used on each line to align the equations:\n\\[\n\\begin{align}\nx_1 + x_2 &\\leq 5 \\\\\ny &\\leq \\frac{1}{2}.\n\\end{align}\n\\]\n$$\n\\begin{align}\nx_1 + x_2 &\\leq 5 \\\\\ny &\\leq \\frac{1}{2}.\n\\end{align}\n$$"
  },
  {
    "objectID": "resources/quarto.html#sizing-parentheses-or-brackets",
    "href": "resources/quarto.html#sizing-parentheses-or-brackets",
    "title": "Quarto",
    "section": "Sizing Parentheses or Brackets",
    "text": "Sizing Parentheses or Brackets\nBy default, parentheses and brackets are sized for simple characters, but will look bad when used to surround fractions or sums, particularly when they are not used in-line: \\[\nx_n = (\\frac{1}{2})^n.\n\\] To make this look better, use \\left and \\right around the left and right parentheses or brackets: \\[\nx_n = \\left(\\frac{1}{2}\\right)^n\n\\]\n$$\nx_n = \\left(\\frac{1}{2}\\right)^n.\n$$\nThis is totally optional, but helps!"
  },
  {
    "objectID": "resources/quarto.html#using-latex-in-figures",
    "href": "resources/quarto.html#using-latex-in-figures",
    "title": "Quarto",
    "section": "Using LaTeX in Figures",
    "text": "Using LaTeX in Figures\nYou may want to use LaTeX in figures, for example if your \\(x\\)-axis should have a title like \\(x\\). To do this, load the LaTeXStrings package and precede the relevant LaTeX-formatted string (within in-line dollar signs $) with L, as in:\n\nusing Plots\nusing LaTeXStrings\n\nx = (-2π):0.01:(2π)\nplot(x, exp.(sin.(x)); xlabel=L\"$x$\", ylabel=L\"$e^{\\sin(x)}$\", legend=false)"
  },
  {
    "objectID": "resources/llm.html",
    "href": "resources/llm.html",
    "title": "Tutorial: Responsible use of large language models",
    "section": "",
    "text": "Large language models (LLMs), like GPT, are powerful tools for generating text that can be used for coding and doing data analysis. This is at once empowering (LLMs are powerful and can save you time) and risky (LLMs can make mistakes that are hard to detect).\nOur general view is that LLMs are powerful tools that you will encounter and use when you leave this classroom, so it’s important to learn how to use them responsibly and effectively. As described in the syllabus, you are generally permitted to use LLMs in this course, but ultimately, you are responsible for guaranteeing, understanding, and interpreting your results, and you can’t do this if you don’t understand the code that you are running (this isn’t exclusive to code generated by LLMs – it also applies to code that you copy from the internet!).\nThe use of LLMs for coding is a new and rapidly evolving area. Rather than provide a lesson plan for you, this page will provide some resources for self-learning."
  },
  {
    "objectID": "resources/llm.html#links-and-resources",
    "href": "resources/llm.html#links-and-resources",
    "title": "Tutorial: Responsible use of large language models",
    "section": "Links and resources",
    "text": "Links and resources\n\nGitHub Copilot is an extension for VS Code that can provide suggestions for code completion and editing. It is free for students and educators.\nBlog: “Bob Carpenter thinks GPT-4 is awesome”: this post highlights how GPT-4 is able to write a program in Stan, a statistical programming language, and also the mistakes that it makes. Finding and correcting these mistakes requires knowing the Stan language and having a deep understanding of the statistical model, but someone with this expertise could potentially use GPT-4 to accelerate their coding workflow. The comments are also interesting and insightful.\nAI Snake Oil is a blog that seeks to dispel hype, remove misconceptions, and clarify the limits of AI. The authors are in the Princeton University Department of Computer Science.\nChatGPT has both free and paid tiers and can be helpful with writing and interpreting code, though care is needed as described above"
  },
  {
    "objectID": "slides/2023-11-29-review.html#tbd",
    "href": "slides/2023-11-29-review.html#tbd",
    "title": "Module III Review",
    "section": "TBD",
    "text": "TBD\nThis is a placeholder for a planned lecture. The schedule will be modified as needed."
  },
  {
    "objectID": "slides/2023-11-15-model-selection.html#motivation",
    "href": "slides/2023-11-15-model-selection.html#motivation",
    "title": "Quantitative and Graphical Model Selection",
    "section": "Motivation",
    "text": "Motivation\nWe have to make choices about which distribution to use, which covariates (if any) to use for nonstationarity, which (if any) parameters to model as nonstationary, how to pool information across space, etc. There is no single “right” answer; how can we proceed in a principled way?\n\nThis is a general problem in statistics beyond extreme value analysis."
  },
  {
    "objectID": "slides/2023-11-15-model-selection.html#data-and-fit",
    "href": "slides/2023-11-15-model-selection.html#data-and-fit",
    "title": "Quantitative and Graphical Model Selection",
    "section": "Data and fit",
    "text": "Data and fit\n\n\nCode\nannmax_precip = CSV.read(\"data/dur01d_ams_na14v11_houston.csv\", DataFrame)\nhobby = annmax_precip[annmax_precip.name .== \"HOUSTON HOBBY AP\", :]\nhobby_bayes = gevfitbayes(hobby, :precip_in)\nplot(\n    hobby.year,\n    hobby.precip_in;\n    xlabel=\"Year\",\n    ylabel=\"Annual Max Precip (in)\",\n    label=\"HOUSTON HOBBY AP\",\n    marker=:circ,\n)"
  },
  {
    "objectID": "slides/2023-11-15-model-selection.html#plot-the-distribution",
    "href": "slides/2023-11-15-model-selection.html#plot-the-distribution",
    "title": "Quantitative and Graphical Model Selection",
    "section": "Plot the distribution",
    "text": "Plot the distribution\n\nWhat we plot: histogram of the data and the probability density function\nIdeal case: the histogram and the PDF appear to show the same distribution\nWarnings: systematic deviations\nLimitations: hard to learn much about the tails of the distribution from this plot"
  },
  {
    "objectID": "slides/2023-11-15-model-selection.html#probability-plot",
    "href": "slides/2023-11-15-model-selection.html#probability-plot",
    "title": "Quantitative and Graphical Model Selection",
    "section": "Probability plot",
    "text": "Probability plot\n\nWhat we plot: empirical CDF (1 - AEP) of against the fitted GEV’s CDF\nIdeal case: a straight line, indicating perfect agreement between the empirical CDF ann the fitted CDF\nWarnings: curvature or systematic deviations from the line, especially in the tails\nLimitations: sampling uncertainty!"
  },
  {
    "objectID": "slides/2023-11-15-model-selection.html#qq-plot",
    "href": "slides/2023-11-15-model-selection.html#qq-plot",
    "title": "Quantitative and Graphical Model Selection",
    "section": "QQ plot",
    "text": "QQ plot\n\nWhat we plot: quantiles (i.e., return levels) of the data against quantiles of the fitted GEV\nIdeal case: a straight line through the data\nWarnings: curvature or systematic deviations from the line, especially in the tails\nLimitations: sampling uncertainty!"
  },
  {
    "objectID": "slides/2023-11-15-model-selection.html#with-uncertainty",
    "href": "slides/2023-11-15-model-selection.html#with-uncertainty",
    "title": "Quantitative and Graphical Model Selection",
    "section": "With Uncertainty",
    "text": "With Uncertainty\n\nExtremes.qqplotci(hobby_bayes)\n\n\n\n\n\n\n  \n    \n  \n\n\n  \n    \n      \n        Model\n      \n    \n  \n  \n    \n      \n        0\n      \n    \n    \n      \n        5\n      \n    \n    \n      \n        10\n      \n    \n    \n      \n        15\n      \n    \n    \n      \n        20\n      \n    \n    \n      \n        0\n      \n    \n    \n      \n        1\n      \n    \n    \n      \n        2\n      \n    \n    \n      \n        3\n      \n    \n    \n      \n        4\n      \n    \n    \n      \n        5\n      \n    \n    \n      \n        6\n      \n    \n    \n      \n        7\n      \n    \n    \n      \n        8\n      \n    \n    \n      \n        9\n      \n    \n    \n      \n        10\n      \n    \n    \n      \n        11\n      \n    \n    \n      \n        12\n      \n    \n    \n      \n        13\n      \n    \n    \n      \n        14\n      \n    \n    \n      \n        15\n      \n    \n    \n      \n        16\n      \n    \n    \n      \n        17\n      \n    \n    \n      \n        18\n      \n    \n    \n      \n        19\n      \n    \n    \n      \n        20\n      \n    \n    \n      \n        0.0\n      \n    \n    \n      \n        0.1\n      \n    \n    \n      \n        0.2\n      \n    \n    \n      \n        0.3\n      \n    \n    \n      \n        0.4\n      \n    \n    \n      \n        0.5\n      \n    \n    \n      \n        0.6\n      \n    \n    \n      \n        0.7\n      \n    \n    \n      \n        0.8\n      \n    \n    \n      \n        0.9\n      \n    \n    \n      \n        1.0\n      \n    \n    \n      \n        1.1\n      \n    \n    \n      \n        1.2\n      \n    \n    \n      \n        1.3\n      \n    \n    \n      \n        1.4\n      \n    \n    \n      \n        1.5\n      \n    \n    \n      \n        1.6\n      \n    \n    \n      \n        1.7\n      \n    \n    \n      \n        1.8\n      \n    \n    \n      \n        1.9\n      \n    \n    \n      \n        2.0\n      \n    \n    \n      \n        2.1\n      \n    \n    \n      \n        2.2\n      \n    \n    \n      \n        2.3\n      \n    \n    \n      \n        2.4\n      \n    \n    \n      \n        2.5\n      \n    \n    \n      \n        2.6\n      \n    \n    \n      \n        2.7\n      \n    \n    \n      \n        2.8\n      \n    \n    \n      \n        2.9\n      \n    \n    \n      \n        3.0\n      \n    \n    \n      \n        3.1\n      \n    \n    \n      \n        3.2\n      \n    \n    \n      \n        3.3\n      \n    \n    \n      \n        3.4\n      \n    \n    \n      \n        3.5\n      \n    \n    \n      \n        3.6\n      \n    \n    \n      \n        3.7\n      \n    \n    \n      \n        3.8\n      \n    \n    \n      \n        3.9\n      \n    \n    \n      \n        4.0\n      \n    \n    \n      \n        4.1\n      \n    \n    \n      \n        4.2\n      \n    \n    \n      \n        4.3\n      \n    \n    \n      \n        4.4\n      \n    \n    \n      \n        4.5\n      \n    \n    \n      \n        4.6\n      \n    \n    \n      \n        4.7\n      \n    \n    \n      \n        4.8\n      \n    \n    \n      \n        4.9\n      \n    \n    \n      \n        5.0\n      \n    \n    \n      \n        5.1\n      \n    \n    \n      \n        5.2\n      \n    \n    \n      \n        5.3\n      \n    \n    \n      \n        5.4\n      \n    \n    \n      \n        5.5\n      \n    \n    \n      \n        5.6\n      \n    \n    \n      \n        5.7\n      \n    \n    \n      \n        5.8\n      \n    \n    \n      \n        5.9\n      \n    \n    \n      \n        6.0\n      \n    \n    \n      \n        6.1\n      \n    \n    \n      \n        6.2\n      \n    \n    \n      \n        6.3\n      \n    \n    \n      \n        6.4\n      \n    \n    \n      \n        6.5\n      \n    \n    \n      \n        6.6\n      \n    \n    \n      \n        6.7\n      \n    \n    \n      \n        6.8\n      \n    \n    \n      \n        6.9\n      \n    \n    \n      \n        7.0\n      \n    \n    \n      \n        7.1\n      \n    \n    \n      \n        7.2\n      \n    \n    \n      \n        7.3\n      \n    \n    \n      \n        7.4\n      \n    \n    \n      \n        7.5\n      \n    \n    \n      \n        7.6\n      \n    \n    \n      \n        7.7\n      \n    \n    \n      \n        7.8\n      \n    \n    \n      \n        7.9\n      \n    \n    \n      \n        8.0\n      \n    \n    \n      \n        8.1\n      \n    \n    \n      \n        8.2\n      \n    \n    \n      \n        8.3\n      \n    \n    \n      \n        8.4\n      \n    \n    \n      \n        8.5\n      \n    \n    \n      \n        8.6\n      \n    \n    \n      \n        8.7\n      \n    \n    \n      \n        8.8\n      \n    \n    \n      \n        8.9\n      \n    \n    \n      \n        9.0\n      \n    \n    \n      \n        9.1\n      \n    \n    \n      \n        9.2\n      \n    \n    \n      \n        9.3\n      \n    \n    \n      \n        9.4\n      \n    \n    \n      \n        9.5\n      \n    \n    \n      \n        9.6\n      \n    \n    \n      \n        9.7\n      \n    \n    \n      \n        9.8\n      \n    \n    \n      \n        9.9\n      \n    \n    \n      \n        10.0\n      \n    \n    \n      \n        10.1\n      \n    \n    \n      \n        10.2\n      \n    \n    \n      \n        10.3\n      \n    \n    \n      \n        10.4\n      \n    \n    \n      \n        10.5\n      \n    \n    \n      \n        10.6\n      \n    \n    \n      \n        10.7\n      \n    \n    \n      \n        10.8\n      \n    \n    \n      \n        10.9\n      \n    \n    \n      \n        11.0\n      \n    \n    \n      \n        11.1\n      \n    \n    \n      \n        11.2\n      \n    \n    \n      \n        11.3\n      \n    \n    \n      \n        11.4\n      \n    \n    \n      \n        11.5\n      \n    \n    \n      \n        11.6\n      \n    \n    \n      \n        11.7\n      \n    \n    \n      \n        11.8\n      \n    \n    \n      \n        11.9\n      \n    \n    \n      \n        12.0\n      \n    \n    \n      \n        12.1\n      \n    \n    \n      \n        12.2\n      \n    \n    \n      \n        12.3\n      \n    \n    \n      \n        12.4\n      \n    \n    \n      \n        12.5\n      \n    \n    \n      \n        12.6\n      \n    \n    \n      \n        12.7\n      \n    \n    \n      \n        12.8\n      \n    \n    \n      \n        12.9\n      \n    \n    \n      \n        13.0\n      \n    \n    \n      \n        13.1\n      \n    \n    \n      \n        13.2\n      \n    \n    \n      \n        13.3\n      \n    \n    \n      \n        13.4\n      \n    \n    \n      \n        13.5\n      \n    \n    \n      \n        13.6\n      \n    \n    \n      \n        13.7\n      \n    \n    \n      \n        13.8\n      \n    \n    \n      \n        13.9\n      \n    \n    \n      \n        14.0\n      \n    \n    \n      \n        14.1\n      \n    \n    \n      \n        14.2\n      \n    \n    \n      \n        14.3\n      \n    \n    \n      \n        14.4\n      \n    \n    \n      \n        14.5\n      \n    \n    \n      \n        14.6\n      \n    \n    \n      \n        14.7\n      \n    \n    \n      \n        14.8\n      \n    \n    \n      \n        14.9\n      \n    \n    \n      \n        15.0\n      \n    \n    \n      \n        15.1\n      \n    \n    \n      \n        15.2\n      \n    \n    \n      \n        15.3\n      \n    \n    \n      \n        15.4\n      \n    \n    \n      \n        15.5\n      \n    \n    \n      \n        15.6\n      \n    \n    \n      \n        15.7\n      \n    \n    \n      \n        15.8\n      \n    \n    \n      \n        15.9\n      \n    \n    \n      \n        16.0\n      \n    \n    \n      \n        16.1\n      \n    \n    \n      \n        16.2\n      \n    \n    \n      \n        16.3\n      \n    \n    \n      \n        16.4\n      \n    \n    \n      \n        16.5\n      \n    \n    \n      \n        16.6\n      \n    \n    \n      \n        16.7\n      \n    \n    \n      \n        16.8\n      \n    \n    \n      \n        16.9\n      \n    \n    \n      \n        17.0\n      \n    \n    \n      \n        17.1\n      \n    \n    \n      \n        17.2\n      \n    \n    \n      \n        17.3\n      \n    \n    \n      \n        17.4\n      \n    \n    \n      \n        17.5\n      \n    \n    \n      \n        17.6\n      \n    \n    \n      \n        17.7\n      \n    \n    \n      \n        17.8\n      \n    \n    \n      \n        17.9\n      \n    \n    \n      \n        18.0\n      \n    \n    \n      \n        18.1\n      \n    \n    \n      \n        18.2\n      \n    \n    \n      \n        18.3\n      \n    \n    \n      \n        18.4\n      \n    \n    \n      \n        18.5\n      \n    \n    \n      \n        18.6\n      \n    \n    \n      \n        18.7\n      \n    \n    \n      \n        18.8\n      \n    \n    \n      \n        18.9\n      \n    \n    \n      \n        19.0\n      \n    \n    \n      \n        19.1\n      \n    \n    \n      \n        19.2\n      \n    \n    \n      \n        19.3\n      \n    \n    \n      \n        19.4\n      \n    \n    \n      \n        19.5\n      \n    \n    \n      \n        19.6\n      \n    \n    \n      \n        19.7\n      \n    \n    \n      \n        19.8\n      \n    \n    \n      \n        19.9\n      \n    \n    \n      \n        20.0\n      \n    \n    \n      \n        0\n      \n    \n    \n      \n        20\n      \n    \n  \n  \n    \n      \n        \n          \n        \n      \n      \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n      \n      \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n      \n      \n        \n          \n          \n        \n        \n          \n            \n              \n            \n          \n        \n        \n          \n            \n              \n            \n          \n        \n        \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  16.15348766209949716.86\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  12.96165458145544314.63\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  11.37458643221427411.12\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  10.3546891751633469.95\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  9.6177831033539829.48\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  9.0478410273003728.83\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  8.5868919930657078.73\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  8.2021092323487138.58\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  7.873234162100728.05\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  7.5869509266993247.86\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  7.3340739926804337.07\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  7.1080200157450917.02\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  6.9039218312657696.79\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  6.7180872456744256.57\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  6.5476539626639046.5\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  6.3903615203210796.35\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  6.2443958885145246.3\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  6.1082807556092726.2\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  5.9807997159714145.87\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  5.8609394432174745.75\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  5.7478474431558345.69\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  5.6408001423035825.63\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  5.539178436665345.63\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  5.44244871342340455.62\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  5.3501479469268395.42\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  5.2618718684761315.18\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  5.1772654834576245.02\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  5.0960154011408014.97\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  5.0178435786414094.86\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4.9425021786114784.85\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4.8697693117175174.74\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4.7994454877155564.6\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4.731350638268374.6\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4.66532160428186154.58\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4.6012100030685914.53\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4.5388804079277144.45\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4.4782087860935364.35\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4.4190811514153534.28\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4.3613923962986984.26\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4.3050452738861914.25\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4.2499495065766074.22\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4.1960210010671784.19\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4.1431811533792714.18\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4.0913562299607164.18\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4.0404768130788764.17\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.99047730042627664.1\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.9412954502317783.98\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.89287196426291843.88\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.84515010196351973.85\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.79807531962741153.73\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.7515949289871733.68\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.7056577699107223.68\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.66021389205492963.62\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.61521424032316263.6\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.57061033880314233.5\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.52635396750366863.5\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.48239682563237673.45\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.438690174315923.37\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.39518445049225553.3\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.35182884210833663.26\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.30857081260299963.22\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.2653555597569283.08\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.22212539008204723.07\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.1788189846176113.05\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.13537052473941553.04\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.09170863654505233.03\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.04775509831026662.99\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.0034232355229662.9\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2.95861589913922932.85\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2.91322288027198262.85\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2.86711755086619572.8\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2.82015242223872332.75\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2.77215315963188052.73\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2.7229103419263632.7\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2.67216783890205762.67\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2.61960595410762582.65\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2.56481616604423172.63\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2.5072617848363082.6\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2.44621372266964962.56\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2.3806393371276112.53\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2.30899515379674462.52\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2.22879996205890542.32\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2.1356233402838332.3\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2.020110381463181.88\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1.85493215739168221.66\n                \n              \n            \n          \n        \n      \n      \n        \n          \n            \n              \n            \n          \n        \n      \n      \n        \n          \n            \n              \n            \n          \n          \n            \n              \n                h,j,k,l,arrows,drag to pan\n              \n            \n            \n              \n                i,o,+,-,scroll,shift-drag to zoom\n              \n            \n            \n              \n                r,dbl-click to reset\n              \n            \n            \n              \n                c for coordinates\n              \n            \n            \n              \n                ? for help\n              \n            \n          \n        \n      \n      \n        \n          \n            \n              ?\n            \n          \n        \n      \n    \n  \n  \n    \n      \n        0\n      \n    \n    \n      \n        5\n      \n    \n    \n      \n        10\n      \n    \n    \n      \n        15\n      \n    \n    \n      \n        20\n      \n    \n    \n      \n        25\n      \n    \n    \n      \n        0\n      \n    \n    \n      \n        1\n      \n    \n    \n      \n        2\n      \n    \n    \n      \n        3\n      \n    \n    \n      \n        4\n      \n    \n    \n      \n        5\n      \n    \n    \n      \n        6\n      \n    \n    \n      \n        7\n      \n    \n    \n      \n        8\n      \n    \n    \n      \n        9\n      \n    \n    \n      \n        10\n      \n    \n    \n      \n        11\n      \n    \n    \n      \n        12\n      \n    \n    \n      \n        13\n      \n    \n    \n      \n        14\n      \n    \n    \n      \n        15\n      \n    \n    \n      \n        16\n      \n    \n    \n      \n        17\n      \n    \n    \n      \n        18\n      \n    \n    \n      \n        19\n      \n    \n    \n      \n        20\n      \n    \n    \n      \n        21\n      \n    \n    \n      \n        22\n      \n    \n    \n      \n        23\n      \n    \n    \n      \n        24\n      \n    \n    \n      \n        25\n      \n    \n    \n      \n        0.0\n      \n    \n    \n      \n        0.1\n      \n    \n    \n      \n        0.2\n      \n    \n    \n      \n        0.3\n      \n    \n    \n      \n        0.4\n      \n    \n    \n      \n        0.5\n      \n    \n    \n      \n        0.6\n      \n    \n    \n      \n        0.7\n      \n    \n    \n      \n        0.8\n      \n    \n    \n      \n        0.9\n      \n    \n    \n      \n        1.0\n      \n    \n    \n      \n        1.1\n      \n    \n    \n      \n        1.2\n      \n    \n    \n      \n        1.3\n      \n    \n    \n      \n        1.4\n      \n    \n    \n      \n        1.5\n      \n    \n    \n      \n        1.6\n      \n    \n    \n      \n        1.7\n      \n    \n    \n      \n        1.8\n      \n    \n    \n      \n        1.9\n      \n    \n    \n      \n        2.0\n      \n    \n    \n      \n        2.1\n      \n    \n    \n      \n        2.2\n      \n    \n    \n      \n        2.3\n      \n    \n    \n      \n        2.4\n      \n    \n    \n      \n        2.5\n      \n    \n    \n      \n        2.6\n      \n    \n    \n      \n        2.7\n      \n    \n    \n      \n        2.8\n      \n    \n    \n      \n        2.9\n      \n    \n    \n      \n        3.0\n      \n    \n    \n      \n        3.1\n      \n    \n    \n      \n        3.2\n      \n    \n    \n      \n        3.3\n      \n    \n    \n      \n        3.4\n      \n    \n    \n      \n        3.5\n      \n    \n    \n      \n        3.6\n      \n    \n    \n      \n        3.7\n      \n    \n    \n      \n        3.8\n      \n    \n    \n      \n        3.9\n      \n    \n    \n      \n        4.0\n      \n    \n    \n      \n        4.1\n      \n    \n    \n      \n        4.2\n      \n    \n    \n      \n        4.3\n      \n    \n    \n      \n        4.4\n      \n    \n    \n      \n        4.5\n      \n    \n    \n      \n        4.6\n      \n    \n    \n      \n        4.7\n      \n    \n    \n      \n        4.8\n      \n    \n    \n      \n        4.9\n      \n    \n    \n      \n        5.0\n      \n    \n    \n      \n        5.1\n      \n    \n    \n      \n        5.2\n      \n    \n    \n      \n        5.3\n      \n    \n    \n      \n        5.4\n      \n    \n    \n      \n        5.5\n      \n    \n    \n      \n        5.6\n      \n    \n    \n      \n        5.7\n      \n    \n    \n      \n        5.8\n      \n    \n    \n      \n        5.9\n      \n    \n    \n      \n        6.0\n      \n    \n    \n      \n        6.1\n      \n    \n    \n      \n        6.2\n      \n    \n    \n      \n        6.3\n      \n    \n    \n      \n        6.4\n      \n    \n    \n      \n        6.5\n      \n    \n    \n      \n        6.6\n      \n    \n    \n      \n        6.7\n      \n    \n    \n      \n        6.8\n      \n    \n    \n      \n        6.9\n      \n    \n    \n      \n        7.0\n      \n    \n    \n      \n        7.1\n      \n    \n    \n      \n        7.2\n      \n    \n    \n      \n        7.3\n      \n    \n    \n      \n        7.4\n      \n    \n    \n      \n        7.5\n      \n    \n    \n      \n        7.6\n      \n    \n    \n      \n        7.7\n      \n    \n    \n      \n        7.8\n      \n    \n    \n      \n        7.9\n      \n    \n    \n      \n        8.0\n      \n    \n    \n      \n        8.1\n      \n    \n    \n      \n        8.2\n      \n    \n    \n      \n        8.3\n      \n    \n    \n      \n        8.4\n      \n    \n    \n      \n        8.5\n      \n    \n    \n      \n        8.6\n      \n    \n    \n      \n        8.7\n      \n    \n    \n      \n        8.8\n      \n    \n    \n      \n        8.9\n      \n    \n    \n      \n        9.0\n      \n    \n    \n      \n        9.1\n      \n    \n    \n      \n        9.2\n      \n    \n    \n      \n        9.3\n      \n    \n    \n      \n        9.4\n      \n    \n    \n      \n        9.5\n      \n    \n    \n      \n        9.6\n      \n    \n    \n      \n        9.7\n      \n    \n    \n      \n        9.8\n      \n    \n    \n      \n        9.9\n      \n    \n    \n      \n        10.0\n      \n    \n    \n      \n        10.1\n      \n    \n    \n      \n        10.2\n      \n    \n    \n      \n        10.3\n      \n    \n    \n      \n        10.4\n      \n    \n    \n      \n        10.5\n      \n    \n    \n      \n        10.6\n      \n    \n    \n      \n        10.7\n      \n    \n    \n      \n        10.8\n      \n    \n    \n      \n        10.9\n      \n    \n    \n      \n        11.0\n      \n    \n    \n      \n        11.1\n      \n    \n    \n      \n        11.2\n      \n    \n    \n      \n        11.3\n      \n    \n    \n      \n        11.4\n      \n    \n    \n      \n        11.5\n      \n    \n    \n      \n        11.6\n      \n    \n    \n      \n        11.7\n      \n    \n    \n      \n        11.8\n      \n    \n    \n      \n        11.9\n      \n    \n    \n      \n        12.0\n      \n    \n    \n      \n        12.1\n      \n    \n    \n      \n        12.2\n      \n    \n    \n      \n        12.3\n      \n    \n    \n      \n        12.4\n      \n    \n    \n      \n        12.5\n      \n    \n    \n      \n        12.6\n      \n    \n    \n      \n        12.7\n      \n    \n    \n      \n        12.8\n      \n    \n    \n      \n        12.9\n      \n    \n    \n      \n        13.0\n      \n    \n    \n      \n        13.1\n      \n    \n    \n      \n        13.2\n      \n    \n    \n      \n        13.3\n      \n    \n    \n      \n        13.4\n      \n    \n    \n      \n        13.5\n      \n    \n    \n      \n        13.6\n      \n    \n    \n      \n        13.7\n      \n    \n    \n      \n        13.8\n      \n    \n    \n      \n        13.9\n      \n    \n    \n      \n        14.0\n      \n    \n    \n      \n        14.1\n      \n    \n    \n      \n        14.2\n      \n    \n    \n      \n        14.3\n      \n    \n    \n      \n        14.4\n      \n    \n    \n      \n        14.5\n      \n    \n    \n      \n        14.6\n      \n    \n    \n      \n        14.7\n      \n    \n    \n      \n        14.8\n      \n    \n    \n      \n        14.9\n      \n    \n    \n      \n        15.0\n      \n    \n    \n      \n        15.1\n      \n    \n    \n      \n        15.2\n      \n    \n    \n      \n        15.3\n      \n    \n    \n      \n        15.4\n      \n    \n    \n      \n        15.5\n      \n    \n    \n      \n        15.6\n      \n    \n    \n      \n        15.7\n      \n    \n    \n      \n        15.8\n      \n    \n    \n      \n        15.9\n      \n    \n    \n      \n        16.0\n      \n    \n    \n      \n        16.1\n      \n    \n    \n      \n        16.2\n      \n    \n    \n      \n        16.3\n      \n    \n    \n      \n        16.4\n      \n    \n    \n      \n        16.5\n      \n    \n    \n      \n        16.6\n      \n    \n    \n      \n        16.7\n      \n    \n    \n      \n        16.8\n      \n    \n    \n      \n        16.9\n      \n    \n    \n      \n        17.0\n      \n    \n    \n      \n        17.1\n      \n    \n    \n      \n        17.2\n      \n    \n    \n      \n        17.3\n      \n    \n    \n      \n        17.4\n      \n    \n    \n      \n        17.5\n      \n    \n    \n      \n        17.6\n      \n    \n    \n      \n        17.7\n      \n    \n    \n      \n        17.8\n      \n    \n    \n      \n        17.9\n      \n    \n    \n      \n        18.0\n      \n    \n    \n      \n        18.1\n      \n    \n    \n      \n        18.2\n      \n    \n    \n      \n        18.3\n      \n    \n    \n      \n        18.4\n      \n    \n    \n      \n        18.5\n      \n    \n    \n      \n        18.6\n      \n    \n    \n      \n        18.7\n      \n    \n    \n      \n        18.8\n      \n    \n    \n      \n        18.9\n      \n    \n    \n      \n        19.0\n      \n    \n    \n      \n        19.1\n      \n    \n    \n      \n        19.2\n      \n    \n    \n      \n        19.3\n      \n    \n    \n      \n        19.4\n      \n    \n    \n      \n        19.5\n      \n    \n    \n      \n        19.6\n      \n    \n    \n      \n        19.7\n      \n    \n    \n      \n        19.8\n      \n    \n    \n      \n        19.9\n      \n    \n    \n      \n        20.0\n      \n    \n    \n      \n        20.1\n      \n    \n    \n      \n        20.2\n      \n    \n    \n      \n        20.3\n      \n    \n    \n      \n        20.4\n      \n    \n    \n      \n        20.5\n      \n    \n    \n      \n        20.6\n      \n    \n    \n      \n        20.7\n      \n    \n    \n      \n        20.8\n      \n    \n    \n      \n        20.9\n      \n    \n    \n      \n        21.0\n      \n    \n    \n      \n        21.1\n      \n    \n    \n      \n        21.2\n      \n    \n    \n      \n        21.3\n      \n    \n    \n      \n        21.4\n      \n    \n    \n      \n        21.5\n      \n    \n    \n      \n        21.6\n      \n    \n    \n      \n        21.7\n      \n    \n    \n      \n        21.8\n      \n    \n    \n      \n        21.9\n      \n    \n    \n      \n        22.0\n      \n    \n    \n      \n        22.1\n      \n    \n    \n      \n        22.2\n      \n    \n    \n      \n        22.3\n      \n    \n    \n      \n        22.4\n      \n    \n    \n      \n        22.5\n      \n    \n    \n      \n        22.6\n      \n    \n    \n      \n        22.7\n      \n    \n    \n      \n        22.8\n      \n    \n    \n      \n        22.9\n      \n    \n    \n      \n        23.0\n      \n    \n    \n      \n        23.1\n      \n    \n    \n      \n        23.2\n      \n    \n    \n      \n        23.3\n      \n    \n    \n      \n        23.4\n      \n    \n    \n      \n        23.5\n      \n    \n    \n      \n        23.6\n      \n    \n    \n      \n        23.7\n      \n    \n    \n      \n        23.8\n      \n    \n    \n      \n        23.9\n      \n    \n    \n      \n        24.0\n      \n    \n    \n      \n        24.1\n      \n    \n    \n      \n        24.2\n      \n    \n    \n      \n        24.3\n      \n    \n    \n      \n        24.4\n      \n    \n    \n      \n        24.5\n      \n    \n    \n      \n        24.6\n      \n    \n    \n      \n        24.7\n      \n    \n    \n      \n        24.8\n      \n    \n    \n      \n        24.9\n      \n    \n    \n      \n        25.0\n      \n    \n    \n      \n        0\n      \n    \n    \n      \n        25\n      \n    \n  \n  \n    \n      \n        Empirical"
  },
  {
    "objectID": "slides/2023-11-15-model-selection.html#all-in-one",
    "href": "slides/2023-11-15-model-selection.html#all-in-one",
    "title": "Quantitative and Graphical Model Selection",
    "section": "All in one",
    "text": "All in one\n\nExtremes.diagnosticplots(hobby_bayes)\n\n\n\n\n\n  \n    \n  \n\n\n  \n    \n      \n        Return Period\n      \n    \n  \n  \n    \n      \n        100.0\n      \n    \n    \n      \n        100.5\n      \n    \n    \n      \n        101.0\n      \n    \n    \n      \n        101.5\n      \n    \n    \n      \n        102.0\n      \n    \n    \n      \n        100.0\n      \n    \n    \n      \n        100.1\n      \n    \n    \n      \n        100.2\n      \n    \n    \n      \n        100.3\n      \n    \n    \n      \n        100.4\n      \n    \n    \n      \n        100.5\n      \n    \n    \n      \n        100.6\n      \n    \n    \n      \n        100.7\n      \n    \n    \n      \n        100.8\n      \n    \n    \n      \n        100.9\n      \n    \n    \n      \n        101.0\n      \n    \n    \n      \n        101.1\n      \n    \n    \n      \n        101.2\n      \n    \n    \n      \n        101.3\n      \n    \n    \n      \n        101.4\n      \n    \n    \n      \n        101.5\n      \n    \n    \n      \n        101.6\n      \n    \n    \n      \n        101.7\n      \n    \n    \n      \n        101.8\n      \n    \n    \n      \n        101.9\n      \n    \n    \n      \n        102.0\n      \n    \n    \n      \n        100.00\n      \n    \n    \n      \n        100.01\n      \n    \n    \n      \n        100.02\n      \n    \n    \n      \n        100.03\n      \n    \n    \n      \n        100.04\n      \n    \n    \n      \n        100.05\n      \n    \n    \n      \n        100.06\n      \n    \n    \n      \n        100.07\n      \n    \n    \n      \n        100.08\n      \n    \n    \n      \n        100.09\n      \n    \n    \n      \n        100.10\n      \n    \n    \n      \n        100.11\n      \n    \n    \n      \n        100.12\n      \n    \n    \n      \n        100.13\n      \n    \n    \n      \n        100.14\n      \n    \n    \n      \n        100.15\n      \n    \n    \n      \n        100.16\n      \n    \n    \n      \n        100.17\n      \n    \n    \n      \n        100.18\n      \n    \n    \n      \n        100.19\n      \n    \n    \n      \n        100.20\n      \n    \n    \n      \n        100.21\n      \n    \n    \n      \n        100.22\n      \n    \n    \n      \n        100.23\n      \n    \n    \n      \n        100.24\n      \n    \n    \n      \n        100.25\n      \n    \n    \n      \n        100.26\n      \n    \n    \n      \n        100.27\n      \n    \n    \n      \n        100.28\n      \n    \n    \n      \n        100.29\n      \n    \n    \n      \n        100.30\n      \n    \n    \n      \n        100.31\n      \n    \n    \n      \n        100.32\n      \n    \n    \n      \n        100.33\n      \n    \n    \n      \n        100.34\n      \n    \n    \n      \n        100.35\n      \n    \n    \n      \n        100.36\n      \n    \n    \n      \n        100.37\n      \n    \n    \n      \n        100.38\n      \n    \n    \n      \n        100.39\n      \n    \n    \n      \n        100.40\n      \n    \n    \n      \n        100.41\n      \n    \n    \n      \n        100.42\n      \n    \n    \n      \n        100.43\n      \n    \n    \n      \n        100.44\n      \n    \n    \n      \n        100.45\n      \n    \n    \n      \n        100.46\n      \n    \n    \n      \n        100.47\n      \n    \n    \n      \n        100.48\n      \n    \n    \n      \n        100.49\n      \n    \n    \n      \n        100.50\n      \n    \n    \n      \n        100.51\n      \n    \n    \n      \n        100.52\n      \n    \n    \n      \n        100.53\n      \n    \n    \n      \n        100.54\n      \n    \n    \n      \n        100.55\n      \n    \n    \n      \n        100.56\n      \n    \n    \n      \n        100.57\n      \n    \n    \n      \n        100.58\n      \n    \n    \n      \n        100.59\n      \n    \n    \n      \n        100.60\n      \n    \n    \n      \n        100.61\n      \n    \n    \n      \n        100.62\n      \n    \n    \n      \n        100.63\n      \n    \n    \n      \n        100.64\n      \n    \n    \n      \n        100.65\n      \n    \n    \n      \n        100.66\n      \n    \n    \n      \n        100.67\n      \n    \n    \n      \n        100.68\n      \n    \n    \n      \n        100.69\n      \n    \n    \n      \n        100.70\n      \n    \n    \n      \n        100.71\n      \n    \n    \n      \n        100.72\n      \n    \n    \n      \n        100.73\n      \n    \n    \n      \n        100.74\n      \n    \n    \n      \n        100.75\n      \n    \n    \n      \n        100.76\n      \n    \n    \n      \n        100.77\n      \n    \n    \n      \n        100.78\n      \n    \n    \n      \n        100.79\n      \n    \n    \n      \n        100.80\n      \n    \n    \n      \n        100.81\n      \n    \n    \n      \n        100.82\n      \n    \n    \n      \n        100.83\n      \n    \n    \n      \n        100.84\n      \n    \n    \n      \n        100.85\n      \n    \n    \n      \n        100.86\n      \n    \n    \n      \n        100.87\n      \n    \n    \n      \n        100.88\n      \n    \n    \n      \n        100.89\n      \n    \n    \n      \n        100.90\n      \n    \n    \n      \n        100.91\n      \n    \n    \n      \n        100.92\n      \n    \n    \n      \n        100.93\n      \n    \n    \n      \n        100.94\n      \n    \n    \n      \n        100.95\n      \n    \n    \n      \n        100.96\n      \n    \n    \n      \n        100.97\n      \n    \n    \n      \n        100.98\n      \n    \n    \n      \n        100.99\n      \n    \n    \n      \n        101.00\n      \n    \n    \n      \n        101.01\n      \n    \n    \n      \n        101.02\n      \n    \n    \n      \n        101.03\n      \n    \n    \n      \n        101.04\n      \n    \n    \n      \n        101.05\n      \n    \n    \n      \n        101.06\n      \n    \n    \n      \n        101.07\n      \n    \n    \n      \n        101.08\n      \n    \n    \n      \n        101.09\n      \n    \n    \n      \n        101.10\n      \n    \n    \n      \n        101.11\n      \n    \n    \n      \n        101.12\n      \n    \n    \n      \n        101.13\n      \n    \n    \n      \n        101.14\n      \n    \n    \n      \n        101.15\n      \n    \n    \n      \n        101.16\n      \n    \n    \n      \n        101.17\n      \n    \n    \n      \n        101.18\n      \n    \n    \n      \n        101.19\n      \n    \n    \n      \n        101.20\n      \n    \n    \n      \n        101.21\n      \n    \n    \n      \n        101.22\n      \n    \n    \n      \n        101.23\n      \n    \n    \n      \n        101.24\n      \n    \n    \n      \n        101.25\n      \n    \n    \n      \n        101.26\n      \n    \n    \n      \n        101.27\n      \n    \n    \n      \n        101.28\n      \n    \n    \n      \n        101.29\n      \n    \n    \n      \n        101.30\n      \n    \n    \n      \n        101.31\n      \n    \n    \n      \n        101.32\n      \n    \n    \n      \n        101.33\n      \n    \n    \n      \n        101.34\n      \n    \n    \n      \n        101.35\n      \n    \n    \n      \n        101.36\n      \n    \n    \n      \n        101.37\n      \n    \n    \n      \n        101.38\n      \n    \n    \n      \n        101.39\n      \n    \n    \n      \n        101.40\n      \n    \n    \n      \n        101.41\n      \n    \n    \n      \n        101.42\n      \n    \n    \n      \n        101.43\n      \n    \n    \n      \n        101.44\n      \n    \n    \n      \n        101.45\n      \n    \n    \n      \n        101.46\n      \n    \n    \n      \n        101.47\n      \n    \n    \n      \n        101.48\n      \n    \n    \n      \n        101.49\n      \n    \n    \n      \n        101.50\n      \n    \n    \n      \n        101.51\n      \n    \n    \n      \n        101.52\n      \n    \n    \n      \n        101.53\n      \n    \n    \n      \n        101.54\n      \n    \n    \n      \n        101.55\n      \n    \n    \n      \n        101.56\n      \n    \n    \n      \n        101.57\n      \n    \n    \n      \n        101.58\n      \n    \n    \n      \n        101.59\n      \n    \n    \n      \n        101.60\n      \n    \n    \n      \n        101.61\n      \n    \n    \n      \n        101.62\n      \n    \n    \n      \n        101.63\n      \n    \n    \n      \n        101.64\n      \n    \n    \n      \n        101.65\n      \n    \n    \n      \n        101.66\n      \n    \n    \n      \n        101.67\n      \n    \n    \n      \n        101.68\n      \n    \n    \n      \n        101.69\n      \n    \n    \n      \n        101.70\n      \n    \n    \n      \n        101.71\n      \n    \n    \n      \n        101.72\n      \n    \n    \n      \n        101.73\n      \n    \n    \n      \n        101.74\n      \n    \n    \n      \n        101.75\n      \n    \n    \n      \n        101.76\n      \n    \n    \n      \n        101.77\n      \n    \n    \n      \n        101.78\n      \n    \n    \n      \n        101.79\n      \n    \n    \n      \n        101.80\n      \n    \n    \n      \n        101.81\n      \n    \n    \n      \n        101.82\n      \n    \n    \n      \n        101.83\n      \n    \n    \n      \n        101.84\n      \n    \n    \n      \n        101.85\n      \n    \n    \n      \n        101.86\n      \n    \n    \n      \n        101.87\n      \n    \n    \n      \n        101.88\n      \n    \n    \n      \n        101.89\n      \n    \n    \n      \n        101.90\n      \n    \n    \n      \n        101.91\n      \n    \n    \n      \n        101.92\n      \n    \n    \n      \n        101.93\n      \n    \n    \n      \n        101.94\n      \n    \n    \n      \n        101.95\n      \n    \n    \n      \n        101.96\n      \n    \n    \n      \n        101.97\n      \n    \n    \n      \n        101.98\n      \n    \n    \n      \n        101.99\n      \n    \n    \n      \n        102.00\n      \n    \n    \n      \n        100\n      \n    \n    \n      \n        102\n      \n    \n  \n  \n    \n      \n        \n          \n        \n      \n      \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n      \n      \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n      \n      \n        \n          \n          \n        \n        \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1.93449845124356716.86\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1.633468455579585814.63\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1.45737719652390611.12\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1.33243845991560569.95\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1.2355284469075499.48\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1.1563472008599248.83\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1.08940041122931088.73\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1.0314084642516248.58\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.9802559418042438.05\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.93449845124356787.86\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.89310576608534267.07\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.85531720519594287.02\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.82055509893673086.79\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.78837041556532986.57\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.75840719218788656.5\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.7303784685876436.35\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.70404952986529386.3\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.67922594614026156.2\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.65574485029073885.87\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.63346845557958665.75\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.61227915650964855.69\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.59207577042136145.63\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.57277061522597485.63\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.55428720953196165.62\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.53655844257153025.42\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.51952510327274985.18\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.50313468708458045.02\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.487340419901348474.97\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.47210045334461164.86\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.457377196523905354.85\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.44313675740929514.74\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.429348472923661744.6\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.41598451136568024.6\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.40301953420131264.58\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.39043040689329214.53\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.37819595047628054.45\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.366296727176572754.35\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.354714854626757564.28\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.343433844217068464.26\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.332438459915605334.25\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.32171459452383224.22\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.311249160845667344.19\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.30102999566398124.18\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.291045774757380254.18\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.2812859374682244.17\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.271740619561993634.1\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.26240059330785023.98\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.25325721386798053.88\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.244302371215054053.85\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.235528446907548963.73\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.22692827514563143.68\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.218495107608768593.68\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.21022258164277873.62\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.20210469142059923.6\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.194135761749323843.5\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.186310424237367273.5\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.178623595571076273.45\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.171070457680630453.37\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.163646439601423553.3\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.15634720085992413.26\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.149168616232800753.22\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.14210676174531393.08\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.135157901789986023.07\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.128318477259680543.05\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.121585094600712143.04\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.114954515701699043.03\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.108423648542741272.99\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.101989538537331462.9\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.09564936050631242.85\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.089400411229310922.85\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.083240102524492422.8\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.077165954812299332.75\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.071175591123111842.73\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.065266731512591572.7\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.05943718785186772.67\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.0536848589627763542.65\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.0480077260710858252.63\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.0424038485530872642.6\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.0368713599531263062.56\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.031408464251624122.53\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.0260134323649179952.52\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.020684598859851052.32\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.015420358867493822.3\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.0102191651816861221.88\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.0050795255292749711.66\n                \n              \n            \n          \n        \n        \n          \n            \n              \n            \n          \n        \n      \n      \n        \n          \n            \n              \n            \n          \n        \n      \n      \n        \n          \n            \n              \n            \n          \n          \n            \n              \n                h,j,k,l,arrows,drag to pan\n              \n            \n            \n              \n                i,o,+,-,scroll,shift-drag to zoom\n              \n            \n            \n              \n                r,dbl-click to reset\n              \n            \n            \n              \n                c for coordinates\n              \n            \n            \n              \n                ? for help\n              \n            \n          \n        \n      \n      \n        \n          \n            \n              ?\n            \n          \n        \n      \n    \n  \n  \n    \n      \n        0\n      \n    \n    \n      \n        5\n      \n    \n    \n      \n        10\n      \n    \n    \n      \n        15\n      \n    \n    \n      \n        20\n      \n    \n    \n      \n        0\n      \n    \n    \n      \n        1\n      \n    \n    \n      \n        2\n      \n    \n    \n      \n        3\n      \n    \n    \n      \n        4\n      \n    \n    \n      \n        5\n      \n    \n    \n      \n        6\n      \n    \n    \n      \n        7\n      \n    \n    \n      \n        8\n      \n    \n    \n      \n        9\n      \n    \n    \n      \n        10\n      \n    \n    \n      \n        11\n      \n    \n    \n      \n        12\n      \n    \n    \n      \n        13\n      \n    \n    \n      \n        14\n      \n    \n    \n      \n        15\n      \n    \n    \n      \n        16\n      \n    \n    \n      \n        17\n      \n    \n    \n      \n        18\n      \n    \n    \n      \n        19\n      \n    \n    \n      \n        20\n      \n    \n    \n      \n        0.0\n      \n    \n    \n      \n        0.1\n      \n    \n    \n      \n        0.2\n      \n    \n    \n      \n        0.3\n      \n    \n    \n      \n        0.4\n      \n    \n    \n      \n        0.5\n      \n    \n    \n      \n        0.6\n      \n    \n    \n      \n        0.7\n      \n    \n    \n      \n        0.8\n      \n    \n    \n      \n        0.9\n      \n    \n    \n      \n        1.0\n      \n    \n    \n      \n        1.1\n      \n    \n    \n      \n        1.2\n      \n    \n    \n      \n        1.3\n      \n    \n    \n      \n        1.4\n      \n    \n    \n      \n        1.5\n      \n    \n    \n      \n        1.6\n      \n    \n    \n      \n        1.7\n      \n    \n    \n      \n        1.8\n      \n    \n    \n      \n        1.9\n      \n    \n    \n      \n        2.0\n      \n    \n    \n      \n        2.1\n      \n    \n    \n      \n        2.2\n      \n    \n    \n      \n        2.3\n      \n    \n    \n      \n        2.4\n      \n    \n    \n      \n        2.5\n      \n    \n    \n      \n        2.6\n      \n    \n    \n      \n        2.7\n      \n    \n    \n      \n        2.8\n      \n    \n    \n      \n        2.9\n      \n    \n    \n      \n        3.0\n      \n    \n    \n      \n        3.1\n      \n    \n    \n      \n        3.2\n      \n    \n    \n      \n        3.3\n      \n    \n    \n      \n        3.4\n      \n    \n    \n      \n        3.5\n      \n    \n    \n      \n        3.6\n      \n    \n    \n      \n        3.7\n      \n    \n    \n      \n        3.8\n      \n    \n    \n      \n        3.9\n      \n    \n    \n      \n        4.0\n      \n    \n    \n      \n        4.1\n      \n    \n    \n      \n        4.2\n      \n    \n    \n      \n        4.3\n      \n    \n    \n      \n        4.4\n      \n    \n    \n      \n        4.5\n      \n    \n    \n      \n        4.6\n      \n    \n    \n      \n        4.7\n      \n    \n    \n      \n        4.8\n      \n    \n    \n      \n        4.9\n      \n    \n    \n      \n        5.0\n      \n    \n    \n      \n        5.1\n      \n    \n    \n      \n        5.2\n      \n    \n    \n      \n        5.3\n      \n    \n    \n      \n        5.4\n      \n    \n    \n      \n        5.5\n      \n    \n    \n      \n        5.6\n      \n    \n    \n      \n        5.7\n      \n    \n    \n      \n        5.8\n      \n    \n    \n      \n        5.9\n      \n    \n    \n      \n        6.0\n      \n    \n    \n      \n        6.1\n      \n    \n    \n      \n        6.2\n      \n    \n    \n      \n        6.3\n      \n    \n    \n      \n        6.4\n      \n    \n    \n      \n        6.5\n      \n    \n    \n      \n        6.6\n      \n    \n    \n      \n        6.7\n      \n    \n    \n      \n        6.8\n      \n    \n    \n      \n        6.9\n      \n    \n    \n      \n        7.0\n      \n    \n    \n      \n        7.1\n      \n    \n    \n      \n        7.2\n      \n    \n    \n      \n        7.3\n      \n    \n    \n      \n        7.4\n      \n    \n    \n      \n        7.5\n      \n    \n    \n      \n        7.6\n      \n    \n    \n      \n        7.7\n      \n    \n    \n      \n        7.8\n      \n    \n    \n      \n        7.9\n      \n    \n    \n      \n        8.0\n      \n    \n    \n      \n        8.1\n      \n    \n    \n      \n        8.2\n      \n    \n    \n      \n        8.3\n      \n    \n    \n      \n        8.4\n      \n    \n    \n      \n        8.5\n      \n    \n    \n      \n        8.6\n      \n    \n    \n      \n        8.7\n      \n    \n    \n      \n        8.8\n      \n    \n    \n      \n        8.9\n      \n    \n    \n      \n        9.0\n      \n    \n    \n      \n        9.1\n      \n    \n    \n      \n        9.2\n      \n    \n    \n      \n        9.3\n      \n    \n    \n      \n        9.4\n      \n    \n    \n      \n        9.5\n      \n    \n    \n      \n        9.6\n      \n    \n    \n      \n        9.7\n      \n    \n    \n      \n        9.8\n      \n    \n    \n      \n        9.9\n      \n    \n    \n      \n        10.0\n      \n    \n    \n      \n        10.1\n      \n    \n    \n      \n        10.2\n      \n    \n    \n      \n        10.3\n      \n    \n    \n      \n        10.4\n      \n    \n    \n      \n        10.5\n      \n    \n    \n      \n        10.6\n      \n    \n    \n      \n        10.7\n      \n    \n    \n      \n        10.8\n      \n    \n    \n      \n        10.9\n      \n    \n    \n      \n        11.0\n      \n    \n    \n      \n        11.1\n      \n    \n    \n      \n        11.2\n      \n    \n    \n      \n        11.3\n      \n    \n    \n      \n        11.4\n      \n    \n    \n      \n        11.5\n      \n    \n    \n      \n        11.6\n      \n    \n    \n      \n        11.7\n      \n    \n    \n      \n        11.8\n      \n    \n    \n      \n        11.9\n      \n    \n    \n      \n        12.0\n      \n    \n    \n      \n        12.1\n      \n    \n    \n      \n        12.2\n      \n    \n    \n      \n        12.3\n      \n    \n    \n      \n        12.4\n      \n    \n    \n      \n        12.5\n      \n    \n    \n      \n        12.6\n      \n    \n    \n      \n        12.7\n      \n    \n    \n      \n        12.8\n      \n    \n    \n      \n        12.9\n      \n    \n    \n      \n        13.0\n      \n    \n    \n      \n        13.1\n      \n    \n    \n      \n        13.2\n      \n    \n    \n      \n        13.3\n      \n    \n    \n      \n        13.4\n      \n    \n    \n      \n        13.5\n      \n    \n    \n      \n        13.6\n      \n    \n    \n      \n        13.7\n      \n    \n    \n      \n        13.8\n      \n    \n    \n      \n        13.9\n      \n    \n    \n      \n        14.0\n      \n    \n    \n      \n        14.1\n      \n    \n    \n      \n        14.2\n      \n    \n    \n      \n        14.3\n      \n    \n    \n      \n        14.4\n      \n    \n    \n      \n        14.5\n      \n    \n    \n      \n        14.6\n      \n    \n    \n      \n        14.7\n      \n    \n    \n      \n        14.8\n      \n    \n    \n      \n        14.9\n      \n    \n    \n      \n        15.0\n      \n    \n    \n      \n        15.1\n      \n    \n    \n      \n        15.2\n      \n    \n    \n      \n        15.3\n      \n    \n    \n      \n        15.4\n      \n    \n    \n      \n        15.5\n      \n    \n    \n      \n        15.6\n      \n    \n    \n      \n        15.7\n      \n    \n    \n      \n        15.8\n      \n    \n    \n      \n        15.9\n      \n    \n    \n      \n        16.0\n      \n    \n    \n      \n        16.1\n      \n    \n    \n      \n        16.2\n      \n    \n    \n      \n        16.3\n      \n    \n    \n      \n        16.4\n      \n    \n    \n      \n        16.5\n      \n    \n    \n      \n        16.6\n      \n    \n    \n      \n        16.7\n      \n    \n    \n      \n        16.8\n      \n    \n    \n      \n        16.9\n      \n    \n    \n      \n        17.0\n      \n    \n    \n      \n        17.1\n      \n    \n    \n      \n        17.2\n      \n    \n    \n      \n        17.3\n      \n    \n    \n      \n        17.4\n      \n    \n    \n      \n        17.5\n      \n    \n    \n      \n        17.6\n      \n    \n    \n      \n        17.7\n      \n    \n    \n      \n        17.8\n      \n    \n    \n      \n        17.9\n      \n    \n    \n      \n        18.0\n      \n    \n    \n      \n        18.1\n      \n    \n    \n      \n        18.2\n      \n    \n    \n      \n        18.3\n      \n    \n    \n      \n        18.4\n      \n    \n    \n      \n        18.5\n      \n    \n    \n      \n        18.6\n      \n    \n    \n      \n        18.7\n      \n    \n    \n      \n        18.8\n      \n    \n    \n      \n        18.9\n      \n    \n    \n      \n        19.0\n      \n    \n    \n      \n        19.1\n      \n    \n    \n      \n        19.2\n      \n    \n    \n      \n        19.3\n      \n    \n    \n      \n        19.4\n      \n    \n    \n      \n        19.5\n      \n    \n    \n      \n        19.6\n      \n    \n    \n      \n        19.7\n      \n    \n    \n      \n        19.8\n      \n    \n    \n      \n        19.9\n      \n    \n    \n      \n        20.0\n      \n    \n    \n      \n        0\n      \n    \n    \n      \n        20\n      \n    \n  \n  \n    \n      \n        Return Level\n      \n    \n  \n  \n    \n      \n        Return Level Plot\n      \n    \n  \n\n\n  \n    \n      \n        Data\n      \n    \n  \n  \n    \n      \n        0\n      \n    \n    \n      \n        10\n      \n    \n    \n      \n        20\n      \n    \n    \n      \n        30\n      \n    \n    \n      \n        40\n      \n    \n    \n      \n        50\n      \n    \n    \n      \n        60\n      \n    \n    \n      \n        0\n      \n    \n    \n      \n        2\n      \n    \n    \n      \n        4\n      \n    \n    \n      \n        6\n      \n    \n    \n      \n        8\n      \n    \n    \n      \n        10\n      \n    \n    \n      \n        12\n      \n    \n    \n      \n        14\n      \n    \n    \n      \n        16\n      \n    \n    \n      \n        18\n      \n    \n    \n      \n        20\n      \n    \n    \n      \n        22\n      \n    \n    \n      \n        24\n      \n    \n    \n      \n        26\n      \n    \n    \n      \n        28\n      \n    \n    \n      \n        30\n      \n    \n    \n      \n        32\n      \n    \n    \n      \n        34\n      \n    \n    \n      \n        36\n      \n    \n    \n      \n        38\n      \n    \n    \n      \n        40\n      \n    \n    \n      \n        42\n      \n    \n    \n      \n        44\n      \n    \n    \n      \n        46\n      \n    \n    \n      \n        48\n      \n    \n    \n      \n        50\n      \n    \n    \n      \n        52\n      \n    \n    \n      \n        54\n      \n    \n    \n      \n        0.0\n      \n    \n    \n      \n        0.2\n      \n    \n    \n      \n        0.4\n      \n    \n    \n      \n        0.6\n      \n    \n    \n      \n        0.8\n      \n    \n    \n      \n        1.0\n      \n    \n    \n      \n        1.2\n      \n    \n    \n      \n        1.4\n      \n    \n    \n      \n        1.6\n      \n    \n    \n      \n        1.8\n      \n    \n    \n      \n        2.0\n      \n    \n    \n      \n        2.2\n      \n    \n    \n      \n        2.4\n      \n    \n    \n      \n        2.6\n      \n    \n    \n      \n        2.8\n      \n    \n    \n      \n        3.0\n      \n    \n    \n      \n        3.2\n      \n    \n    \n      \n        3.4\n      \n    \n    \n      \n        3.6\n      \n    \n    \n      \n        3.8\n      \n    \n    \n      \n        4.0\n      \n    \n    \n      \n        4.2\n      \n    \n    \n      \n        4.4\n      \n    \n    \n      \n        4.6\n      \n    \n    \n      \n        4.8\n      \n    \n    \n      \n        5.0\n      \n    \n    \n      \n        5.2\n      \n    \n    \n      \n        5.4\n      \n    \n    \n      \n        5.6\n      \n    \n    \n      \n        5.8\n      \n    \n    \n      \n        6.0\n      \n    \n    \n      \n        6.2\n      \n    \n    \n      \n        6.4\n      \n    \n    \n      \n        6.6\n      \n    \n    \n      \n        6.8\n      \n    \n    \n      \n        7.0\n      \n    \n    \n      \n        7.2\n      \n    \n    \n      \n        7.4\n      \n    \n    \n      \n        7.6\n      \n    \n    \n      \n        7.8\n      \n    \n    \n      \n        8.0\n      \n    \n    \n      \n        8.2\n      \n    \n    \n      \n        8.4\n      \n    \n    \n      \n        8.6\n      \n    \n    \n      \n        8.8\n      \n    \n    \n      \n        9.0\n      \n    \n    \n      \n        9.2\n      \n    \n    \n      \n        9.4\n      \n    \n    \n      \n        9.6\n      \n    \n    \n      \n        9.8\n      \n    \n    \n      \n        10.0\n      \n    \n    \n      \n        10.2\n      \n    \n    \n      \n        10.4\n      \n    \n    \n      \n        10.6\n      \n    \n    \n      \n        10.8\n      \n    \n    \n      \n        11.0\n      \n    \n    \n      \n        11.2\n      \n    \n    \n      \n        11.4\n      \n    \n    \n      \n        11.6\n      \n    \n    \n      \n        11.8\n      \n    \n    \n      \n        12.0\n      \n    \n    \n      \n        12.2\n      \n    \n    \n      \n        12.4\n      \n    \n    \n      \n        12.6\n      \n    \n    \n      \n        12.8\n      \n    \n    \n      \n        13.0\n      \n    \n    \n      \n        13.2\n      \n    \n    \n      \n        13.4\n      \n    \n    \n      \n        13.6\n      \n    \n    \n      \n        13.8\n      \n    \n    \n      \n        14.0\n      \n    \n    \n      \n        14.2\n      \n    \n    \n      \n        14.4\n      \n    \n    \n      \n        14.6\n      \n    \n    \n      \n        14.8\n      \n    \n    \n      \n        15.0\n      \n    \n    \n      \n        15.2\n      \n    \n    \n      \n        15.4\n      \n    \n    \n      \n        15.6\n      \n    \n    \n      \n        15.8\n      \n    \n    \n      \n        16.0\n      \n    \n    \n      \n        16.2\n      \n    \n    \n      \n        16.4\n      \n    \n    \n      \n        16.6\n      \n    \n    \n      \n        16.8\n      \n    \n    \n      \n        17.0\n      \n    \n    \n      \n        17.2\n      \n    \n    \n      \n        17.4\n      \n    \n    \n      \n        17.6\n      \n    \n    \n      \n        17.8\n      \n    \n    \n      \n        18.0\n      \n    \n    \n      \n        18.2\n      \n    \n    \n      \n        18.4\n      \n    \n    \n      \n        18.6\n      \n    \n    \n      \n        18.8\n      \n    \n    \n      \n        19.0\n      \n    \n    \n      \n        19.2\n      \n    \n    \n      \n        19.4\n      \n    \n    \n      \n        19.6\n      \n    \n    \n      \n        19.8\n      \n    \n    \n      \n        20.0\n      \n    \n    \n      \n        20.2\n      \n    \n    \n      \n        20.4\n      \n    \n    \n      \n        20.6\n      \n    \n    \n      \n        20.8\n      \n    \n    \n      \n        21.0\n      \n    \n    \n      \n        21.2\n      \n    \n    \n      \n        21.4\n      \n    \n    \n      \n        21.6\n      \n    \n    \n      \n        21.8\n      \n    \n    \n      \n        22.0\n      \n    \n    \n      \n        22.2\n      \n    \n    \n      \n        22.4\n      \n    \n    \n      \n        22.6\n      \n    \n    \n      \n        22.8\n      \n    \n    \n      \n        23.0\n      \n    \n    \n      \n        23.2\n      \n    \n    \n      \n        23.4\n      \n    \n    \n      \n        23.6\n      \n    \n    \n      \n        23.8\n      \n    \n    \n      \n        24.0\n      \n    \n    \n      \n        24.2\n      \n    \n    \n      \n        24.4\n      \n    \n    \n      \n        24.6\n      \n    \n    \n      \n        24.8\n      \n    \n    \n      \n        25.0\n      \n    \n    \n      \n        25.2\n      \n    \n    \n      \n        25.4\n      \n    \n    \n      \n        25.6\n      \n    \n    \n      \n        25.8\n      \n    \n    \n      \n        26.0\n      \n    \n    \n      \n        26.2\n      \n    \n    \n      \n        26.4\n      \n    \n    \n      \n        26.6\n      \n    \n    \n      \n        26.8\n      \n    \n    \n      \n        27.0\n      \n    \n    \n      \n        27.2\n      \n    \n    \n      \n        27.4\n      \n    \n    \n      \n        27.6\n      \n    \n    \n      \n        27.8\n      \n    \n    \n      \n        28.0\n      \n    \n    \n      \n        28.2\n      \n    \n    \n      \n        28.4\n      \n    \n    \n      \n        28.6\n      \n    \n    \n      \n        28.8\n      \n    \n    \n      \n        29.0\n      \n    \n    \n      \n        29.2\n      \n    \n    \n      \n        29.4\n      \n    \n    \n      \n        29.6\n      \n    \n    \n      \n        29.8\n      \n    \n    \n      \n        30.0\n      \n    \n    \n      \n        30.2\n      \n    \n    \n      \n        30.4\n      \n    \n    \n      \n        30.6\n      \n    \n    \n      \n        30.8\n      \n    \n    \n      \n        31.0\n      \n    \n    \n      \n        31.2\n      \n    \n    \n      \n        31.4\n      \n    \n    \n      \n        31.6\n      \n    \n    \n      \n        31.8\n      \n    \n    \n      \n        32.0\n      \n    \n    \n      \n        32.2\n      \n    \n    \n      \n        32.4\n      \n    \n    \n      \n        32.6\n      \n    \n    \n      \n        32.8\n      \n    \n    \n      \n        33.0\n      \n    \n    \n      \n        33.2\n      \n    \n    \n      \n        33.4\n      \n    \n    \n      \n        33.6\n      \n    \n    \n      \n        33.8\n      \n    \n    \n      \n        34.0\n      \n    \n    \n      \n        34.2\n      \n    \n    \n      \n        34.4\n      \n    \n    \n      \n        34.6\n      \n    \n    \n      \n        34.8\n      \n    \n    \n      \n        35.0\n      \n    \n    \n      \n        35.2\n      \n    \n    \n      \n        35.4\n      \n    \n    \n      \n        35.6\n      \n    \n    \n      \n        35.8\n      \n    \n    \n      \n        36.0\n      \n    \n    \n      \n        36.2\n      \n    \n    \n      \n        36.4\n      \n    \n    \n      \n        36.6\n      \n    \n    \n      \n        36.8\n      \n    \n    \n      \n        37.0\n      \n    \n    \n      \n        37.2\n      \n    \n    \n      \n        37.4\n      \n    \n    \n      \n        37.6\n      \n    \n    \n      \n        37.8\n      \n    \n    \n      \n        38.0\n      \n    \n    \n      \n        38.2\n      \n    \n    \n      \n        38.4\n      \n    \n    \n      \n        38.6\n      \n    \n    \n      \n        38.8\n      \n    \n    \n      \n        39.0\n      \n    \n    \n      \n        39.2\n      \n    \n    \n      \n        39.4\n      \n    \n    \n      \n        39.6\n      \n    \n    \n      \n        39.8\n      \n    \n    \n      \n        40.0\n      \n    \n    \n      \n        40.2\n      \n    \n    \n      \n        40.4\n      \n    \n    \n      \n        40.6\n      \n    \n    \n      \n        40.8\n      \n    \n    \n      \n        41.0\n      \n    \n    \n      \n        41.2\n      \n    \n    \n      \n        41.4\n      \n    \n    \n      \n        41.6\n      \n    \n    \n      \n        41.8\n      \n    \n    \n      \n        42.0\n      \n    \n    \n      \n        42.2\n      \n    \n    \n      \n        42.4\n      \n    \n    \n      \n        42.6\n      \n    \n    \n      \n        42.8\n      \n    \n    \n      \n        43.0\n      \n    \n    \n      \n        43.2\n      \n    \n    \n      \n        43.4\n      \n    \n    \n      \n        43.6\n      \n    \n    \n      \n        43.8\n      \n    \n    \n      \n        44.0\n      \n    \n    \n      \n        44.2\n      \n    \n    \n      \n        44.4\n      \n    \n    \n      \n        44.6\n      \n    \n    \n      \n        44.8\n      \n    \n    \n      \n        45.0\n      \n    \n    \n      \n        45.2\n      \n    \n    \n      \n        45.4\n      \n    \n    \n      \n        45.6\n      \n    \n    \n      \n        45.8\n      \n    \n    \n      \n        46.0\n      \n    \n    \n      \n        46.2\n      \n    \n    \n      \n        46.4\n      \n    \n    \n      \n        46.6\n      \n    \n    \n      \n        46.8\n      \n    \n    \n      \n        47.0\n      \n    \n    \n      \n        47.2\n      \n    \n    \n      \n        47.4\n      \n    \n    \n      \n        47.6\n      \n    \n    \n      \n        47.8\n      \n    \n    \n      \n        48.0\n      \n    \n    \n      \n        48.2\n      \n    \n    \n      \n        48.4\n      \n    \n    \n      \n        48.6\n      \n    \n    \n      \n        48.8\n      \n    \n    \n      \n        49.0\n      \n    \n    \n      \n        49.2\n      \n    \n    \n      \n        49.4\n      \n    \n    \n      \n        49.6\n      \n    \n    \n      \n        49.8\n      \n    \n    \n      \n        50.0\n      \n    \n    \n      \n        50.2\n      \n    \n    \n      \n        50.4\n      \n    \n    \n      \n        50.6\n      \n    \n    \n      \n        50.8\n      \n    \n    \n      \n        51.0\n      \n    \n    \n      \n        51.2\n      \n    \n    \n      \n        51.4\n      \n    \n    \n      \n        51.6\n      \n    \n    \n      \n        51.8\n      \n    \n    \n      \n        52.0\n      \n    \n    \n      \n        52.2\n      \n    \n    \n      \n        0\n      \n    \n    \n      \n        100\n      \n    \n  \n  \n    \n      \n        \n          \n        \n      \n      \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n      \n      \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n      \n      \n        \n          \n          \n        \n        \n          \n            \n              \n            \n            \n              \n            \n            \n              \n            \n            \n              \n            \n            \n              \n            \n            \n              \n            \n            \n              \n            \n            \n              \n            \n            \n              \n            \n            \n              \n            \n          \n        \n        \n          \n            \n              \n            \n          \n        \n      \n      \n        \n          \n            \n              \n            \n          \n        \n      \n      \n        \n          \n            \n              \n            \n          \n          \n            \n              \n                h,j,k,l,arrows,drag to pan\n              \n            \n            \n              \n                i,o,+,-,scroll,shift-drag to zoom\n              \n            \n            \n              \n                r,dbl-click to reset\n              \n            \n            \n              \n                c for coordinates\n              \n            \n            \n              \n                ? for help\n              \n            \n          \n        \n      \n      \n        \n          \n            \n              ?\n            \n          \n        \n      \n    \n  \n  \n    \n      \n        0.0\n      \n    \n    \n      \n        0.1\n      \n    \n    \n      \n        0.2\n      \n    \n    \n      \n        0.3\n      \n    \n    \n      \n        0.00\n      \n    \n    \n      \n        0.01\n      \n    \n    \n      \n        0.02\n      \n    \n    \n      \n        0.03\n      \n    \n    \n      \n        0.04\n      \n    \n    \n      \n        0.05\n      \n    \n    \n      \n        0.06\n      \n    \n    \n      \n        0.07\n      \n    \n    \n      \n        0.08\n      \n    \n    \n      \n        0.09\n      \n    \n    \n      \n        0.10\n      \n    \n    \n      \n        0.11\n      \n    \n    \n      \n        0.12\n      \n    \n    \n      \n        0.13\n      \n    \n    \n      \n        0.14\n      \n    \n    \n      \n        0.15\n      \n    \n    \n      \n        0.16\n      \n    \n    \n      \n        0.17\n      \n    \n    \n      \n        0.18\n      \n    \n    \n      \n        0.19\n      \n    \n    \n      \n        0.20\n      \n    \n    \n      \n        0.21\n      \n    \n    \n      \n        0.22\n      \n    \n    \n      \n        0.23\n      \n    \n    \n      \n        0.24\n      \n    \n    \n      \n        0.25\n      \n    \n    \n      \n        0.26\n      \n    \n    \n      \n        0.27\n      \n    \n    \n      \n        0.28\n      \n    \n    \n      \n        0.29\n      \n    \n    \n      \n        0.30\n      \n    \n    \n      \n        0.000\n      \n    \n    \n      \n        0.001\n      \n    \n    \n      \n        0.002\n      \n    \n    \n      \n        0.003\n      \n    \n    \n      \n        0.004\n      \n    \n    \n      \n        0.005\n      \n    \n    \n      \n        0.006\n      \n    \n    \n      \n        0.007\n      \n    \n    \n      \n        0.008\n      \n    \n    \n      \n        0.009\n      \n    \n    \n      \n        0.010\n      \n    \n    \n      \n        0.011\n      \n    \n    \n      \n        0.012\n      \n    \n    \n      \n        0.013\n      \n    \n    \n      \n        0.014\n      \n    \n    \n      \n        0.015\n      \n    \n    \n      \n        0.016\n      \n    \n    \n      \n        0.017\n      \n    \n    \n      \n        0.018\n      \n    \n    \n      \n        0.019\n      \n    \n    \n      \n        0.020\n      \n    \n    \n      \n        0.021\n      \n    \n    \n      \n        0.022\n      \n    \n    \n      \n        0.023\n      \n    \n    \n      \n        0.024\n      \n    \n    \n      \n        0.025\n      \n    \n    \n      \n        0.026\n      \n    \n    \n      \n        0.027\n      \n    \n    \n      \n        0.028\n      \n    \n    \n      \n        0.029\n      \n    \n    \n      \n        0.030\n      \n    \n    \n      \n        0.031\n      \n    \n    \n      \n        0.032\n      \n    \n    \n      \n        0.033\n      \n    \n    \n      \n        0.034\n      \n    \n    \n      \n        0.035\n      \n    \n    \n      \n        0.036\n      \n    \n    \n      \n        0.037\n      \n    \n    \n      \n        0.038\n      \n    \n    \n      \n        0.039\n      \n    \n    \n      \n        0.040\n      \n    \n    \n      \n        0.041\n      \n    \n    \n      \n        0.042\n      \n    \n    \n      \n        0.043\n      \n    \n    \n      \n        0.044\n      \n    \n    \n      \n        0.045\n      \n    \n    \n      \n        0.046\n      \n    \n    \n      \n        0.047\n      \n    \n    \n      \n        0.048\n      \n    \n    \n      \n        0.049\n      \n    \n    \n      \n        0.050\n      \n    \n    \n      \n        0.051\n      \n    \n    \n      \n        0.052\n      \n    \n    \n      \n        0.053\n      \n    \n    \n      \n        0.054\n      \n    \n    \n      \n        0.055\n      \n    \n    \n      \n        0.056\n      \n    \n    \n      \n        0.057\n      \n    \n    \n      \n        0.058\n      \n    \n    \n      \n        0.059\n      \n    \n    \n      \n        0.060\n      \n    \n    \n      \n        0.061\n      \n    \n    \n      \n        0.062\n      \n    \n    \n      \n        0.063\n      \n    \n    \n      \n        0.064\n      \n    \n    \n      \n        0.065\n      \n    \n    \n      \n        0.066\n      \n    \n    \n      \n        0.067\n      \n    \n    \n      \n        0.068\n      \n    \n    \n      \n        0.069\n      \n    \n    \n      \n        0.070\n      \n    \n    \n      \n        0.071\n      \n    \n    \n      \n        0.072\n      \n    \n    \n      \n        0.073\n      \n    \n    \n      \n        0.074\n      \n    \n    \n      \n        0.075\n      \n    \n    \n      \n        0.076\n      \n    \n    \n      \n        0.077\n      \n    \n    \n      \n        0.078\n      \n    \n    \n      \n        0.079\n      \n    \n    \n      \n        0.080\n      \n    \n    \n      \n        0.081\n      \n    \n    \n      \n        0.082\n      \n    \n    \n      \n        0.083\n      \n    \n    \n      \n        0.084\n      \n    \n    \n      \n        0.085\n      \n    \n    \n      \n        0.086\n      \n    \n    \n      \n        0.087\n      \n    \n    \n      \n        0.088\n      \n    \n    \n      \n        0.089\n      \n    \n    \n      \n        0.090\n      \n    \n    \n      \n        0.091\n      \n    \n    \n      \n        0.092\n      \n    \n    \n      \n        0.093\n      \n    \n    \n      \n        0.094\n      \n    \n    \n      \n        0.095\n      \n    \n    \n      \n        0.096\n      \n    \n    \n      \n        0.097\n      \n    \n    \n      \n        0.098\n      \n    \n    \n      \n        0.099\n      \n    \n    \n      \n        0.100\n      \n    \n    \n      \n        0.101\n      \n    \n    \n      \n        0.102\n      \n    \n    \n      \n        0.103\n      \n    \n    \n      \n        0.104\n      \n    \n    \n      \n        0.105\n      \n    \n    \n      \n        0.106\n      \n    \n    \n      \n        0.107\n      \n    \n    \n      \n        0.108\n      \n    \n    \n      \n        0.109\n      \n    \n    \n      \n        0.110\n      \n    \n    \n      \n        0.111\n      \n    \n    \n      \n        0.112\n      \n    \n    \n      \n        0.113\n      \n    \n    \n      \n        0.114\n      \n    \n    \n      \n        0.115\n      \n    \n    \n      \n        0.116\n      \n    \n    \n      \n        0.117\n      \n    \n    \n      \n        0.118\n      \n    \n    \n      \n        0.119\n      \n    \n    \n      \n        0.120\n      \n    \n    \n      \n        0.121\n      \n    \n    \n      \n        0.122\n      \n    \n    \n      \n        0.123\n      \n    \n    \n      \n        0.124\n      \n    \n    \n      \n        0.125\n      \n    \n    \n      \n        0.126\n      \n    \n    \n      \n        0.127\n      \n    \n    \n      \n        0.128\n      \n    \n    \n      \n        0.129\n      \n    \n    \n      \n        0.130\n      \n    \n    \n      \n        0.131\n      \n    \n    \n      \n        0.132\n      \n    \n    \n      \n        0.133\n      \n    \n    \n      \n        0.134\n      \n    \n    \n      \n        0.135\n      \n    \n    \n      \n        0.136\n      \n    \n    \n      \n        0.137\n      \n    \n    \n      \n        0.138\n      \n    \n    \n      \n        0.139\n      \n    \n    \n      \n        0.140\n      \n    \n    \n      \n        0.141\n      \n    \n    \n      \n        0.142\n      \n    \n    \n      \n        0.143\n      \n    \n    \n      \n        0.144\n      \n    \n    \n      \n        0.145\n      \n    \n    \n      \n        0.146\n      \n    \n    \n      \n        0.147\n      \n    \n    \n      \n        0.148\n      \n    \n    \n      \n        0.149\n      \n    \n    \n      \n        0.150\n      \n    \n    \n      \n        0.151\n      \n    \n    \n      \n        0.152\n      \n    \n    \n      \n        0.153\n      \n    \n    \n      \n        0.154\n      \n    \n    \n      \n        0.155\n      \n    \n    \n      \n        0.156\n      \n    \n    \n      \n        0.157\n      \n    \n    \n      \n        0.158\n      \n    \n    \n      \n        0.159\n      \n    \n    \n      \n        0.160\n      \n    \n    \n      \n        0.161\n      \n    \n    \n      \n        0.162\n      \n    \n    \n      \n        0.163\n      \n    \n    \n      \n        0.164\n      \n    \n    \n      \n        0.165\n      \n    \n    \n      \n        0.166\n      \n    \n    \n      \n        0.167\n      \n    \n    \n      \n        0.168\n      \n    \n    \n      \n        0.169\n      \n    \n    \n      \n        0.170\n      \n    \n    \n      \n        0.171\n      \n    \n    \n      \n        0.172\n      \n    \n    \n      \n        0.173\n      \n    \n    \n      \n        0.174\n      \n    \n    \n      \n        0.175\n      \n    \n    \n      \n        0.176\n      \n    \n    \n      \n        0.177\n      \n    \n    \n      \n        0.178\n      \n    \n    \n      \n        0.179\n      \n    \n    \n      \n        0.180\n      \n    \n    \n      \n        0.181\n      \n    \n    \n      \n        0.182\n      \n    \n    \n      \n        0.183\n      \n    \n    \n      \n        0.184\n      \n    \n    \n      \n        0.185\n      \n    \n    \n      \n        0.186\n      \n    \n    \n      \n        0.187\n      \n    \n    \n      \n        0.188\n      \n    \n    \n      \n        0.189\n      \n    \n    \n      \n        0.190\n      \n    \n    \n      \n        0.191\n      \n    \n    \n      \n        0.192\n      \n    \n    \n      \n        0.193\n      \n    \n    \n      \n        0.194\n      \n    \n    \n      \n        0.195\n      \n    \n    \n      \n        0.196\n      \n    \n    \n      \n        0.197\n      \n    \n    \n      \n        0.198\n      \n    \n    \n      \n        0.199\n      \n    \n    \n      \n        0.200\n      \n    \n    \n      \n        0.201\n      \n    \n    \n      \n        0.202\n      \n    \n    \n      \n        0.203\n      \n    \n    \n      \n        0.204\n      \n    \n    \n      \n        0.205\n      \n    \n    \n      \n        0.206\n      \n    \n    \n      \n        0.207\n      \n    \n    \n      \n        0.208\n      \n    \n    \n      \n        0.209\n      \n    \n    \n      \n        0.210\n      \n    \n    \n      \n        0.211\n      \n    \n    \n      \n        0.212\n      \n    \n    \n      \n        0.213\n      \n    \n    \n      \n        0.214\n      \n    \n    \n      \n        0.215\n      \n    \n    \n      \n        0.216\n      \n    \n    \n      \n        0.217\n      \n    \n    \n      \n        0.218\n      \n    \n    \n      \n        0.219\n      \n    \n    \n      \n        0.220\n      \n    \n    \n      \n        0.221\n      \n    \n    \n      \n        0.222\n      \n    \n    \n      \n        0.223\n      \n    \n    \n      \n        0.224\n      \n    \n    \n      \n        0.225\n      \n    \n    \n      \n        0.226\n      \n    \n    \n      \n        0.227\n      \n    \n    \n      \n        0.228\n      \n    \n    \n      \n        0.229\n      \n    \n    \n      \n        0.230\n      \n    \n    \n      \n        0.231\n      \n    \n    \n      \n        0.232\n      \n    \n    \n      \n        0.233\n      \n    \n    \n      \n        0.234\n      \n    \n    \n      \n        0.235\n      \n    \n    \n      \n        0.236\n      \n    \n    \n      \n        0.237\n      \n    \n    \n      \n        0.238\n      \n    \n    \n      \n        0.239\n      \n    \n    \n      \n        0.240\n      \n    \n    \n      \n        0.241\n      \n    \n    \n      \n        0.242\n      \n    \n    \n      \n        0.243\n      \n    \n    \n      \n        0.244\n      \n    \n    \n      \n        0.245\n      \n    \n    \n      \n        0.246\n      \n    \n    \n      \n        0.247\n      \n    \n    \n      \n        0.248\n      \n    \n    \n      \n        0.249\n      \n    \n    \n      \n        0.250\n      \n    \n    \n      \n        0.251\n      \n    \n    \n      \n        0.252\n      \n    \n    \n      \n        0.253\n      \n    \n    \n      \n        0.254\n      \n    \n    \n      \n        0.255\n      \n    \n    \n      \n        0.256\n      \n    \n    \n      \n        0.257\n      \n    \n    \n      \n        0.258\n      \n    \n    \n      \n        0.259\n      \n    \n    \n      \n        0.260\n      \n    \n    \n      \n        0.261\n      \n    \n    \n      \n        0.262\n      \n    \n    \n      \n        0.263\n      \n    \n    \n      \n        0.264\n      \n    \n    \n      \n        0.265\n      \n    \n    \n      \n        0.266\n      \n    \n    \n      \n        0.267\n      \n    \n    \n      \n        0.268\n      \n    \n    \n      \n        0.269\n      \n    \n    \n      \n        0.270\n      \n    \n    \n      \n        0.271\n      \n    \n    \n      \n        0.272\n      \n    \n    \n      \n        0.273\n      \n    \n    \n      \n        0.274\n      \n    \n    \n      \n        0.275\n      \n    \n    \n      \n        0.276\n      \n    \n    \n      \n        0.277\n      \n    \n    \n      \n        0.278\n      \n    \n    \n      \n        0.279\n      \n    \n    \n      \n        0.280\n      \n    \n    \n      \n        0.281\n      \n    \n    \n      \n        0.282\n      \n    \n    \n      \n        0.283\n      \n    \n    \n      \n        0.284\n      \n    \n    \n      \n        0.285\n      \n    \n    \n      \n        0.286\n      \n    \n    \n      \n        0.287\n      \n    \n    \n      \n        0.288\n      \n    \n    \n      \n        0.289\n      \n    \n    \n      \n        0.290\n      \n    \n    \n      \n        0.291\n      \n    \n    \n      \n        0.292\n      \n    \n    \n      \n        0.293\n      \n    \n    \n      \n        0.294\n      \n    \n    \n      \n        0.295\n      \n    \n    \n      \n        0.296\n      \n    \n    \n      \n        0.297\n      \n    \n    \n      \n        0.298\n      \n    \n    \n      \n        0.299\n      \n    \n    \n      \n        0.300\n      \n    \n    \n      \n        0.301\n      \n    \n    \n      \n        0.0\n      \n    \n    \n      \n        0.3\n      \n    \n  \n  \n    \n      \n        Density\n      \n    \n  \n  \n    \n      \n        Density Plot\n      \n    \n  \n\n\n  \n    \n      \n        Model\n      \n    \n  \n  \n    \n      \n        0\n      \n    \n    \n      \n        5\n      \n    \n    \n      \n        10\n      \n    \n    \n      \n        15\n      \n    \n    \n      \n        20\n      \n    \n    \n      \n        0\n      \n    \n    \n      \n        1\n      \n    \n    \n      \n        2\n      \n    \n    \n      \n        3\n      \n    \n    \n      \n        4\n      \n    \n    \n      \n        5\n      \n    \n    \n      \n        6\n      \n    \n    \n      \n        7\n      \n    \n    \n      \n        8\n      \n    \n    \n      \n        9\n      \n    \n    \n      \n        10\n      \n    \n    \n      \n        11\n      \n    \n    \n      \n        12\n      \n    \n    \n      \n        13\n      \n    \n    \n      \n        14\n      \n    \n    \n      \n        15\n      \n    \n    \n      \n        16\n      \n    \n    \n      \n        17\n      \n    \n    \n      \n        18\n      \n    \n    \n      \n        19\n      \n    \n    \n      \n        20\n      \n    \n    \n      \n        0.0\n      \n    \n    \n      \n        0.1\n      \n    \n    \n      \n        0.2\n      \n    \n    \n      \n        0.3\n      \n    \n    \n      \n        0.4\n      \n    \n    \n      \n        0.5\n      \n    \n    \n      \n        0.6\n      \n    \n    \n      \n        0.7\n      \n    \n    \n      \n        0.8\n      \n    \n    \n      \n        0.9\n      \n    \n    \n      \n        1.0\n      \n    \n    \n      \n        1.1\n      \n    \n    \n      \n        1.2\n      \n    \n    \n      \n        1.3\n      \n    \n    \n      \n        1.4\n      \n    \n    \n      \n        1.5\n      \n    \n    \n      \n        1.6\n      \n    \n    \n      \n        1.7\n      \n    \n    \n      \n        1.8\n      \n    \n    \n      \n        1.9\n      \n    \n    \n      \n        2.0\n      \n    \n    \n      \n        2.1\n      \n    \n    \n      \n        2.2\n      \n    \n    \n      \n        2.3\n      \n    \n    \n      \n        2.4\n      \n    \n    \n      \n        2.5\n      \n    \n    \n      \n        2.6\n      \n    \n    \n      \n        2.7\n      \n    \n    \n      \n        2.8\n      \n    \n    \n      \n        2.9\n      \n    \n    \n      \n        3.0\n      \n    \n    \n      \n        3.1\n      \n    \n    \n      \n        3.2\n      \n    \n    \n      \n        3.3\n      \n    \n    \n      \n        3.4\n      \n    \n    \n      \n        3.5\n      \n    \n    \n      \n        3.6\n      \n    \n    \n      \n        3.7\n      \n    \n    \n      \n        3.8\n      \n    \n    \n      \n        3.9\n      \n    \n    \n      \n        4.0\n      \n    \n    \n      \n        4.1\n      \n    \n    \n      \n        4.2\n      \n    \n    \n      \n        4.3\n      \n    \n    \n      \n        4.4\n      \n    \n    \n      \n        4.5\n      \n    \n    \n      \n        4.6\n      \n    \n    \n      \n        4.7\n      \n    \n    \n      \n        4.8\n      \n    \n    \n      \n        4.9\n      \n    \n    \n      \n        5.0\n      \n    \n    \n      \n        5.1\n      \n    \n    \n      \n        5.2\n      \n    \n    \n      \n        5.3\n      \n    \n    \n      \n        5.4\n      \n    \n    \n      \n        5.5\n      \n    \n    \n      \n        5.6\n      \n    \n    \n      \n        5.7\n      \n    \n    \n      \n        5.8\n      \n    \n    \n      \n        5.9\n      \n    \n    \n      \n        6.0\n      \n    \n    \n      \n        6.1\n      \n    \n    \n      \n        6.2\n      \n    \n    \n      \n        6.3\n      \n    \n    \n      \n        6.4\n      \n    \n    \n      \n        6.5\n      \n    \n    \n      \n        6.6\n      \n    \n    \n      \n        6.7\n      \n    \n    \n      \n        6.8\n      \n    \n    \n      \n        6.9\n      \n    \n    \n      \n        7.0\n      \n    \n    \n      \n        7.1\n      \n    \n    \n      \n        7.2\n      \n    \n    \n      \n        7.3\n      \n    \n    \n      \n        7.4\n      \n    \n    \n      \n        7.5\n      \n    \n    \n      \n        7.6\n      \n    \n    \n      \n        7.7\n      \n    \n    \n      \n        7.8\n      \n    \n    \n      \n        7.9\n      \n    \n    \n      \n        8.0\n      \n    \n    \n      \n        8.1\n      \n    \n    \n      \n        8.2\n      \n    \n    \n      \n        8.3\n      \n    \n    \n      \n        8.4\n      \n    \n    \n      \n        8.5\n      \n    \n    \n      \n        8.6\n      \n    \n    \n      \n        8.7\n      \n    \n    \n      \n        8.8\n      \n    \n    \n      \n        8.9\n      \n    \n    \n      \n        9.0\n      \n    \n    \n      \n        9.1\n      \n    \n    \n      \n        9.2\n      \n    \n    \n      \n        9.3\n      \n    \n    \n      \n        9.4\n      \n    \n    \n      \n        9.5\n      \n    \n    \n      \n        9.6\n      \n    \n    \n      \n        9.7\n      \n    \n    \n      \n        9.8\n      \n    \n    \n      \n        9.9\n      \n    \n    \n      \n        10.0\n      \n    \n    \n      \n        10.1\n      \n    \n    \n      \n        10.2\n      \n    \n    \n      \n        10.3\n      \n    \n    \n      \n        10.4\n      \n    \n    \n      \n        10.5\n      \n    \n    \n      \n        10.6\n      \n    \n    \n      \n        10.7\n      \n    \n    \n      \n        10.8\n      \n    \n    \n      \n        10.9\n      \n    \n    \n      \n        11.0\n      \n    \n    \n      \n        11.1\n      \n    \n    \n      \n        11.2\n      \n    \n    \n      \n        11.3\n      \n    \n    \n      \n        11.4\n      \n    \n    \n      \n        11.5\n      \n    \n    \n      \n        11.6\n      \n    \n    \n      \n        11.7\n      \n    \n    \n      \n        11.8\n      \n    \n    \n      \n        11.9\n      \n    \n    \n      \n        12.0\n      \n    \n    \n      \n        12.1\n      \n    \n    \n      \n        12.2\n      \n    \n    \n      \n        12.3\n      \n    \n    \n      \n        12.4\n      \n    \n    \n      \n        12.5\n      \n    \n    \n      \n        12.6\n      \n    \n    \n      \n        12.7\n      \n    \n    \n      \n        12.8\n      \n    \n    \n      \n        12.9\n      \n    \n    \n      \n        13.0\n      \n    \n    \n      \n        13.1\n      \n    \n    \n      \n        13.2\n      \n    \n    \n      \n        13.3\n      \n    \n    \n      \n        13.4\n      \n    \n    \n      \n        13.5\n      \n    \n    \n      \n        13.6\n      \n    \n    \n      \n        13.7\n      \n    \n    \n      \n        13.8\n      \n    \n    \n      \n        13.9\n      \n    \n    \n      \n        14.0\n      \n    \n    \n      \n        14.1\n      \n    \n    \n      \n        14.2\n      \n    \n    \n      \n        14.3\n      \n    \n    \n      \n        14.4\n      \n    \n    \n      \n        14.5\n      \n    \n    \n      \n        14.6\n      \n    \n    \n      \n        14.7\n      \n    \n    \n      \n        14.8\n      \n    \n    \n      \n        14.9\n      \n    \n    \n      \n        15.0\n      \n    \n    \n      \n        15.1\n      \n    \n    \n      \n        15.2\n      \n    \n    \n      \n        15.3\n      \n    \n    \n      \n        15.4\n      \n    \n    \n      \n        15.5\n      \n    \n    \n      \n        15.6\n      \n    \n    \n      \n        15.7\n      \n    \n    \n      \n        15.8\n      \n    \n    \n      \n        15.9\n      \n    \n    \n      \n        16.0\n      \n    \n    \n      \n        16.1\n      \n    \n    \n      \n        16.2\n      \n    \n    \n      \n        16.3\n      \n    \n    \n      \n        16.4\n      \n    \n    \n      \n        16.5\n      \n    \n    \n      \n        16.6\n      \n    \n    \n      \n        16.7\n      \n    \n    \n      \n        16.8\n      \n    \n    \n      \n        16.9\n      \n    \n    \n      \n        17.0\n      \n    \n    \n      \n        17.1\n      \n    \n    \n      \n        17.2\n      \n    \n    \n      \n        17.3\n      \n    \n    \n      \n        17.4\n      \n    \n    \n      \n        17.5\n      \n    \n    \n      \n        17.6\n      \n    \n    \n      \n        17.7\n      \n    \n    \n      \n        17.8\n      \n    \n    \n      \n        17.9\n      \n    \n    \n      \n        18.0\n      \n    \n    \n      \n        18.1\n      \n    \n    \n      \n        18.2\n      \n    \n    \n      \n        18.3\n      \n    \n    \n      \n        18.4\n      \n    \n    \n      \n        18.5\n      \n    \n    \n      \n        18.6\n      \n    \n    \n      \n        18.7\n      \n    \n    \n      \n        18.8\n      \n    \n    \n      \n        18.9\n      \n    \n    \n      \n        19.0\n      \n    \n    \n      \n        19.1\n      \n    \n    \n      \n        19.2\n      \n    \n    \n      \n        19.3\n      \n    \n    \n      \n        19.4\n      \n    \n    \n      \n        19.5\n      \n    \n    \n      \n        19.6\n      \n    \n    \n      \n        19.7\n      \n    \n    \n      \n        19.8\n      \n    \n    \n      \n        19.9\n      \n    \n    \n      \n        20.0\n      \n    \n    \n      \n        0\n      \n    \n    \n      \n        20\n      \n    \n  \n  \n    \n      \n        \n          \n        \n      \n      \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n      \n      \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n      \n      \n        \n          \n          \n        \n        \n          \n            \n              \n            \n          \n        \n        \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  16.15348766209949716.86\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  12.96165458145544314.63\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  11.37458643221427411.12\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  10.3546891751633469.95\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  9.6177831033539829.48\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  9.0478410273003728.83\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  8.5868919930657078.73\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  8.2021092323487138.58\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  7.873234162100728.05\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  7.5869509266993247.86\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  7.3340739926804337.07\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  7.1080200157450917.02\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  6.9039218312657696.79\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  6.7180872456744256.57\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  6.5476539626639046.5\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  6.3903615203210796.35\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  6.2443958885145246.3\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  6.1082807556092726.2\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  5.9807997159714145.87\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  5.8609394432174745.75\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  5.7478474431558345.69\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  5.6408001423035825.63\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  5.539178436665345.63\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  5.44244871342340455.62\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  5.3501479469268395.42\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  5.2618718684761315.18\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  5.1772654834576245.02\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  5.0960154011408014.97\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  5.0178435786414094.86\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4.9425021786114784.85\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4.8697693117175174.74\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4.7994454877155564.6\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4.731350638268374.6\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4.66532160428186154.58\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4.6012100030685914.53\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4.5388804079277144.45\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4.4782087860935364.35\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4.4190811514153534.28\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4.3613923962986984.26\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4.3050452738861914.25\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4.2499495065766074.22\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4.1960210010671784.19\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4.1431811533792714.18\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4.0913562299607164.18\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  4.0404768130788764.17\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.99047730042627664.1\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.9412954502317783.98\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.89287196426291843.88\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.84515010196351973.85\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.79807531962741153.73\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.7515949289871733.68\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.7056577699107223.68\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.66021389205492963.62\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.61521424032316263.6\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.57061033880314233.5\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.52635396750366863.5\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.48239682563237673.45\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.438690174315923.37\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.39518445049225553.3\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.35182884210833663.26\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.30857081260299963.22\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.2653555597569283.08\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.22212539008204723.07\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.1788189846176113.05\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.13537052473941553.04\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.09170863654505233.03\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.04775509831026662.99\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  3.0034232355229662.9\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2.95861589913922932.85\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2.91322288027198262.85\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2.86711755086619572.8\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2.82015242223872332.75\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2.77215315963188052.73\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2.7229103419263632.7\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2.67216783890205762.67\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2.61960595410762582.65\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2.56481616604423172.63\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2.5072617848363082.6\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2.44621372266964962.56\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2.3806393371276112.53\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2.30899515379674462.52\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2.22879996205890542.32\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2.1356233402838332.3\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  2.020110381463181.88\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  1.85493215739168221.66\n                \n              \n            \n          \n        \n      \n      \n        \n          \n            \n              \n            \n          \n        \n      \n      \n        \n          \n            \n              \n            \n          \n          \n            \n              \n                h,j,k,l,arrows,drag to pan\n              \n            \n            \n              \n                i,o,+,-,scroll,shift-drag to zoom\n              \n            \n            \n              \n                r,dbl-click to reset\n              \n            \n            \n              \n                c for coordinates\n              \n            \n            \n              \n                ? for help\n              \n            \n          \n        \n      \n      \n        \n          \n            \n              ?\n            \n          \n        \n      \n    \n  \n  \n    \n      \n        0\n      \n    \n    \n      \n        5\n      \n    \n    \n      \n        10\n      \n    \n    \n      \n        15\n      \n    \n    \n      \n        20\n      \n    \n    \n      \n        0\n      \n    \n    \n      \n        1\n      \n    \n    \n      \n        2\n      \n    \n    \n      \n        3\n      \n    \n    \n      \n        4\n      \n    \n    \n      \n        5\n      \n    \n    \n      \n        6\n      \n    \n    \n      \n        7\n      \n    \n    \n      \n        8\n      \n    \n    \n      \n        9\n      \n    \n    \n      \n        10\n      \n    \n    \n      \n        11\n      \n    \n    \n      \n        12\n      \n    \n    \n      \n        13\n      \n    \n    \n      \n        14\n      \n    \n    \n      \n        15\n      \n    \n    \n      \n        16\n      \n    \n    \n      \n        17\n      \n    \n    \n      \n        18\n      \n    \n    \n      \n        19\n      \n    \n    \n      \n        20\n      \n    \n    \n      \n        0.0\n      \n    \n    \n      \n        0.1\n      \n    \n    \n      \n        0.2\n      \n    \n    \n      \n        0.3\n      \n    \n    \n      \n        0.4\n      \n    \n    \n      \n        0.5\n      \n    \n    \n      \n        0.6\n      \n    \n    \n      \n        0.7\n      \n    \n    \n      \n        0.8\n      \n    \n    \n      \n        0.9\n      \n    \n    \n      \n        1.0\n      \n    \n    \n      \n        1.1\n      \n    \n    \n      \n        1.2\n      \n    \n    \n      \n        1.3\n      \n    \n    \n      \n        1.4\n      \n    \n    \n      \n        1.5\n      \n    \n    \n      \n        1.6\n      \n    \n    \n      \n        1.7\n      \n    \n    \n      \n        1.8\n      \n    \n    \n      \n        1.9\n      \n    \n    \n      \n        2.0\n      \n    \n    \n      \n        2.1\n      \n    \n    \n      \n        2.2\n      \n    \n    \n      \n        2.3\n      \n    \n    \n      \n        2.4\n      \n    \n    \n      \n        2.5\n      \n    \n    \n      \n        2.6\n      \n    \n    \n      \n        2.7\n      \n    \n    \n      \n        2.8\n      \n    \n    \n      \n        2.9\n      \n    \n    \n      \n        3.0\n      \n    \n    \n      \n        3.1\n      \n    \n    \n      \n        3.2\n      \n    \n    \n      \n        3.3\n      \n    \n    \n      \n        3.4\n      \n    \n    \n      \n        3.5\n      \n    \n    \n      \n        3.6\n      \n    \n    \n      \n        3.7\n      \n    \n    \n      \n        3.8\n      \n    \n    \n      \n        3.9\n      \n    \n    \n      \n        4.0\n      \n    \n    \n      \n        4.1\n      \n    \n    \n      \n        4.2\n      \n    \n    \n      \n        4.3\n      \n    \n    \n      \n        4.4\n      \n    \n    \n      \n        4.5\n      \n    \n    \n      \n        4.6\n      \n    \n    \n      \n        4.7\n      \n    \n    \n      \n        4.8\n      \n    \n    \n      \n        4.9\n      \n    \n    \n      \n        5.0\n      \n    \n    \n      \n        5.1\n      \n    \n    \n      \n        5.2\n      \n    \n    \n      \n        5.3\n      \n    \n    \n      \n        5.4\n      \n    \n    \n      \n        5.5\n      \n    \n    \n      \n        5.6\n      \n    \n    \n      \n        5.7\n      \n    \n    \n      \n        5.8\n      \n    \n    \n      \n        5.9\n      \n    \n    \n      \n        6.0\n      \n    \n    \n      \n        6.1\n      \n    \n    \n      \n        6.2\n      \n    \n    \n      \n        6.3\n      \n    \n    \n      \n        6.4\n      \n    \n    \n      \n        6.5\n      \n    \n    \n      \n        6.6\n      \n    \n    \n      \n        6.7\n      \n    \n    \n      \n        6.8\n      \n    \n    \n      \n        6.9\n      \n    \n    \n      \n        7.0\n      \n    \n    \n      \n        7.1\n      \n    \n    \n      \n        7.2\n      \n    \n    \n      \n        7.3\n      \n    \n    \n      \n        7.4\n      \n    \n    \n      \n        7.5\n      \n    \n    \n      \n        7.6\n      \n    \n    \n      \n        7.7\n      \n    \n    \n      \n        7.8\n      \n    \n    \n      \n        7.9\n      \n    \n    \n      \n        8.0\n      \n    \n    \n      \n        8.1\n      \n    \n    \n      \n        8.2\n      \n    \n    \n      \n        8.3\n      \n    \n    \n      \n        8.4\n      \n    \n    \n      \n        8.5\n      \n    \n    \n      \n        8.6\n      \n    \n    \n      \n        8.7\n      \n    \n    \n      \n        8.8\n      \n    \n    \n      \n        8.9\n      \n    \n    \n      \n        9.0\n      \n    \n    \n      \n        9.1\n      \n    \n    \n      \n        9.2\n      \n    \n    \n      \n        9.3\n      \n    \n    \n      \n        9.4\n      \n    \n    \n      \n        9.5\n      \n    \n    \n      \n        9.6\n      \n    \n    \n      \n        9.7\n      \n    \n    \n      \n        9.8\n      \n    \n    \n      \n        9.9\n      \n    \n    \n      \n        10.0\n      \n    \n    \n      \n        10.1\n      \n    \n    \n      \n        10.2\n      \n    \n    \n      \n        10.3\n      \n    \n    \n      \n        10.4\n      \n    \n    \n      \n        10.5\n      \n    \n    \n      \n        10.6\n      \n    \n    \n      \n        10.7\n      \n    \n    \n      \n        10.8\n      \n    \n    \n      \n        10.9\n      \n    \n    \n      \n        11.0\n      \n    \n    \n      \n        11.1\n      \n    \n    \n      \n        11.2\n      \n    \n    \n      \n        11.3\n      \n    \n    \n      \n        11.4\n      \n    \n    \n      \n        11.5\n      \n    \n    \n      \n        11.6\n      \n    \n    \n      \n        11.7\n      \n    \n    \n      \n        11.8\n      \n    \n    \n      \n        11.9\n      \n    \n    \n      \n        12.0\n      \n    \n    \n      \n        12.1\n      \n    \n    \n      \n        12.2\n      \n    \n    \n      \n        12.3\n      \n    \n    \n      \n        12.4\n      \n    \n    \n      \n        12.5\n      \n    \n    \n      \n        12.6\n      \n    \n    \n      \n        12.7\n      \n    \n    \n      \n        12.8\n      \n    \n    \n      \n        12.9\n      \n    \n    \n      \n        13.0\n      \n    \n    \n      \n        13.1\n      \n    \n    \n      \n        13.2\n      \n    \n    \n      \n        13.3\n      \n    \n    \n      \n        13.4\n      \n    \n    \n      \n        13.5\n      \n    \n    \n      \n        13.6\n      \n    \n    \n      \n        13.7\n      \n    \n    \n      \n        13.8\n      \n    \n    \n      \n        13.9\n      \n    \n    \n      \n        14.0\n      \n    \n    \n      \n        14.1\n      \n    \n    \n      \n        14.2\n      \n    \n    \n      \n        14.3\n      \n    \n    \n      \n        14.4\n      \n    \n    \n      \n        14.5\n      \n    \n    \n      \n        14.6\n      \n    \n    \n      \n        14.7\n      \n    \n    \n      \n        14.8\n      \n    \n    \n      \n        14.9\n      \n    \n    \n      \n        15.0\n      \n    \n    \n      \n        15.1\n      \n    \n    \n      \n        15.2\n      \n    \n    \n      \n        15.3\n      \n    \n    \n      \n        15.4\n      \n    \n    \n      \n        15.5\n      \n    \n    \n      \n        15.6\n      \n    \n    \n      \n        15.7\n      \n    \n    \n      \n        15.8\n      \n    \n    \n      \n        15.9\n      \n    \n    \n      \n        16.0\n      \n    \n    \n      \n        16.1\n      \n    \n    \n      \n        16.2\n      \n    \n    \n      \n        16.3\n      \n    \n    \n      \n        16.4\n      \n    \n    \n      \n        16.5\n      \n    \n    \n      \n        16.6\n      \n    \n    \n      \n        16.7\n      \n    \n    \n      \n        16.8\n      \n    \n    \n      \n        16.9\n      \n    \n    \n      \n        17.0\n      \n    \n    \n      \n        17.1\n      \n    \n    \n      \n        17.2\n      \n    \n    \n      \n        17.3\n      \n    \n    \n      \n        17.4\n      \n    \n    \n      \n        17.5\n      \n    \n    \n      \n        17.6\n      \n    \n    \n      \n        17.7\n      \n    \n    \n      \n        17.8\n      \n    \n    \n      \n        17.9\n      \n    \n    \n      \n        18.0\n      \n    \n    \n      \n        18.1\n      \n    \n    \n      \n        18.2\n      \n    \n    \n      \n        18.3\n      \n    \n    \n      \n        18.4\n      \n    \n    \n      \n        18.5\n      \n    \n    \n      \n        18.6\n      \n    \n    \n      \n        18.7\n      \n    \n    \n      \n        18.8\n      \n    \n    \n      \n        18.9\n      \n    \n    \n      \n        19.0\n      \n    \n    \n      \n        19.1\n      \n    \n    \n      \n        19.2\n      \n    \n    \n      \n        19.3\n      \n    \n    \n      \n        19.4\n      \n    \n    \n      \n        19.5\n      \n    \n    \n      \n        19.6\n      \n    \n    \n      \n        19.7\n      \n    \n    \n      \n        19.8\n      \n    \n    \n      \n        19.9\n      \n    \n    \n      \n        20.0\n      \n    \n    \n      \n        0\n      \n    \n    \n      \n        20\n      \n    \n  \n  \n    \n      \n        Empirical\n      \n    \n  \n  \n    \n      \n        Quantile Plot\n      \n    \n  \n\n\n  \n    \n      \n        Model\n      \n    \n  \n  \n    \n      \n        0.0\n      \n    \n    \n      \n        0.5\n      \n    \n    \n      \n        1.0\n      \n    \n    \n      \n        0.00\n      \n    \n    \n      \n        0.05\n      \n    \n    \n      \n        0.10\n      \n    \n    \n      \n        0.15\n      \n    \n    \n      \n        0.20\n      \n    \n    \n      \n        0.25\n      \n    \n    \n      \n        0.30\n      \n    \n    \n      \n        0.35\n      \n    \n    \n      \n        0.40\n      \n    \n    \n      \n        0.45\n      \n    \n    \n      \n        0.50\n      \n    \n    \n      \n        0.55\n      \n    \n    \n      \n        0.60\n      \n    \n    \n      \n        0.65\n      \n    \n    \n      \n        0.70\n      \n    \n    \n      \n        0.75\n      \n    \n    \n      \n        0.80\n      \n    \n    \n      \n        0.85\n      \n    \n    \n      \n        0.90\n      \n    \n    \n      \n        0.95\n      \n    \n    \n      \n        1.00\n      \n    \n    \n      \n        0.000\n      \n    \n    \n      \n        0.005\n      \n    \n    \n      \n        0.010\n      \n    \n    \n      \n        0.015\n      \n    \n    \n      \n        0.020\n      \n    \n    \n      \n        0.025\n      \n    \n    \n      \n        0.030\n      \n    \n    \n      \n        0.035\n      \n    \n    \n      \n        0.040\n      \n    \n    \n      \n        0.045\n      \n    \n    \n      \n        0.050\n      \n    \n    \n      \n        0.055\n      \n    \n    \n      \n        0.060\n      \n    \n    \n      \n        0.065\n      \n    \n    \n      \n        0.070\n      \n    \n    \n      \n        0.075\n      \n    \n    \n      \n        0.080\n      \n    \n    \n      \n        0.085\n      \n    \n    \n      \n        0.090\n      \n    \n    \n      \n        0.095\n      \n    \n    \n      \n        0.100\n      \n    \n    \n      \n        0.105\n      \n    \n    \n      \n        0.110\n      \n    \n    \n      \n        0.115\n      \n    \n    \n      \n        0.120\n      \n    \n    \n      \n        0.125\n      \n    \n    \n      \n        0.130\n      \n    \n    \n      \n        0.135\n      \n    \n    \n      \n        0.140\n      \n    \n    \n      \n        0.145\n      \n    \n    \n      \n        0.150\n      \n    \n    \n      \n        0.155\n      \n    \n    \n      \n        0.160\n      \n    \n    \n      \n        0.165\n      \n    \n    \n      \n        0.170\n      \n    \n    \n      \n        0.175\n      \n    \n    \n      \n        0.180\n      \n    \n    \n      \n        0.185\n      \n    \n    \n      \n        0.190\n      \n    \n    \n      \n        0.195\n      \n    \n    \n      \n        0.200\n      \n    \n    \n      \n        0.205\n      \n    \n    \n      \n        0.210\n      \n    \n    \n      \n        0.215\n      \n    \n    \n      \n        0.220\n      \n    \n    \n      \n        0.225\n      \n    \n    \n      \n        0.230\n      \n    \n    \n      \n        0.235\n      \n    \n    \n      \n        0.240\n      \n    \n    \n      \n        0.245\n      \n    \n    \n      \n        0.250\n      \n    \n    \n      \n        0.255\n      \n    \n    \n      \n        0.260\n      \n    \n    \n      \n        0.265\n      \n    \n    \n      \n        0.270\n      \n    \n    \n      \n        0.275\n      \n    \n    \n      \n        0.280\n      \n    \n    \n      \n        0.285\n      \n    \n    \n      \n        0.290\n      \n    \n    \n      \n        0.295\n      \n    \n    \n      \n        0.300\n      \n    \n    \n      \n        0.305\n      \n    \n    \n      \n        0.310\n      \n    \n    \n      \n        0.315\n      \n    \n    \n      \n        0.320\n      \n    \n    \n      \n        0.325\n      \n    \n    \n      \n        0.330\n      \n    \n    \n      \n        0.335\n      \n    \n    \n      \n        0.340\n      \n    \n    \n      \n        0.345\n      \n    \n    \n      \n        0.350\n      \n    \n    \n      \n        0.355\n      \n    \n    \n      \n        0.360\n      \n    \n    \n      \n        0.365\n      \n    \n    \n      \n        0.370\n      \n    \n    \n      \n        0.375\n      \n    \n    \n      \n        0.380\n      \n    \n    \n      \n        0.385\n      \n    \n    \n      \n        0.390\n      \n    \n    \n      \n        0.395\n      \n    \n    \n      \n        0.400\n      \n    \n    \n      \n        0.405\n      \n    \n    \n      \n        0.410\n      \n    \n    \n      \n        0.415\n      \n    \n    \n      \n        0.420\n      \n    \n    \n      \n        0.425\n      \n    \n    \n      \n        0.430\n      \n    \n    \n      \n        0.435\n      \n    \n    \n      \n        0.440\n      \n    \n    \n      \n        0.445\n      \n    \n    \n      \n        0.450\n      \n    \n    \n      \n        0.455\n      \n    \n    \n      \n        0.460\n      \n    \n    \n      \n        0.465\n      \n    \n    \n      \n        0.470\n      \n    \n    \n      \n        0.475\n      \n    \n    \n      \n        0.480\n      \n    \n    \n      \n        0.485\n      \n    \n    \n      \n        0.490\n      \n    \n    \n      \n        0.495\n      \n    \n    \n      \n        0.500\n      \n    \n    \n      \n        0.505\n      \n    \n    \n      \n        0.510\n      \n    \n    \n      \n        0.515\n      \n    \n    \n      \n        0.520\n      \n    \n    \n      \n        0.525\n      \n    \n    \n      \n        0.530\n      \n    \n    \n      \n        0.535\n      \n    \n    \n      \n        0.540\n      \n    \n    \n      \n        0.545\n      \n    \n    \n      \n        0.550\n      \n    \n    \n      \n        0.555\n      \n    \n    \n      \n        0.560\n      \n    \n    \n      \n        0.565\n      \n    \n    \n      \n        0.570\n      \n    \n    \n      \n        0.575\n      \n    \n    \n      \n        0.580\n      \n    \n    \n      \n        0.585\n      \n    \n    \n      \n        0.590\n      \n    \n    \n      \n        0.595\n      \n    \n    \n      \n        0.600\n      \n    \n    \n      \n        0.605\n      \n    \n    \n      \n        0.610\n      \n    \n    \n      \n        0.615\n      \n    \n    \n      \n        0.620\n      \n    \n    \n      \n        0.625\n      \n    \n    \n      \n        0.630\n      \n    \n    \n      \n        0.635\n      \n    \n    \n      \n        0.640\n      \n    \n    \n      \n        0.645\n      \n    \n    \n      \n        0.650\n      \n    \n    \n      \n        0.655\n      \n    \n    \n      \n        0.660\n      \n    \n    \n      \n        0.665\n      \n    \n    \n      \n        0.670\n      \n    \n    \n      \n        0.675\n      \n    \n    \n      \n        0.680\n      \n    \n    \n      \n        0.685\n      \n    \n    \n      \n        0.690\n      \n    \n    \n      \n        0.695\n      \n    \n    \n      \n        0.700\n      \n    \n    \n      \n        0.705\n      \n    \n    \n      \n        0.710\n      \n    \n    \n      \n        0.715\n      \n    \n    \n      \n        0.720\n      \n    \n    \n      \n        0.725\n      \n    \n    \n      \n        0.730\n      \n    \n    \n      \n        0.735\n      \n    \n    \n      \n        0.740\n      \n    \n    \n      \n        0.745\n      \n    \n    \n      \n        0.750\n      \n    \n    \n      \n        0.755\n      \n    \n    \n      \n        0.760\n      \n    \n    \n      \n        0.765\n      \n    \n    \n      \n        0.770\n      \n    \n    \n      \n        0.775\n      \n    \n    \n      \n        0.780\n      \n    \n    \n      \n        0.785\n      \n    \n    \n      \n        0.790\n      \n    \n    \n      \n        0.795\n      \n    \n    \n      \n        0.800\n      \n    \n    \n      \n        0.805\n      \n    \n    \n      \n        0.810\n      \n    \n    \n      \n        0.815\n      \n    \n    \n      \n        0.820\n      \n    \n    \n      \n        0.825\n      \n    \n    \n      \n        0.830\n      \n    \n    \n      \n        0.835\n      \n    \n    \n      \n        0.840\n      \n    \n    \n      \n        0.845\n      \n    \n    \n      \n        0.850\n      \n    \n    \n      \n        0.855\n      \n    \n    \n      \n        0.860\n      \n    \n    \n      \n        0.865\n      \n    \n    \n      \n        0.870\n      \n    \n    \n      \n        0.875\n      \n    \n    \n      \n        0.880\n      \n    \n    \n      \n        0.885\n      \n    \n    \n      \n        0.890\n      \n    \n    \n      \n        0.895\n      \n    \n    \n      \n        0.900\n      \n    \n    \n      \n        0.905\n      \n    \n    \n      \n        0.910\n      \n    \n    \n      \n        0.915\n      \n    \n    \n      \n        0.920\n      \n    \n    \n      \n        0.925\n      \n    \n    \n      \n        0.930\n      \n    \n    \n      \n        0.935\n      \n    \n    \n      \n        0.940\n      \n    \n    \n      \n        0.945\n      \n    \n    \n      \n        0.950\n      \n    \n    \n      \n        0.955\n      \n    \n    \n      \n        0.960\n      \n    \n    \n      \n        0.965\n      \n    \n    \n      \n        0.970\n      \n    \n    \n      \n        0.975\n      \n    \n    \n      \n        0.980\n      \n    \n    \n      \n        0.985\n      \n    \n    \n      \n        0.990\n      \n    \n    \n      \n        0.995\n      \n    \n    \n      \n        1.000\n      \n    \n    \n      \n        0\n      \n    \n    \n      \n        1\n      \n    \n  \n  \n    \n      \n        \n          \n        \n      \n      \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n      \n      \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n        \n          \n        \n      \n      \n        \n          \n          \n        \n        \n          \n            \n              \n            \n          \n        \n        \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.98967559243492530.9883720930232558\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.98407312824275870.9767441860465116\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.96306353740975530.9651162790697675\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.94821716461897480.9534883720930233\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.94009442577372620.9418604651162791\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.92595871243092850.9302325581395349\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.92342041599021730.9186046511627907\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.9194025295916060.9069767441860465\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.9028924369371940.8953488372093024\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.89595419107518030.8837209302325582\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.8595738835795220.872093023255814\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.85678190564000820.8604651162790697\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.84303554372622850.8488372093023255\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.8283683153386910.8372093023255814\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.82336026283044670.8255813953488372\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.81202944964050830.813953488372093\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.8080621189616240.8023255813953488\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.79982656637996330.7906976744186046\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.7695545976690370.7790697674418605\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.75725396811524330.7674418604651163\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.75082259372960060.7558139534883721\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.7441964541730840.7441860465116279\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.7441964541730840.7325581395348837\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.74307276622982820.7209302325581395\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.71938888610203780.7093023255813954\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.68771108355968880.6976744186046512\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.6644441223974680.686046511627907\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.65679961195599890.6744186046511628\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.63933143286382620.6627906976744186\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.6376982483497630.6511627906976745\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.61922464862120740.6395348837209303\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.59433265902445920.627906976744186\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.59433265902445920.6162790697674418\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.59064778237978470.6046511627906976\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.58129269772366590.5930232558139535\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.56589670346352080.5813953488372093\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.5459048233016650.5697674418604651\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.53141443505362980.5581395348837209\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.52719929256446960.5465116279069767\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.52507923593857630.5348837209302325\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.51866920540677320.5232558139534884\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.51218456806076310.5116279069767442\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.510006492650670.5\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.510006492650670.4883720930232558\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.50782017338768830.47674418604651164\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.4922864242587160.46511627906976744\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.464739001972302950.45348837209302323\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.44092954161341380.4418604651162791\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.433642209232127540.43023255813953487\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.40386804952212070.4186046511627907\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.391186708781732230.4069767441860465\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.391186708781732230.3953488372093023\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.375775907958248830.38372093023255816\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.370595418839957560.37209302325581395\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.344404947789251460.36046511627906974\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.344404947789251460.3488372093023256\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.33115536670153840.3372093023255814\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.30979632189817720.32558139534883723\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.29100036627298330.313953488372093\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.28023898899918060.3023255813953488\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.26947745160260350.29069767441860467\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.232002329805032550.27906976744186046\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.229347444821168980.26744186046511625\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.224050461539215120.2558139534883721\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.221408859287325040.2441860465116279\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.218772184677934860.23255813953488372\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.2082798770036840.22093023255813954\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.18506241153164380.20930232558139536\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.172454647678992550.19767441860465115\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.172454647678992550.18604651162790697\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.16009933212260420.1744186046511628\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.148032613714149640.16279069767441862\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.14329490169997590.1511627906976744\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.136290967854083760.13953488372093023\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.129417176970771680.12790697674418605\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.124910802834650590.11627906976744186\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.12046801912526150.10465116279069768\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.113928006070569380.09302325581395349\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.105451343871516240.08139534883720931\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.09928573683295180.06976744186046512\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.097268466371047760.05813953488372093\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.061288035219081530.046511627906976744\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.058179236211390.03488372093023256\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.0144381777016325070.023255813953488372\n                \n              \n            \n          \n          \n            \n              \n                \n              \n            \n            \n              \n                \n                  0.0054201120951420370.011627906976744186\n                \n              \n            \n          \n        \n      \n      \n        \n          \n            \n              \n            \n          \n        \n      \n      \n        \n          \n            \n              \n            \n          \n          \n            \n              \n                h,j,k,l,arrows,drag to pan\n              \n            \n            \n              \n                i,o,+,-,scroll,shift-drag to zoom\n              \n            \n            \n              \n                r,dbl-click to reset\n              \n            \n            \n              \n                c for coordinates\n              \n            \n            \n              \n                ? for help\n              \n            \n          \n        \n      \n      \n        \n          \n            \n              ?\n            \n          \n        \n      \n    \n  \n  \n    \n      \n        0.0\n      \n    \n    \n      \n        0.5\n      \n    \n    \n      \n        1.0\n      \n    \n    \n      \n        0.00\n      \n    \n    \n      \n        0.05\n      \n    \n    \n      \n        0.10\n      \n    \n    \n      \n        0.15\n      \n    \n    \n      \n        0.20\n      \n    \n    \n      \n        0.25\n      \n    \n    \n      \n        0.30\n      \n    \n    \n      \n        0.35\n      \n    \n    \n      \n        0.40\n      \n    \n    \n      \n        0.45\n      \n    \n    \n      \n        0.50\n      \n    \n    \n      \n        0.55\n      \n    \n    \n      \n        0.60\n      \n    \n    \n      \n        0.65\n      \n    \n    \n      \n        0.70\n      \n    \n    \n      \n        0.75\n      \n    \n    \n      \n        0.80\n      \n    \n    \n      \n        0.85\n      \n    \n    \n      \n        0.90\n      \n    \n    \n      \n        0.95\n      \n    \n    \n      \n        1.00\n      \n    \n    \n      \n        0.000\n      \n    \n    \n      \n        0.005\n      \n    \n    \n      \n        0.010\n      \n    \n    \n      \n        0.015\n      \n    \n    \n      \n        0.020\n      \n    \n    \n      \n        0.025\n      \n    \n    \n      \n        0.030\n      \n    \n    \n      \n        0.035\n      \n    \n    \n      \n        0.040\n      \n    \n    \n      \n        0.045\n      \n    \n    \n      \n        0.050\n      \n    \n    \n      \n        0.055\n      \n    \n    \n      \n        0.060\n      \n    \n    \n      \n        0.065\n      \n    \n    \n      \n        0.070\n      \n    \n    \n      \n        0.075\n      \n    \n    \n      \n        0.080\n      \n    \n    \n      \n        0.085\n      \n    \n    \n      \n        0.090\n      \n    \n    \n      \n        0.095\n      \n    \n    \n      \n        0.100\n      \n    \n    \n      \n        0.105\n      \n    \n    \n      \n        0.110\n      \n    \n    \n      \n        0.115\n      \n    \n    \n      \n        0.120\n      \n    \n    \n      \n        0.125\n      \n    \n    \n      \n        0.130\n      \n    \n    \n      \n        0.135\n      \n    \n    \n      \n        0.140\n      \n    \n    \n      \n        0.145\n      \n    \n    \n      \n        0.150\n      \n    \n    \n      \n        0.155\n      \n    \n    \n      \n        0.160\n      \n    \n    \n      \n        0.165\n      \n    \n    \n      \n        0.170\n      \n    \n    \n      \n        0.175\n      \n    \n    \n      \n        0.180\n      \n    \n    \n      \n        0.185\n      \n    \n    \n      \n        0.190\n      \n    \n    \n      \n        0.195\n      \n    \n    \n      \n        0.200\n      \n    \n    \n      \n        0.205\n      \n    \n    \n      \n        0.210\n      \n    \n    \n      \n        0.215\n      \n    \n    \n      \n        0.220\n      \n    \n    \n      \n        0.225\n      \n    \n    \n      \n        0.230\n      \n    \n    \n      \n        0.235\n      \n    \n    \n      \n        0.240\n      \n    \n    \n      \n        0.245\n      \n    \n    \n      \n        0.250\n      \n    \n    \n      \n        0.255\n      \n    \n    \n      \n        0.260\n      \n    \n    \n      \n        0.265\n      \n    \n    \n      \n        0.270\n      \n    \n    \n      \n        0.275\n      \n    \n    \n      \n        0.280\n      \n    \n    \n      \n        0.285\n      \n    \n    \n      \n        0.290\n      \n    \n    \n      \n        0.295\n      \n    \n    \n      \n        0.300\n      \n    \n    \n      \n        0.305\n      \n    \n    \n      \n        0.310\n      \n    \n    \n      \n        0.315\n      \n    \n    \n      \n        0.320\n      \n    \n    \n      \n        0.325\n      \n    \n    \n      \n        0.330\n      \n    \n    \n      \n        0.335\n      \n    \n    \n      \n        0.340\n      \n    \n    \n      \n        0.345\n      \n    \n    \n      \n        0.350\n      \n    \n    \n      \n        0.355\n      \n    \n    \n      \n        0.360\n      \n    \n    \n      \n        0.365\n      \n    \n    \n      \n        0.370\n      \n    \n    \n      \n        0.375\n      \n    \n    \n      \n        0.380\n      \n    \n    \n      \n        0.385\n      \n    \n    \n      \n        0.390\n      \n    \n    \n      \n        0.395\n      \n    \n    \n      \n        0.400\n      \n    \n    \n      \n        0.405\n      \n    \n    \n      \n        0.410\n      \n    \n    \n      \n        0.415\n      \n    \n    \n      \n        0.420\n      \n    \n    \n      \n        0.425\n      \n    \n    \n      \n        0.430\n      \n    \n    \n      \n        0.435\n      \n    \n    \n      \n        0.440\n      \n    \n    \n      \n        0.445\n      \n    \n    \n      \n        0.450\n      \n    \n    \n      \n        0.455\n      \n    \n    \n      \n        0.460\n      \n    \n    \n      \n        0.465\n      \n    \n    \n      \n        0.470\n      \n    \n    \n      \n        0.475\n      \n    \n    \n      \n        0.480\n      \n    \n    \n      \n        0.485\n      \n    \n    \n      \n        0.490\n      \n    \n    \n      \n        0.495\n      \n    \n    \n      \n        0.500\n      \n    \n    \n      \n        0.505\n      \n    \n    \n      \n        0.510\n      \n    \n    \n      \n        0.515\n      \n    \n    \n      \n        0.520\n      \n    \n    \n      \n        0.525\n      \n    \n    \n      \n        0.530\n      \n    \n    \n      \n        0.535\n      \n    \n    \n      \n        0.540\n      \n    \n    \n      \n        0.545\n      \n    \n    \n      \n        0.550\n      \n    \n    \n      \n        0.555\n      \n    \n    \n      \n        0.560\n      \n    \n    \n      \n        0.565\n      \n    \n    \n      \n        0.570\n      \n    \n    \n      \n        0.575\n      \n    \n    \n      \n        0.580\n      \n    \n    \n      \n        0.585\n      \n    \n    \n      \n        0.590\n      \n    \n    \n      \n        0.595\n      \n    \n    \n      \n        0.600\n      \n    \n    \n      \n        0.605\n      \n    \n    \n      \n        0.610\n      \n    \n    \n      \n        0.615\n      \n    \n    \n      \n        0.620\n      \n    \n    \n      \n        0.625\n      \n    \n    \n      \n        0.630\n      \n    \n    \n      \n        0.635\n      \n    \n    \n      \n        0.640\n      \n    \n    \n      \n        0.645\n      \n    \n    \n      \n        0.650\n      \n    \n    \n      \n        0.655\n      \n    \n    \n      \n        0.660\n      \n    \n    \n      \n        0.665\n      \n    \n    \n      \n        0.670\n      \n    \n    \n      \n        0.675\n      \n    \n    \n      \n        0.680\n      \n    \n    \n      \n        0.685\n      \n    \n    \n      \n        0.690\n      \n    \n    \n      \n        0.695\n      \n    \n    \n      \n        0.700\n      \n    \n    \n      \n        0.705\n      \n    \n    \n      \n        0.710\n      \n    \n    \n      \n        0.715\n      \n    \n    \n      \n        0.720\n      \n    \n    \n      \n        0.725\n      \n    \n    \n      \n        0.730\n      \n    \n    \n      \n        0.735\n      \n    \n    \n      \n        0.740\n      \n    \n    \n      \n        0.745\n      \n    \n    \n      \n        0.750\n      \n    \n    \n      \n        0.755\n      \n    \n    \n      \n        0.760\n      \n    \n    \n      \n        0.765\n      \n    \n    \n      \n        0.770\n      \n    \n    \n      \n        0.775\n      \n    \n    \n      \n        0.780\n      \n    \n    \n      \n        0.785\n      \n    \n    \n      \n        0.790\n      \n    \n    \n      \n        0.795\n      \n    \n    \n      \n        0.800\n      \n    \n    \n      \n        0.805\n      \n    \n    \n      \n        0.810\n      \n    \n    \n      \n        0.815\n      \n    \n    \n      \n        0.820\n      \n    \n    \n      \n        0.825\n      \n    \n    \n      \n        0.830\n      \n    \n    \n      \n        0.835\n      \n    \n    \n      \n        0.840\n      \n    \n    \n      \n        0.845\n      \n    \n    \n      \n        0.850\n      \n    \n    \n      \n        0.855\n      \n    \n    \n      \n        0.860\n      \n    \n    \n      \n        0.865\n      \n    \n    \n      \n        0.870\n      \n    \n    \n      \n        0.875\n      \n    \n    \n      \n        0.880\n      \n    \n    \n      \n        0.885\n      \n    \n    \n      \n        0.890\n      \n    \n    \n      \n        0.895\n      \n    \n    \n      \n        0.900\n      \n    \n    \n      \n        0.905\n      \n    \n    \n      \n        0.910\n      \n    \n    \n      \n        0.915\n      \n    \n    \n      \n        0.920\n      \n    \n    \n      \n        0.925\n      \n    \n    \n      \n        0.930\n      \n    \n    \n      \n        0.935\n      \n    \n    \n      \n        0.940\n      \n    \n    \n      \n        0.945\n      \n    \n    \n      \n        0.950\n      \n    \n    \n      \n        0.955\n      \n    \n    \n      \n        0.960\n      \n    \n    \n      \n        0.965\n      \n    \n    \n      \n        0.970\n      \n    \n    \n      \n        0.975\n      \n    \n    \n      \n        0.980\n      \n    \n    \n      \n        0.985\n      \n    \n    \n      \n        0.990\n      \n    \n    \n      \n        0.995\n      \n    \n    \n      \n        1.000\n      \n    \n    \n      \n        0\n      \n    \n    \n      \n        1\n      \n    \n  \n  \n    \n      \n        Empirical\n      \n    \n  \n  \n    \n      \n        Probability Plot"
  },
  {
    "objectID": "slides/2023-11-15-model-selection.html#calibration-histogram",
    "href": "slides/2023-11-15-model-selection.html#calibration-histogram",
    "title": "Quantitative and Graphical Model Selection",
    "section": "Calibration histogram",
    "text": "Calibration histogram\nSometimes we want to summarize calibration across a large number of sites at once\n\nWhat we plot: a histogram, where each observation is the observed quantile of the data, given the (conditional) GEV at that location / year\nIdeal case: a uniform distribution\nWarnings: systematic deviations from uniformity\nLimitations: aggregating over sites can hide local issues"
  },
  {
    "objectID": "slides/2023-11-15-model-selection.html#credit",
    "href": "slides/2023-11-15-model-selection.html#credit",
    "title": "Quantitative and Graphical Model Selection",
    "section": "Credit",
    "text": "Credit\nThis content borrows heavily from a literature review that I developed with Vivek Srikrishnan.\nFor a more accessible discussion, see Chapter 7 of McElreath (2020). For a more technical discussion, see Piironen & Vehtari (2017)."
  },
  {
    "objectID": "slides/2023-11-15-model-selection.html#the-challenge",
    "href": "slides/2023-11-15-model-selection.html#the-challenge",
    "title": "Quantitative and Graphical Model Selection",
    "section": "The challenge",
    "text": "The challenge\nWe want to make probabilistic predictions about unobserved data \\(\\tilde{y}\\). This is hard because Earth systems are:\n\nhigh-dimensional\nmulti-scale\nnonlinear / complex\n\nTo approximate the true system, we come up with a model space \\(\\mathcal{M}\\) defining a family of candidate models, then use them to make predictions."
  },
  {
    "objectID": "slides/2023-11-15-model-selection.html#note",
    "href": "slides/2023-11-15-model-selection.html#note",
    "title": "Quantitative and Graphical Model Selection",
    "section": "Note",
    "text": "Note\nThis content gets fairly technical. You will not be tested on the equations, but you should understand the key points (which we will recap)."
  },
  {
    "objectID": "slides/2023-11-15-model-selection.html#how-similar-are-two-distributions",
    "href": "slides/2023-11-15-model-selection.html#how-similar-are-two-distributions",
    "title": "Quantitative and Graphical Model Selection",
    "section": "How similar are two distributions?",
    "text": "How similar are two distributions?\n\\[\nD_\\text{KL} (P \\parallel Q) = \\sum_{x \\in \\mathcal{X}} P(x) \\log \\left[ \\frac{P(x)}{Q(x)} \\right]\n\\]\nOne interpretation of \\(D_\\text{KL} (P \\parallel Q)\\) is the measure of information gained by revising one’s beliefes from the prior distribution \\(Q\\) to the posterior distribution \\(P\\). Another interpretation is the amount of information lost when \\(Q\\) is used to approximate \\(P\\). Note that for continuous RVs the above sum can be written as an integral."
  },
  {
    "objectID": "slides/2023-11-15-model-selection.html#measures-of-predictive-accuracy",
    "href": "slides/2023-11-15-model-selection.html#measures-of-predictive-accuracy",
    "title": "Quantitative and Graphical Model Selection",
    "section": "Measures of predictive accuracy",
    "text": "Measures of predictive accuracy\nPredictive performance of a model defined in terms of a utility function \\(u(M, \\tilde{y})\\). Commonly used: log predictive density: \\[\n\\log p(\\tilde{y} | D, M).\n\\] Future observations \\(\\tilde{y}\\) are unknown, so we must approach it in expectation: \\[\n\\overline{u}(M) = \\mathbb{E}\\left[ \\log p(\\tilde{y} | D, M) \\right] = \\int p_t(\\tilde{y}) \\log [(\\tilde{y} | D, M) d\\tilde{y}\n\\] where \\(p_t(\\tilde{y})\\) is the true data generating distribution (unknown!)\n\nMaximizing \\(\\overline{u}(M)\\) is equivalent to minimizing KL divergence from candidate model \\(p(\\tilde{y} | D, M)\\) to true data distribution \\(p_t(\\tilde{y})\\)"
  },
  {
    "objectID": "slides/2023-11-15-model-selection.html#in-practice-we-work-with-estimates",
    "href": "slides/2023-11-15-model-selection.html#in-practice-we-work-with-estimates",
    "title": "Quantitative and Graphical Model Selection",
    "section": "In practice we work with estimates",
    "text": "In practice we work with estimates\nWe don’t know the true distribution \\(\\theta\\) so we have to approximate it. The log pointwise predictive density is \\[\n\\begin{align}\n\\text{lppd} &= \\log \\prod_{i=1}^N p_\\text{post}(y_i) = \\sum_{i=1}^N \\log \\int p(y_i | \\theta) p_\\text{post} (\\theta) d \\theta \\\\\n&\\approx \\sum_{i=1}^N \\log \\left[ \\frac{1}{S} \\sum_{i=1}^S p(y_i | \\theta^s) \\right]\n\\end{align}\n\\] where we have approximated the posterior with \\(S\\) simulations from the posterior (eg, using MCMC).\n\nthe LPPD of observed data \\(y\\) is an overestimate of the expected LPPD for future data. Thus tools will start with our approximate form and then derive some correction."
  },
  {
    "objectID": "slides/2023-11-15-model-selection.html#model-combination",
    "href": "slides/2023-11-15-model-selection.html#model-combination",
    "title": "Quantitative and Graphical Model Selection",
    "section": "Model combination",
    "text": "Model combination\nWe could treat “which model to use” as a parameter. If we have an exhaustive list of candidate models \\(\\{ M_\\ell \\}_{\\ell=1}^L\\), then the distribution over the model space is given by \\[\np(M | D) \\propto p(D | M) p(M)\n\\] and we can average over them \\[\np(\\tilde{y} | D) = \\sum_{\\ell=1}^L p(\\tilde{y}|D, M_\\ell) p(M_\\ell | D)\n\\]"
  },
  {
    "objectID": "slides/2023-11-15-model-selection.html#setting-up-information-criteria",
    "href": "slides/2023-11-15-model-selection.html#setting-up-information-criteria",
    "title": "Quantitative and Graphical Model Selection",
    "section": "Setting up Information Criteria",
    "text": "Setting up Information Criteria\nIf our inference on the parameters is summarized by a point estimate \\(\\hat{\\theta}\\) (e.g., the MAP estimate) then out of sample predictive accuracy is defined by \\[\n\\text{elpd}_\\hat{\\theta} = \\mathbb{E}_f \\left[ \\log p(\\tilde{y} | \\hat{\\theta}(y)) \\right]\n\\]"
  },
  {
    "objectID": "slides/2023-11-15-model-selection.html#aic-criterion",
    "href": "slides/2023-11-15-model-selection.html#aic-criterion",
    "title": "Quantitative and Graphical Model Selection",
    "section": "AIC Criterion",
    "text": "AIC Criterion\nIf the model estimates \\(k\\) parameters, and if they are assumed asymptotically normal (ie a normal linear model with known variance and uniofrm prior) then fitting \\(k\\) parameters will increase the predictive accuracy by chance alone: \\[\n\\hat{\\text{elpd}}_\\text{AIC} = \\log p(y | \\hat{\\theta}_\\text{mle}) - k\n\\] ::: {.fragment} Thus we can define \\[\n\\text{AIC} = 2 k - 2 \\ln \\hat{\\mathcal{L}}\n\\] and select the model that minimizes it. ::: ::: {.fragment} For complicated models, what is \\(k\\)? There are formula to approximate effective number of parameters. Note that AIC asssumes residuals are independent given \\(\\hat{\\theta}\\) :::"
  },
  {
    "objectID": "slides/2023-11-15-model-selection.html#dic-criterion",
    "href": "slides/2023-11-15-model-selection.html#dic-criterion",
    "title": "Quantitative and Graphical Model Selection",
    "section": "## DIC Criterion",
    "text": "## DIC Criterion\n\nStart with AIC\nReplace \\(\\hat{\\theta}_\\text{mle}\\) by posterior mean \\(\\hat{\\theta}_\\text{Bayes} = \\mathbb{E}[\\theta | y]\\)\nReplace \\(k\\) by a data-based bias correction; there are different forms\n\n\\[\n\\hat{\\text{elpd}}_\\text{DIC} = \\log p(y | \\hat{\\theta}_\\text{Bayes}) - p_\\text{DIC}\n\\] where \\(p_\\text{DIC}\\) is derived from assumptions about the effective number of parameters. The quantity \\[\n\\text{DIC} = -2 \\log p(y | \\hat{\\theta}_\\text{Bayes}) + 2 p_\\text{DIC}\n\\] can be assigned to each model, and the model with lowest DIC chosen. Note that DIC asssumes residuals are independent given \\(\\hat{\\theta}\\)"
  },
  {
    "objectID": "slides/2023-11-15-model-selection.html#schwarz-criterion-bayesian-information-criterion-bic-sbc-sic-sbic",
    "href": "slides/2023-11-15-model-selection.html#schwarz-criterion-bayesian-information-criterion-bic-sbc-sic-sbic",
    "title": "Quantitative and Graphical Model Selection",
    "section": "Schwarz criterion / “Bayesian” information criterion (BIC, SBC, SIC, SBIC)",
    "text": "Schwarz criterion / “Bayesian” information criterion (BIC, SBC, SIC, SBIC)\nGoal: approximate marginal probability of the data \\(p(y)\\) (this is different)\nAssuming the existence of a true model (\\(\\mathcal{M}-closed\\)), the model that minimizes BIC converges to the “true” model. \\[\n\\text{BIC} = k \\ln (n) - 2 \\ln \\hat{\\mathcal{L}}\n\\] where \\[\n\\hat{\\mathcal{L}}= \\max_\\theta p(x | \\theta, M)\n\\] and where \\(k\\) is the number of model parameters. The BIC can be viewed as a rough approximation to the Bayes factor (Kass & Raftery, 1995)."
  },
  {
    "objectID": "slides/2023-11-15-model-selection.html#significance-criteria",
    "href": "slides/2023-11-15-model-selection.html#significance-criteria",
    "title": "Quantitative and Graphical Model Selection",
    "section": "Significance criteria",
    "text": "Significance criteria\nUse Null Hypothesis Significance Testing (NHST) to decide whether to include a variable. For example, should we add a trend term in our regression?\n\nForm a null hypothesis: \\(\\beta = 0\\)\nTest statistics \\(\\Rightarrow\\) \\(p\\)-value\nIf \\(p &lt; \\alpha\\) then use \\(M_2\\) else use \\(M_1\\)\n\nNote that\n\nThis is equivalent to Bayes factor.\nStill assumes existence of a true model (hence the many problems with NHST)\n\nThis is widely used in practice, often without justification"
  },
  {
    "objectID": "slides/2023-11-15-model-selection.html#key-points-no-magic-here",
    "href": "slides/2023-11-15-model-selection.html#key-points-no-magic-here",
    "title": "Quantitative and Graphical Model Selection",
    "section": "Key points: NO MAGIC HERE",
    "text": "Key points: NO MAGIC HERE\n\nYou cannot look at a single criterion and decide whether a model is good or not; beware those who do!\nModel comparison and selection is subjective 🤷‍♂️\n\nMake your assumptions transparent so others can follow and critique them, rather than pretending to be objective (Doss-Gollin & Keller, 2023)\nSubjective doesn’t mean arbitrary or “in the dark”. We know stuff!"
  },
  {
    "objectID": "slides/2023-11-15-model-selection.html#references",
    "href": "slides/2023-11-15-model-selection.html#references",
    "title": "Quantitative and Graphical Model Selection",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nDoss-Gollin, J., & Keller, K. (2023). A subjective Bayesian framework for synthesizing deep uncertainties in climate risk management. Earth’s Future, 11(1). https://doi.org/10.1029/2022EF003044\n\n\nKass, R. E., & Raftery, A. E. (1995). Bayes factors. Journal of the American Statistical Association, 90(430), 773–795. https://doi.org/10.1080/01621459.1995.10476572\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second edition.). Boca Raton ; CRC Press, Taylor & Francis Group.\n\n\nPiironen, J., & Vehtari, A. (2017). Comparison of Bayesian predictive methods for model selection. Statistics and Computing, 27(3), 711–735. https://doi.org/10.1007/s11222-016-9649-y"
  },
  {
    "objectID": "slides/2023-11-13-regionalization.html#motivation",
    "href": "slides/2023-11-13-regionalization.html#motivation",
    "title": "Regionalization and Spatial Pooling",
    "section": "Motivation",
    "text": "Motivation\n\nFigure 1: Fagnant et al. (2020)"
  },
  {
    "objectID": "slides/2023-11-13-regionalization.html#rationale",
    "href": "slides/2023-11-13-regionalization.html#rationale",
    "title": "Regionalization and Spatial Pooling",
    "section": "Rationale",
    "text": "Rationale\n\n\nNearby stations should (usually) have similar precipitation probabilities\n\nAlternatively: flood, fire, etc.\n\nReduce estimation error by pooling information\nReduce sampling error of random variation betweeen nearby stations\nDOES NOT reduce sampling error of major regional events"
  },
  {
    "objectID": "slides/2023-11-13-regionalization.html#data",
    "href": "slides/2023-11-13-regionalization.html#data",
    "title": "Regionalization and Spatial Pooling",
    "section": "Data",
    "text": "Data\nNOAA Atlas 14 for stations with \\(29.25 \\leq ~\\text{lat}~ \\leq 30.25\\) and \\(-96 \\leq ~\\text{lon}~ \\leq -95\\)\n\n\nCode\nannmax_precip = CSV.read(\"data/dur01d_ams_na14v11_houston.csv\", DataFrame)\nstations = combine(groupby(annmax_precip, :stnid), :lat =&gt; first =&gt; :lat, :lon =&gt; first =&gt; :lon, :name =&gt; first =&gt; :name, :stnid =&gt; length =&gt; :n)\nscatter(stations.lon, stations.lat, zcolor=stations.n, xlabel=\"Longitude\", ylabel=\"Latitude\", colorbar_title=\"Number of Years\", title=\"Locations of $(nrow(stations)) Stations\")"
  },
  {
    "objectID": "slides/2023-11-13-regionalization.html#four-longest-records",
    "href": "slides/2023-11-13-regionalization.html#four-longest-records",
    "title": "Regionalization and Spatial Pooling",
    "section": "Four longest records",
    "text": "Four longest records\n\n\nCode\nstn_plots = []\nfor stnid in sort(stations, :n, rev=true)[!, :stnid][1:4]\n    sub = annmax_precip[annmax_precip.stnid.==stnid, :]\n    name = stations[stations.stnid.==stnid, :name][1]\n    pᵢ = plot(sub.year, sub.precip_in, marker=:circ, xlabel=\"Year\", ylabel=\"Annual Maximum Precipitation [in]\", label=name)\n    push!(stn_plots, pᵢ)\nend\nplot(stn_plots..., layout=(2, 2), size=(1600, 700), link=:all)"
  },
  {
    "objectID": "slides/2023-11-13-regionalization.html#all-stations",
    "href": "slides/2023-11-13-regionalization.html#all-stations",
    "title": "Regionalization and Spatial Pooling",
    "section": "All stations",
    "text": "All stations\n\nprecip_df = sort(unstack(annmax_precip, :year, :stnid, :precip_in), :year)\nplot(precip_df.year, Matrix(precip_df[!, Not(:year)]), label=false, yscale=:log10, yticks=([2, 5, 10, 15, 20, 25], [\"2\", \"5\", \"10\", \"15\", \"20\", \"25\"]), ylabel=\"Annual Maximum Precip [in]\", linewidth=0.5)"
  },
  {
    "objectID": "slides/2023-11-13-regionalization.html#learning-objectives",
    "href": "slides/2023-11-13-regionalization.html#learning-objectives",
    "title": "Regionalization and Spatial Pooling",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nDiscuss the motivation for regionalization\nDescribe key assumptions of different regionalization approaches\nOutline several specific models that implement regionalization"
  },
  {
    "objectID": "slides/2023-11-13-regionalization.html#recall-l-moment-estimators",
    "href": "slides/2023-11-13-regionalization.html#recall-l-moment-estimators",
    "title": "Regionalization and Spatial Pooling",
    "section": "Recall: \\(L\\) moment estimators",
    "text": "Recall: \\(L\\) moment estimators\n\n\nLinear combinations of order statistics\nCan choose parameters of a distribution so that the theoretical \\(L\\) moments of the distribution “match” the empirical \\(L\\) moments of the data\nPros:\n\nComputationally efficient\nWork well inpractice\n\nCons:\n\nInflexible\nDifficult to quantify parametric uncertainty\n\n\n\nSee Hosking (1990) for details."
  },
  {
    "objectID": "slides/2023-11-13-regionalization.html#regional-frequency-analysis",
    "href": "slides/2023-11-13-regionalization.html#regional-frequency-analysis",
    "title": "Regionalization and Spatial Pooling",
    "section": "Regional Frequency Analysis",
    "text": "Regional Frequency Analysis\n\n\nAssign sites to regions\nEstimate \\(L\\) moments for each site\nCheck for homogeneity\nTake regional \\(L\\) moments as the weighted mean of the site \\(L\\) moments\n\nFor floods: apply scaling factor (e.g., average annual maximum flood)\n\n\n\nBest implemented in the R lmomRFA package. (Can use RCall to call R from Julia.)"
  },
  {
    "objectID": "slides/2023-11-13-regionalization.html#region-of-influence",
    "href": "slides/2023-11-13-regionalization.html#region-of-influence",
    "title": "Regionalization and Spatial Pooling",
    "section": "Region of Influence",
    "text": "Region of Influence\n\n\nRFA assumes that all sites are assigned to a single region\n\nBut often regions are not distinct!\n\nROI: define a “similarity” between each pair of sites. E.g., distance, land use, elevation, climate.\nTo make estimates at site \\(i\\), define its “region of influence” as the most similar sites (similar to KNN)\nEstimate \\(L\\) moments for each site and compute weighted average as in RFA"
  },
  {
    "objectID": "slides/2023-11-13-regionalization.html#bayesian-gev-in-turing",
    "href": "slides/2023-11-13-regionalization.html#bayesian-gev-in-turing",
    "title": "Regionalization and Spatial Pooling",
    "section": "Bayesian GEV in Turing",
    "text": "Bayesian GEV in Turing\nWe’ve used Extremes.jl, but we need our own model for customization. Here’s a GEV model for a single site.\n\n@model function stationary_gev(y::AbstractVector)\n    # priors\n1    μ ~ Normal(5, 5)\n2    σ ~ LogNormal(0, 2)\n3    ξ ~ Uniform(-0.5, 0.5)\n    y .~ GeneralizedExtremeValue(μ, σ, ξ)\nend\n\n\n1\n\nWide priors on our parameters for now\n\n2\n\nWork on the log scale for \\(\\sigma\\) to ensure positivity\n\n3\n\nRestrict shape parameter to the interval \\((-1, 1)\\)"
  },
  {
    "objectID": "slides/2023-11-13-regionalization.html#sampling",
    "href": "slides/2023-11-13-regionalization.html#sampling",
    "title": "Regionalization and Spatial Pooling",
    "section": "Sampling",
    "text": "Sampling\nWe draw samples as\n\nchn_stationary_gev = let # variables defined in a let...end block are temporary\n    sub = vec(annmax_precip[annmax_precip.stnid.==\"41-4321\", :precip_in])\n    model = stationary_gev(sub)\n    sampler = Turing.NUTS()\n    n_per_chain = 5000\n    nchains = 4\n    sample(model, sampler, MCMCThreads(), n_per_chain, nchains; drop_warmup=true)\nend\nsummarystats(chn_stationary_gev)\n\n\nSummary Statistics\n  parameters      mean       std      mcse    ess_bulk    ess_tail      rhat   ⋯\n      Symbol   Float64   Float64   Float64     Float64     Float64   Float64   ⋯\n           μ    3.4930    0.1880    0.0023   6509.0259   7713.4040    1.0005   ⋯\n           σ    1.2617    0.1629    0.0019   7289.1721   9834.1796    1.0004   ⋯\n           ξ    0.2697    0.1052    0.0013   6436.0820   4114.3823    1.0007   ⋯\n                                                                1 column omitted"
  },
  {
    "objectID": "slides/2023-11-13-regionalization.html#visualization",
    "href": "slides/2023-11-13-regionalization.html#visualization",
    "title": "Regionalization and Spatial Pooling",
    "section": "Visualization",
    "text": "Visualization\n\n\nCode\ndists = [\n    GeneralizedExtremeValue(μ, σ, ξ) for (μ, σ, ξ) in\n    zip(vec(chn_stationary_gev[:μ]), vec(chn_stationary_gev[:σ]), vec(chn_stationary_gev[:ξ]))\n]\nhistogram(quantile.(dists, 0.99), normalize=:pdf, label=false, xlabel=\"Annual Maximum Precipitation [in]\", ylabel=\"Density\", title=\"100 Year Return Level Posterior\", linewidth=0)"
  },
  {
    "objectID": "slides/2023-11-13-regionalization.html#full-pooling-concept",
    "href": "slides/2023-11-13-regionalization.html#full-pooling-concept",
    "title": "Regionalization and Spatial Pooling",
    "section": "Full pooling: concept",
    "text": "Full pooling: concept\nStations should “pool” information\n\n\nAssume: within a region, all sites have the same distribution\nEstimate a single distribution for the entire region\n\nAnalagrous to regional frequency analysis"
  },
  {
    "objectID": "slides/2023-11-13-regionalization.html#full-pooling-model",
    "href": "slides/2023-11-13-regionalization.html#full-pooling-model",
    "title": "Regionalization and Spatial Pooling",
    "section": "Full pooling: model",
    "text": "Full pooling: model\n\n@model function gev_fully_pooled(y::AbstractMatrix)\n    N_yr, N_stn = size(y)\n    μ ~ Normal(5, 5)\n    σ ~ LogNormal(0, 2)\n    ξ ~ Uniform(-0.5, 0.5)\n    for s in 1:N_stn\n        for t in 1:N_yr\n            if !ismissing(y[t, s])\n                y[t, s] ~ GeneralizedExtremeValue(μ, σ, ξ)\n            end\n        end\n    end\nend"
  },
  {
    "objectID": "slides/2023-11-13-regionalization.html#full-pooling-sampling",
    "href": "slides/2023-11-13-regionalization.html#full-pooling-sampling",
    "title": "Regionalization and Spatial Pooling",
    "section": "Full pooling: sampling",
    "text": "Full pooling: sampling\nWe have a lot of data per parameter, so sampling is pretty fast.\n\n\nCode\nprecip_array = Matrix(precip_df[!, Not(:year)])\nchn_gev_fully_pooled = let # variables defined in a let...end block are temporary\n    model = gev_fully_pooled(precip_array)\n    sampler = Turing.NUTS()\n    n_per_chain = 5000\n    nchains = 4\n    sample(model, sampler, MCMCThreads(), n_per_chain, nchains; drop_warmup=true)\nend\nsummarystats(chn_gev_fully_pooled)\n\n\n\nSummary Statistics\n  parameters      mean       std      mcse     ess_bulk     ess_tail      rhat ⋯\n      Symbol   Float64   Float64   Float64      Float64      Float64   Float64 ⋯\n           μ    3.5082    0.0351    0.0003   11946.4544   13458.1452    1.0003 ⋯\n           σ    1.3949    0.0286    0.0003   12932.6177   14075.0610    1.0002 ⋯\n           ξ    0.2396    0.0192    0.0002   14374.2644   11676.1996    1.0003 ⋯\n                                                                1 column omitted"
  },
  {
    "objectID": "slides/2023-11-13-regionalization.html#important-caveat",
    "href": "slides/2023-11-13-regionalization.html#important-caveat",
    "title": "Regionalization and Spatial Pooling",
    "section": "Important caveat!",
    "text": "Important caveat!\n\n\nWe weight each observation equally, regardless of site or year\nIf we have more observations for some years than others, we are implicitly weighting those years more heavily\nA better model would correct for this and weight each year equally"
  },
  {
    "objectID": "slides/2023-11-13-regionalization.html#partial-pooling-concept",
    "href": "slides/2023-11-13-regionalization.html#partial-pooling-concept",
    "title": "Regionalization and Spatial Pooling",
    "section": "Partial pooling: concept",
    "text": "Partial pooling: concept\nWhat if we believe there should be some variation between stations, but that they should still share information?\n\n\nIn between the two extremes of “full pooling” and “no pooling”\nModel the parameters at each site as being drawn from a common distribution\n\n\n\nThis leads to models that look like this: \\[\n\\begin{aligned}\n    y_{s, t} &\\sim \\text{GEV}(\\mu_s, \\sigma_s, \\xi_s) \\\\\n    \\mu_s &\\sim \\text{Normal}(\\mu^0, \\tau^\\mu) \\\\\n    \\ldots\n\\end{aligned}\n\\]\n\nwhere \\(s\\) is the site index and \\(t\\) is the year index."
  },
  {
    "objectID": "slides/2023-11-13-regionalization.html#hyperparameters",
    "href": "slides/2023-11-13-regionalization.html#hyperparameters",
    "title": "Regionalization and Spatial Pooling",
    "section": "Hyperparameters",
    "text": "Hyperparameters\nIn machine learning (e.g., Random Forests) we studied hyperparameters that the user must “tune”. In Bayesian statistics, hyperparameters are learned as part of the model.\n\nIn our model, the hyperparameters are \\(\\mu^0\\) and \\(\\tau^\\mu\\). These describe the distribution from which the \\(\\mu_s\\) are drawn. (And similarly for \\(\\sigma\\) and, optionally, \\(\\xi\\).)"
  },
  {
    "objectID": "slides/2023-11-13-regionalization.html#implementation",
    "href": "slides/2023-11-13-regionalization.html#implementation",
    "title": "Regionalization and Spatial Pooling",
    "section": "Implementation",
    "text": "Implementation\nWe implement full pooling on \\(\\xi\\) and partial pooling on \\(\\mu\\) and \\(\\sigma\\).\n\n@model function gev_partial_pool(y::AbstractMatrix)\n    N_yr, N_stn = size(y)\n\n    # First define the hyperparameters. Stronger priors are helpful!\n    μ₀ ~ Normal(5, 3)\n    τμ ~ LogNormal(0, 0.5)\n    σ₀ ~ LogNormal(0.5, 0.5)\n    τσ ~ LogNormal(0, 0.5)\n\n    # Parameters depend on the hyperparameters\n    μ ~ filldist(Normal(μ₀, τμ), N_stn)\n    σ ~ filldist(truncated(Normal(σ₀, τσ), 0, Inf), N_stn)\n\n    # Parameters that don't depend on hyperparameters\n    ξ ~ Uniform(-0.5, 0.5)\n\n    # Likelihood\n    for s in 1:N_stn\n        for t in 1:N_yr\n            y[t, s] ~ GeneralizedExtremeValue(μ[s], σ[s], ξ)\n        end\n    end\nend"
  },
  {
    "objectID": "slides/2023-11-13-regionalization.html#computation",
    "href": "slides/2023-11-13-regionalization.html#computation",
    "title": "Regionalization and Spatial Pooling",
    "section": "Computation",
    "text": "Computation\nDownside: we have a lot of parameters to estimate! If we have \\(N\\) stations, then we have \\(4 + N + N + 1 = 5+2N\\) parameters to estimate. This makes sampling slower.\n\nSampling is left as an exercise. For an example, see Lima et al. (2016)."
  },
  {
    "objectID": "slides/2023-11-13-regionalization.html#spatial-models",
    "href": "slides/2023-11-13-regionalization.html#spatial-models",
    "title": "Regionalization and Spatial Pooling",
    "section": "Spatial Models",
    "text": "Spatial Models\nWe can also model the paramteters as a function of location. This can simplify our model because we only need to estimate the parameters that describe how our parameters vary spatially."
  },
  {
    "objectID": "slides/2023-11-13-regionalization.html#example",
    "href": "slides/2023-11-13-regionalization.html#example",
    "title": "Regionalization and Spatial Pooling",
    "section": "Example",
    "text": "Example\nWe can imagine a very simple toy model:\n\\[\n\\begin{aligned}\n    \\mu(s) &= \\alpha^\\mu + \\beta^\\mu_1 \\cdot \\text{lat}(s) + \\beta^\\mu_2 \\cdot \\text{lon}(s) \\\\\n    \\sigma(s) &= \\alpha^\\sigma + \\beta^\\sigma_1 \\cdot \\text{lat}(s) + \\beta^\\sigma_2 \\cdot \\text{lon}(s)\n    y &\\sim \\text{GEV}(\\mu(s), \\sigma(s), \\xi)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/2023-11-13-regionalization.html#last-friday",
    "href": "slides/2023-11-13-regionalization.html#last-friday",
    "title": "Regionalization and Spatial Pooling",
    "section": "Last Friday",
    "text": "Last Friday\nI presented work from my group combining Bayesian and spatial models at the 2023 Statistical Hydrology conference. Slides here."
  },
  {
    "objectID": "slides/2023-11-13-regionalization.html#references",
    "href": "slides/2023-11-13-regionalization.html#references",
    "title": "Regionalization and Spatial Pooling",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nFagnant, C., Gori, A., Sebastian, A., Bedient, P. B., & Ensor, K. B. (2020). Characterizing spatiotemporal trends in extreme precipitation in Southeast Texas. Natural Hazards, 104(2), 1597–1621. https://doi.org/10.1007/s11069-020-04235-x\n\n\nHosking, J. R. M. (1990). L-Moments: Analysis and Estimation of Distributions Using Linear Combinations of Order Statistics. Journal of the Royal Statistical Society. Series B (Methodological), 52(1), 105–124. Retrieved from https://www.jstor.org/stable/2345653\n\n\nLima, C. H. R., Lall, U., Troy, T., & Devineni, N. (2016). A hierarchical Bayesian GEV model for improving local and regional flood quantile estimates. Journal of Hydrology, 541, 816–823. https://doi.org/10.1016/j.jhydrol.2016.07.042"
  },
  {
    "objectID": "slides/2023-10-18-random-forest.html#reading",
    "href": "slides/2023-10-18-random-forest.html#reading",
    "title": "Random Forest Models",
    "section": "Reading",
    "text": "Reading\nI am drawing heavily from Chapter 8 of James et al. (2013). For more, see:\n\nChapter 15 of Friedman et al. (2001)\nTowards Data Science post on Gradient Boosting"
  },
  {
    "objectID": "slides/2023-10-18-random-forest.html#situating-ourselves",
    "href": "slides/2023-10-18-random-forest.html#situating-ourselves",
    "title": "Random Forest Models",
    "section": "Situating ourselves",
    "text": "Situating ourselves\n\nGiven: \\(\\{(X_i, y_i) \\mid i = 1, 2, \\ldots, n\\}\\) i.e. paired predictors and targets\n\nSupervised learning\n\nGoal: approximate a function \\(f\\)\n\n“Regression”\nIdeally: good predictions on new data"
  },
  {
    "objectID": "slides/2023-10-18-random-forest.html#example-datasett",
    "href": "slides/2023-10-18-random-forest.html#example-datasett",
    "title": "Random Forest Models",
    "section": "Example datasett",
    "text": "Example datasett\nPredict a baseball player’s Salary (thousands of dollars) based on Years (the number of years that he has played in the major leagues) and Hits (the number of hits that he made in the previous year). We first remove observations that are missing Salary values, and log-transform Salary so that its distribution has more of a typical bell-shape.\n\nhitters = dataset(\"ISLR\", \"Hitters\")\nfirst(hitters, 5)\n\n5×20 DataFrame\n\n\n\nRow\nAtBat\nHits\nHmRun\nRuns\nRBI\nWalks\nYears\nCAtBat\nCHits\nCHmRun\nCRuns\nCRBI\nCWalks\nLeague\nDivision\nPutOuts\nAssists\nErrors\nSalary\nNewLeague\n\n\n\nInt32\nInt32\nInt32\nInt32\nInt32\nInt32\nInt32\nInt32\nInt32\nInt32\nInt32\nInt32\nInt32\nCat…\nCat…\nInt32\nInt32\nInt32\nFloat64?\nCat…\n\n\n\n\n1\n293\n66\n1\n30\n29\n14\n1\n293\n66\n1\n30\n29\n14\nA\nE\n446\n33\n20\nmissing\nA\n\n\n2\n315\n81\n7\n24\n38\n39\n14\n3449\n835\n69\n321\n414\n375\nN\nW\n632\n43\n10\n475.0\nN\n\n\n3\n479\n130\n18\n66\n72\n76\n3\n1624\n457\n63\n224\n266\n263\nA\nW\n880\n82\n14\n480.0\nA\n\n\n4\n496\n141\n20\n65\n78\n37\n11\n5628\n1575\n225\n828\n838\n354\nN\nE\n200\n11\n3\n500.0\nN\n\n\n5\n321\n87\n10\n39\n42\n30\n2\n396\n101\n12\n48\n46\n33\nN\nE\n805\n40\n4\n91.5\nN"
  },
  {
    "objectID": "slides/2023-10-18-random-forest.html#nonlinear-relationships",
    "href": "slides/2023-10-18-random-forest.html#nonlinear-relationships",
    "title": "Random Forest Models",
    "section": "Nonlinear Relationships",
    "text": "Nonlinear Relationships\n\n\nCode\n# Separate the data into two groups: with and without missing Salary values\ncomplete_data = dropmissing(hitters, :Salary)\nmissing_data = hitters[ismissing.(hitters[:, :Salary]), :]\n\n# Plot the points with valid Salary values, colored based on Salary\np1 = scatter(\n    complete_data[:, :Years],\n    complete_data[:, :Hits];\n    zcolor=log.(complete_data[:, :Salary]),\n    xlabel=\"Years\",\n    ylabel=\"Hits\",\n    label=\"Log of Salary\",\n)\n\n# Overlay the points with missing Salary as open circles\nscatter!(\n    p1,\n    missing_data[:, :Years],\n    missing_data[:, :Hits];\n    markercolor=:white,\n    label=\"Missing Salary\",\n)"
  },
  {
    "objectID": "slides/2023-10-18-random-forest.html#partition",
    "href": "slides/2023-10-18-random-forest.html#partition",
    "title": "Random Forest Models",
    "section": "Partition",
    "text": "Partition\nOne way we can make predictions is to partition the predictor space into \\(M\\) regions \\(R_1, R_2, \\ldots, R_M\\) and then predict a constant value in each region.\n\n\nCode\np2 = plot(p1)\n\n# Draw the vertical line for Years &lt; 4.5\nplot!(\n    p2, [4.5, 4.5], [0, maximum(hitters[:, :Hits])]; line=:dash, color=:black, label=false\n)\n\n# Draw the horizontal line for Hits &lt; 117.5 for Years &gt;= 4.5\nplot!(\n    p2,\n    [4.5, maximum(hitters[:, :Years])],\n    [117.5, 117.5];\n    line=:dash,\n    color=:black,\n    label=false,\n)\n\n# Annotate the regions\nannotate!(p2, 2, maximum(hitters[:, :Hits]) - 20, text(\"R1\", 12, :left))\nannotate!(p2, 6, 50, text(\"R2\", 12, :left))\nannotate!(p2, 6, maximum(hitters[:, :Hits]) - 20, text(\"R3\", 12, :left))"
  },
  {
    "objectID": "slides/2023-10-18-random-forest.html#terminology",
    "href": "slides/2023-10-18-random-forest.html#terminology",
    "title": "Random Forest Models",
    "section": "Terminology",
    "text": "Terminology\n\nDecision Node\n\n\\(\\text{Years} &lt; 4.5\\)\n\\(\\text{Hits} &lt; 117.5\\)\nHierarchical structure\n\nLeaf Node (aka: terminal node, leaf)\n\n\\(R_1, R_2, R_3\\)"
  },
  {
    "objectID": "slides/2023-10-18-random-forest.html#implementation",
    "href": "slides/2023-10-18-random-forest.html#implementation",
    "title": "Random Forest Models",
    "section": "Implementation",
    "text": "Implementation\n\n\nCode\n# Define the Node structure\nabstract type AbstractNode end\n\nstruct DecisionNode &lt;: AbstractNode\n    feature::Symbol\n    threshold::Float64\n    left::AbstractNode\n    right::AbstractNode\nend\n\nstruct LeafNode &lt;: AbstractNode\n    value::Float64\nend\n\n# Define the Partition structure\nstruct Partition\n    feature::Symbol\n    threshold::Float64\n    left::Union{Partition,Nothing}\n    right::Union{Partition,Nothing}\nend\n\n# Define the DecisionTree structure\nstruct MyDecisionTree\n    root::AbstractNode\nend\n\n# Constructor for DecisionTree from DataFrame and partition\nfunction MyDecisionTree(df::DataFrame, partition::Partition, y::Symbol)\n    # Recursive function to build the tree\n    function build_tree(partition, subset)\n        if partition === nothing\n            return LeafNode(mean(skipmissing(subset[:, y])))\n        end\n\n        left_subset = subset[subset[!, partition.feature] .&lt;= partition.threshold, :]\n        right_subset = subset[subset[!, partition.feature] .&gt; partition.threshold, :]\n\n        left = build_tree(partition.left, left_subset)\n        right = build_tree(partition.right, right_subset)\n\n        return DecisionNode(partition.feature, partition.threshold, left, right)\n    end\n\n    root = build_tree(partition, df)\n    return MyDecisionTree(root)\nend\n\nfunction predict(tree::MyDecisionTree, row::DataFrameRow)\n    node = tree.root\n    while !isa(node, LeafNode)\n        if row[node.feature] &lt;= node.threshold\n            node = node.left\n        else\n            node = node.right\n        end\n    end\n    return node.value\nend"
  },
  {
    "objectID": "slides/2023-10-18-random-forest.html#our-model",
    "href": "slides/2023-10-18-random-forest.html#our-model",
    "title": "Random Forest Models",
    "section": "Our model",
    "text": "Our model\n\npartition = Partition(:Years, 4.5, nothing, Partition(:Hits, 117.5, nothing, nothing))\ntree = MyDecisionTree(hitters, partition, :Salary)\npredictions = [predict(tree, row) for row in eachrow(hitters)]\n\n\n\nCode\np3 = scatter(\n    hitters[:, :Years],\n    hitters[:, :Hits];\n    zcolor=log.(predictions),\n    xlabel=\"Years\",\n    ylabel=\"Hits\",\n    label=\"Predicted\",\n    title=\"Partion Model\",\n)\nplot(plot(p1; title=\"Obs\"), p3; layout=(1, 2), size=(1250, 500), link=:both)"
  },
  {
    "objectID": "slides/2023-10-18-random-forest.html#more-formally",
    "href": "slides/2023-10-18-random-forest.html#more-formally",
    "title": "Random Forest Models",
    "section": "More formally",
    "text": "More formally\nWe are making predictions based on stratification of the feature space\n\nDivide the predictor space \\(X\\) into \\(J\\) distinct regions \\(R_1, R_2, \\ldots, R_J\\)\nFor every observation in \\(R_j\\), make the same prediction\n\n\\(\\hat{y}_j = \\frac{1}{N_j} \\sum_{i \\in R_j} y_i\\)"
  },
  {
    "objectID": "slides/2023-10-18-random-forest.html#choosing-partitions",
    "href": "slides/2023-10-18-random-forest.html#choosing-partitions",
    "title": "Random Forest Models",
    "section": "Choosing partitions",
    "text": "Choosing partitions\nHow do we choose the regions \\(R_1, R_2, \\ldots, R_J\\)?\n\nWe could choose anything!\nHigh-dimensional “boxes” are simple\nFind boxes \\(R_1, R_2, \\ldots, R_J\\) that minimize the residual sum of squares (RSS) \\[\n\\sum_{j=1}^J \\sum_{i \\in R_j} \\left(y_i - \\hat{y}_i \\right)^2\n\\]"
  },
  {
    "objectID": "slides/2023-10-18-random-forest.html#optimization",
    "href": "slides/2023-10-18-random-forest.html#optimization",
    "title": "Random Forest Models",
    "section": "Optimization",
    "text": "Optimization\n\n\n\nExtremely hard problem:\nConsider the space of all possible partitions\n\nHow I choose \\(R_1\\) will affect the best \\(R_63\\)\n\n\n\nFeasible problem: recursive binary splitting\n\nSelect a predictor \\(X_j\\) and cutpoint \\(s\\) so that splitting predictor space into \\(\\{X | X_j &lt; s \\}\\) and \\(\\{X | X_j \\geq s \\}\\) minimizes RSS\n\nConsider \\(J \\times N\\) possible splits\n\nRepeat, considering a partition on each of the two resulting regions\n…\n\nTop-down, greedy algorithm"
  },
  {
    "objectID": "slides/2023-10-18-random-forest.html#overfitting",
    "href": "slides/2023-10-18-random-forest.html#overfitting",
    "title": "Random Forest Models",
    "section": "Overfitting",
    "text": "Overfitting\n\n\nA “deep” (many splits) tree will fit our data well\n\nBut is likely to overfit\nLower bias, higher variance\nWe could have \\(n\\) splits – each observation in its own region!\n\nA “shallow” (few splits) tree will fit our data poorly\n\nBut is likely to generalize better\nHigher bias, lower variance"
  },
  {
    "objectID": "slides/2023-10-18-random-forest.html#cost-complexity-penalty",
    "href": "slides/2023-10-18-random-forest.html#cost-complexity-penalty",
    "title": "Random Forest Models",
    "section": "Cost complexity penalty",
    "text": "Cost complexity penalty\nA penalty for the number of splits \\(|T|\\): \\[\n\\text{Loss} = \\sum_{m=1}^{|T|} \\sum_{i: X_i \\in R_m} \\left(y_i - \\hat{y}_{R_m} \\right)^2 + \\alpha |T|\n\\]"
  },
  {
    "objectID": "slides/2023-10-18-random-forest.html#pruning",
    "href": "slides/2023-10-18-random-forest.html#pruning",
    "title": "Random Forest Models",
    "section": "Pruning",
    "text": "Pruning\nEmpirically, it works well to grow a large tree, then “prune” it back to a smaller tree.\n\n\nUse recursive binary splitting with MSE loss to grow a large tree\n\nExample stopping rule: all regions have fewer than \\(K\\) observations\n\nRecursively, find the node with the “weakest link” by removing one split from the tree and seeing which split has the smallest increase in RSS\nChoose the tree that minimizes the loss function"
  },
  {
    "objectID": "slides/2023-10-18-random-forest.html#classification-trees",
    "href": "slides/2023-10-18-random-forest.html#classification-trees",
    "title": "Random Forest Models",
    "section": "Classification trees",
    "text": "Classification trees\nWe’ve been focusing on regression, but classification is also a common task!\n\nGiven remote sensing images, classify land uses\nGiven information about a house and flood, predict whether it experienced damage\nGiven some parameters describing population growth rates, climate change, etc, predict whether a community will experience water stress\n\nSame idea but different loss function. For example, cross-entropy loss: \\[\nD = - \\sum_{k=1}^K p_{mk} \\log \\hat{p}_{mk}\n\\] where \\(\\hat{p}_{mk}\\) is the proportion of observations in region \\(m\\) that are in class \\(k\\)."
  },
  {
    "objectID": "slides/2023-10-18-random-forest.html#ensemble-methods",
    "href": "slides/2023-10-18-random-forest.html#ensemble-methods",
    "title": "Random Forest Models",
    "section": "Ensemble methods",
    "text": "Ensemble methods\n\nCombine many “weak” learners into a “strong” learner\n“Jury”\n“Wisdom of the crowd”\n\n\n\n\n\n\n\nKey insight\n\n\nEnsemble methods work better when the weak learners are less correlated"
  },
  {
    "objectID": "slides/2023-10-18-random-forest.html#bagging",
    "href": "slides/2023-10-18-random-forest.html#bagging",
    "title": "Random Forest Models",
    "section": "Bagging",
    "text": "Bagging\nBagging is a general approach for ensemble learning that is especially useful for tree methods.\n\nProblem: decision trees have high variance. If we split our data in half, then fit a decision tree separately to each half, they might be very different.\n\n\nConcept: averaging a set of observations reduces variance. Recall that given \\(n\\) IID observations \\(Z_i\\) with mean \\(\\bar{Z}\\) and variance \\(\\sigma^2\\), the variance of the mean is \\(\\sigma^2 / n\\).\n\n\nApproach: use a bootstrap to create \\(B\\) datasets, fit a decision tree to each, and average the predictions. \\[\n\\hat{f}_\\text{bag} = \\frac{1}{B} \\sum_{b=1}^B \\hat{f}^{*b}(x)\n\\] where \\(\\hat{f}^{*b}(x)\\) is the prediction of the tree trained on the \\(b\\)th bootstrap sample, making a prediction for the full dataset."
  },
  {
    "objectID": "slides/2023-10-18-random-forest.html#random-forests-1",
    "href": "slides/2023-10-18-random-forest.html#random-forests-1",
    "title": "Random Forest Models",
    "section": "Random Forests",
    "text": "Random Forests\n\nProblem: the trees in a bagged ensemble are highly correlated. Averaging many highly correlated quantities does not lead to as large of a reduction in variance as averaging many uncorrelated quantities.\n\n\nSolution: at each split in the tree, consider only a random subset of the predictors (do not allow the model to split on the rest)\n\n\nRationale: suppose that there is one very strong predictor in the data set, along with a number of other moderately strong predictors. Then in the collection of bagged trees, most or all of the trees will use this strong predictor in the top split, and they will be closely correlated.\n\n\nImplementation: at each split, randomly select \\(m\\) predictors out of the \\(p\\) possible predictors. Typically, we choose \\(m \\approx \\sqrt{p}\\). (If \\(m=p\\) then we are back to regular bagging.)"
  },
  {
    "objectID": "slides/2023-10-18-random-forest.html#boosting",
    "href": "slides/2023-10-18-random-forest.html#boosting",
    "title": "Random Forest Models",
    "section": "Boosting",
    "text": "Boosting\nLike bagging, boosting is a general approach that is commonly used in tree methods.\n\nIdea: instead of training each “tree” in the “forest” on a bootstrapped sample of the data, train each tree on a modified version of the data set. Specifically, fit a tree using the current residuals, rather than the outcome, as the response.\n\n\nAlgorithm: 1. Initialize prediction \\(\\hat{f}_i(x)=0\\) and residuals \\(r_i = y_i\\) for all \\(i\\) 1. For \\(b=1, 2, \\ldots, B\\): 1. Fit a tree \\(\\hat{f}^b\\) to the training data \\((X, r)\\) with \\(d\\) splits 1. Update the prediction: \\(\\hat{f}(x) = \\hat{f}(x) + \\lambda \\hat{f}^b(x)\\) 1. Update residuals 1. Output the boosted model: \\(\\hat{f}(x) = \\sum_{i=1}^B \\lambda \\hat{f}^b(x)\\).\n\n\nKey parameters: number of trees \\(B\\), shrinkage rate \\(\\lambda\\), number of splits per tree \\(d\\)"
  },
  {
    "objectID": "slides/2023-10-18-random-forest.html#julia-example",
    "href": "slides/2023-10-18-random-forest.html#julia-example",
    "title": "Random Forest Models",
    "section": "Julia example",
    "text": "Julia example\n\n# Drop rows with missing Salary values\nhitters_nm = dropmissing(hitters, :Salary)\n\n# Prepare data for training\nnumerical_cols = [col for col in names(hitters_nm) if eltype(hitters_nm[!, col]) &lt;: Number]\nhitters_nm = hitters_nm[:, numerical_cols]\nfeatures = Matrix(hitters_nm[:, Not(:Salary)])\nlabels = vec(hitters_nm[:, :Salary])\n\n# Train a decision tree regressor\nmodel = DecisionTreeRegressor()\nfit!(model, features, labels)\n\n# Get predictions\npredictions = DecisionTree.predict(model, features)\n\n# Scatter plot of actual vs predicted values\nscatter(\n    labels,\n    predictions;\n    xlabel=\"Actual Salary\",\n    ylabel=\"Predicted Salary\",\n    label=\"Data Points\",\n    legend=:topleft,\n)\n\n# Plot a diagonal line for perfect predictions\nPlots.abline!(1, 0; color=:black, label=\"Perfect Predictions\")"
  },
  {
    "objectID": "slides/2023-10-18-random-forest.html#adjustments",
    "href": "slides/2023-10-18-random-forest.html#adjustments",
    "title": "Random Forest Models",
    "section": "Adjustments",
    "text": "Adjustments\n\n# how many predictors\nm = Int(ceil(sqrt(size(features, 2))))\n\n# Train a decision tree regressor\nmodel = RandomForestRegressor(; n_subfeatures=m, n_trees=250)\nfit!(model, features, labels)\n\n# Get predictions\npredictions = DecisionTree.predict(model, features)\n\n# Scatter plot of actual vs predicted values\nscatter(\n    labels,\n    predictions;\n    xlabel=\"Actual Salary\",\n    ylabel=\"Predicted Salary\",\n    label=\"Data Points\",\n    legend=:topleft,\n)\n\n# Plot a diagonal line for perfect predictions\nPlots.abline!(1, 0; color=:black, label=\"Perfect Predictions\")"
  },
  {
    "objectID": "slides/2023-10-18-random-forest.html#key-things-to-know",
    "href": "slides/2023-10-18-random-forest.html#key-things-to-know",
    "title": "Random Forest Models",
    "section": "Key things to know",
    "text": "Key things to know\n\nDecision trees\n\nWhy would we fit them?\nHow do they work?\nKey trade-offs\n\nTree ensemble methods\n\nHow do boosting / bagging / RFs work?\nBe able to outline the algorithm\nExplain the logic underpinning these methods"
  },
  {
    "objectID": "slides/2023-10-18-random-forest.html#references",
    "href": "slides/2023-10-18-random-forest.html#references",
    "title": "Random Forest Models",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nFriedman, J., Hastie, T., & Tibshirani, R. (2001). The Elements of Statistical Learning (Vol. 1). Springer series in statistics Springer, Berlin.\n\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning (Vol. 103). New York, NY: Springer New York."
  },
  {
    "objectID": "slides/2023-12-01-exam.html#tbd",
    "href": "slides/2023-12-01-exam.html#tbd",
    "title": "Module III Exam",
    "section": "TBD",
    "text": "TBD\nThis is a placeholder for a planned lecture. The schedule will be modified as needed."
  },
  {
    "objectID": "slides/2023-10-27-exam.html#tbd",
    "href": "slides/2023-10-27-exam.html#tbd",
    "title": "Module II Exam",
    "section": "TBD",
    "text": "TBD\nThis is a placeholder for a planned project workshop time. The schedule will be modified as needed."
  },
  {
    "objectID": "slides/2023-10-30-frequency-motivation.html#guest-lecture",
    "href": "slides/2023-10-30-frequency-motivation.html#guest-lecture",
    "title": "Intro to Frequency Analysis",
    "section": "Guest Lecture",
    "text": "Guest Lecture\nThanks to Dr. Phil Bedient for sharing his expertise with us today!"
  },
  {
    "objectID": "slides/2023-11-06-pot-gev.html#reading",
    "href": "slides/2023-11-06-pot-gev.html#reading",
    "title": "Extreme Value Theory and Models",
    "section": "Reading",
    "text": "Reading\nThis lecture follows Chapter 3 of Coles (2001) fairly closely"
  },
  {
    "objectID": "slides/2023-11-06-pot-gev.html#gumbel-fréchet-and-weibull-distributions",
    "href": "slides/2023-11-06-pot-gev.html#gumbel-fréchet-and-weibull-distributions",
    "title": "Extreme Value Theory and Models",
    "section": "Gumbel, Fréchet, and Weibull distributions",
    "text": "Gumbel, Fréchet, and Weibull distributions\n\n\nCode\ndists = [GeneralizedExtremeValue(0, 2.5, ξ) for ξ in [-0.5, 0, 0.5]]\nnames = [\"Weibull\", \"Gumbel\", \"Fréchet\"]\ncolors = [:blue, :red, :green]\n\np = plot()\nfor (dist, name, c) in zip(dists, names, colors)\n    plot!(p, dist; label=name, color=c, xlims=(-10, 25))\n    if dist.ξ &gt; 0\n        lb = dist.μ − dist.σ / dist.ξ\n        scatter!(p, [lb], [0]; label=nothing, color=c)\n    elseif dist.ξ &lt; 0\n        ub = dist.μ − dist.σ / dist.ξ\n        scatter!(p, [ub], [0]; label=nothing, color=c)\n    end\nend\np"
  },
  {
    "objectID": "slides/2023-11-06-pot-gev.html#generalized-extreme-value-distribution",
    "href": "slides/2023-11-06-pot-gev.html#generalized-extreme-value-distribution",
    "title": "Extreme Value Theory and Models",
    "section": "Generalized Extreme Value distribution",
    "text": "Generalized Extreme Value distribution\n\nInstead of heuristically picking which of the 3 distributions to use, we can use the Generalized Extreme Value distribution (GEV)\nAccount for uncertainty in choice of extreme value distribution"
  },
  {
    "objectID": "slides/2023-11-06-pot-gev.html#quantiles",
    "href": "slides/2023-11-06-pot-gev.html#quantiles",
    "title": "Extreme Value Theory and Models",
    "section": "Quantiles",
    "text": "Quantiles\nAlso known as return levels. Following Coles (2001) notation, the level \\(z_p\\) is exceeded with probability \\(p\\)\n\\[\nz_p = \\begin{cases}\n    \\mu - \\frac{\\sigma}{\\xi} \\left[ 1 - y_p^{-\\xi} \\right], & \\quad \\text{for} \\quad \\xi \\neq 0 \\\\\n    \\mu - \\sigma \\log y_p, & \\quad \\text{for} \\quad \\xi = 0 \\\\\n\\end{cases}\n\\] where \\(y_p = -\\log(1-p)\\) for concise notation\n\n\nCode\ny_plot = 0:0.1:7\np = plot(; xlabel=L\"$y_p$\", ylabel=L\"$z_p$\", legend=:topleft)\nfor (dist, name) in zip(dists, names)\n    plot!(p, y_plot, quantile(dist, 1 .- exp.(-y_plot)); label=name)\nend\np"
  },
  {
    "objectID": "slides/2023-11-06-pot-gev.html#extremal-types-theorem",
    "href": "slides/2023-11-06-pot-gev.html#extremal-types-theorem",
    "title": "Extreme Value Theory and Models",
    "section": "Extremal Types Theorem",
    "text": "Extremal Types Theorem\n\nSee Coles (2001) section 3.1.4 for a proof\nJustifies use of GEV for modeling the distribution of maxima of long sequences\nNumerical examples here are based on analytic examples in 3.1.5"
  },
  {
    "objectID": "slides/2023-11-06-pot-gev.html#minima",
    "href": "slides/2023-11-06-pot-gev.html#minima",
    "title": "Extreme Value Theory and Models",
    "section": "Minima",
    "text": "Minima\nStatistical consultants hate this one simple trick:\n\nDefine \\(Y_i = -X_i\\) and use the GEV distribution to model the maxima of \\(Y_i\\)"
  },
  {
    "objectID": "slides/2023-11-06-pot-gev.html#bias-variance-trade-off",
    "href": "slides/2023-11-06-pot-gev.html#bias-variance-trade-off",
    "title": "Extreme Value Theory and Models",
    "section": "Bias-variance trade-off",
    "text": "Bias-variance trade-off\n\n\nBlock size is a choice!\nSmall blocks: the limit model, leading to bias in estimation and extrapolation\nLarge blocks: few block maxima, leading to large estimation variance\nDefault choice in environmental applications is typically 1 year\n\nKey assumption: each block from the same distribution (IID)\nIf you choose 3 months, need to account for differing distirbutions of, eg, temperature in summer and winter"
  },
  {
    "objectID": "slides/2023-11-06-pot-gev.html#overview-of-inference-methods",
    "href": "slides/2023-11-06-pot-gev.html#overview-of-inference-methods",
    "title": "Extreme Value Theory and Models",
    "section": "Overview of inference methods",
    "text": "Overview of inference methods\n\nGraphical techniques\nMoment techniques\nOrder statistics based methods\nLikelihood approaches"
  },
  {
    "objectID": "slides/2023-11-06-pot-gev.html#example-data",
    "href": "slides/2023-11-06-pot-gev.html#example-data",
    "title": "Extreme Value Theory and Models",
    "section": "Example data",
    "text": "Example data\n\ndata = Extremes.dataset(\"portpirie\")\nplot(\n    data[!, :Year], data[!, :SeaLevel]; xlabel=\"Year\", ylabel=\"Sea level (m)\", legend=false\n)"
  },
  {
    "objectID": "slides/2023-11-06-pot-gev.html#maximum-likelihood-estimation-i",
    "href": "slides/2023-11-06-pot-gev.html#maximum-likelihood-estimation-i",
    "title": "Extreme Value Theory and Models",
    "section": "Maximum likelihood estimation I",
    "text": "Maximum likelihood estimation I\nConceptually straightforward: \\[\n\\begin{aligned}\n\\ell(\\mu, \\sigma, \\xi) = -m \\log \\sigma - (1 + 1/\\xi) &\\sum_{i=1}^{m} \\log \\left[ 1 + \\xi \\left( \\frac{z_i - \\mu}{\\sigma} \\right) \\right]\\\\\n&- \\sum_{i=1}^{m} \\left[ 1 + \\xi \\left( \\frac{z_i - \\mu}{\\sigma} \\right) \\right]^{-1/\\xi},\n\\end{aligned}\n\\] provided that \\[\n1 + \\xi \\left( \\frac{z_i - \\mu}{\\sigma} \\right) &gt; 0\n\\] when \\(\\xi=0\\) the equation is a bit different \\[\n\\ell(\\mu, \\sigma) = -m \\log \\sigma - \\sum_{i=1}^{m} \\left( \\frac{z_i - \\mu}{\\sigma} \\right) - \\sum_{i=1}^{m} \\exp \\left\\{ - \\left( \\frac{z_i - \\mu}{\\sigma} \\right) \\right\\}.\n\\]\n\n\n\n\n\n\nTip\n\n\nI don’t need you to know these equations"
  },
  {
    "objectID": "slides/2023-11-06-pot-gev.html#maximum-likelihood-estimation-ii",
    "href": "slides/2023-11-06-pot-gev.html#maximum-likelihood-estimation-ii",
    "title": "Extreme Value Theory and Models",
    "section": "Maximum likelihood estimation II",
    "text": "Maximum likelihood estimation II\nNumerical implementations need to consider\n\nUse the right function around \\(\\xi=0\\)\nReturn a value of \\(-\\inf\\) if bounds constraint is violated\nPossible to derive approximations of uncertainty\n\nCan approximate uncertainty in return levels"
  },
  {
    "objectID": "slides/2023-11-06-pot-gev.html#maximum-likelihood-estimation-iii",
    "href": "slides/2023-11-06-pot-gev.html#maximum-likelihood-estimation-iii",
    "title": "Extreme Value Theory and Models",
    "section": "Maximum likelihood estimation III",
    "text": "Maximum likelihood estimation III\n\ngev_mle = gevfit(data.SeaLevel)\n\nMaximumLikelihoodAbstractExtremeValueModel\nmodel :\n    BlockMaxima{GeneralizedExtremeValue}\n    data :      Vector{Float64}[65]\n    location :  μ ~ 1\n    logscale :  ϕ ~ 1\n    shape :     ξ ~ 1\n\nθ̂  :   [3.874750223091266, -1.6192723640210762, -0.05010719929448139]"
  },
  {
    "objectID": "slides/2023-11-06-pot-gev.html#bayesian-inference",
    "href": "slides/2023-11-06-pot-gev.html#bayesian-inference",
    "title": "Extreme Value Theory and Models",
    "section": "Bayesian inference",
    "text": "Bayesian inference\n\ngev_bayes = gevfitbayes(data.SeaLevel; niter=10_000, warmup=2_000)\n\nBayesianAbstractExtremeValueModel\nmodel :\n    BlockMaxima{GeneralizedExtremeValue}\n    data :      Vector{Float64}[65]\n    location :  μ ~ 1\n    logscale :  ϕ ~ 1\n    shape :     ξ ~ 1\n\nsim :\n    MambaLite.Chains\n    Iterations :        2001:10000\n    Thinning interval : 1\n    Chains :        1\n    Samples per chain : 8000\n    Value :         Array{Float64, 3}[8000,3,1]"
  },
  {
    "objectID": "slides/2023-11-06-pot-gev.html#profile-likelihood",
    "href": "slides/2023-11-06-pot-gev.html#profile-likelihood",
    "title": "Extreme Value Theory and Models",
    "section": "Profile likelihood",
    "text": "Profile likelihood\n\nFor a range of values of the shape parameter \\(\\xi\\):\n\nMaximize the likelihood with respect to \\(\\mu\\) and \\(\\sigma\\)\nCompute the likelihood\n\nPlot the (log) likelihood as a function of \\(\\xi\\)"
  },
  {
    "objectID": "slides/2023-11-06-pot-gev.html#moment-matching",
    "href": "slides/2023-11-06-pot-gev.html#moment-matching",
    "title": "Extreme Value Theory and Models",
    "section": "Moment matching",
    "text": "Moment matching\n\n\nMoment: mean, variance, skewness, kurtosis, etc\nMoment matching: estimate the moments of the samples then match them to the theoretical moments of the distribution\nProbability-weighted moments (Hosking et al., 1985): weight data points by their cumulative probability (i.e., more weight for biggest values)\n\\(L\\) moments (Hosking, 1990): Linear combinations of order statistics, less sensitive to outliers"
  },
  {
    "objectID": "slides/2023-11-06-pot-gev.html#probability-weighted-moments-ii",
    "href": "slides/2023-11-06-pot-gev.html#probability-weighted-moments-ii",
    "title": "Extreme Value Theory and Models",
    "section": "Probability-weighted moments II",
    "text": "Probability-weighted moments II\n\ngev_pwm = gevfitpwm(data.SeaLevel)\n\npwmAbstractExtremeValueModel\nmodel :\n    BlockMaxima{GeneralizedExtremeValue}\n    data :      Vector{Float64}[65]\n    location :  μ ~ 1\n    logscale :  ϕ ~ 1\n    shape :     ξ ~ 1\n\nθ̂  :   [3.8731723562720766, -1.5932320395836068, -0.051477125862911276]"
  },
  {
    "objectID": "slides/2023-11-06-pot-gev.html#uncertainty",
    "href": "slides/2023-11-06-pot-gev.html#uncertainty",
    "title": "Extreme Value Theory and Models",
    "section": "Uncertainty",
    "text": "Uncertainty\n\ncint(gev_mle)\n\n3-element Vector{Vector{Float64}}:\n [3.820004234825991, 3.929496211356541]\n [-1.819669858589598, -1.4188748694525544]\n [-0.24268345866324326, 0.1424690600742805]\n\n\n\ncint(gev_bayes)\n\n3-element Vector{Vector{Float64}}:\n [3.817789629088061, 3.928899159987298]\n [-1.7886588596429014, -1.3888832120165522]\n [-0.2119157154228395, 0.1597075738279036]\n\n\n\ncint(gev_pwm)\n\n3-element Vector{Vector{Float64}}:\n [3.82219678818851, 3.9329570198054546]\n [-1.8238458137877533, -1.4129406745797473]\n [-0.23891541608078742, 0.09151391904454692]"
  },
  {
    "objectID": "slides/2023-11-06-pot-gev.html#return-levels",
    "href": "slides/2023-11-06-pot-gev.html#return-levels",
    "title": "Extreme Value Theory and Models",
    "section": "Return levels",
    "text": "Return levels\n\nfunction weibull_plot_pos(y)\n    N = length(y)\n    ys = sort(y; rev=false) # sorted values of y\n    nxp = xp = [r / (N + 1) for r in 1:N] # exceedance probability\n    xp = 1 .- nxp\n    return xp, ys\nend\n\nfunction plot_rl_extremes(fit, obs)\n    minval = 1 + 1 / (length(obs) + 1)\n    return_period = 10 .^ range(log10(minval), log10(250); length=50)\n    x_ticks = [2, 5, 10, 20, 50, 100, 250]\n\n    p = plot(;\n        xscale=:log10,\n        xticks=(x_ticks, string.(x_ticks)),\n        xlabel=\"Return Period\",\n        ylabel=\"Return Level\",\n        legend=false,\n    )\n\n    rl = zero(return_period)\n    ub = zero(return_period)\n    lb = zero(return_period)\n\n    for (i, rt) in enumerate(return_period)\n        r = returnlevel(fit, rt) # special object\n        rl[i] = mean(r.value)\n        lb[i], ub[i] = cint(r)[1]\n    end\n\n    plot!(p, return_period, rl)\n    plot!(p, return_period, lb; fillrange=ub, fillalpha=0.35, linewidth=0)\n\n    xp, ys = weibull_plot_pos(obs)\n    scatter!(p, 1 ./ xp, ys)\n\n    return p\nend"
  },
  {
    "objectID": "slides/2023-11-06-pot-gev.html#comparison",
    "href": "slides/2023-11-06-pot-gev.html#comparison",
    "title": "Extreme Value Theory and Models",
    "section": "Comparison",
    "text": "Comparison\n\n\nCode\np1 = plot_rl_extremes(gev_mle, data.SeaLevel)\ntitle!(p1, \"MLE\")\n\np2 = plot_rl_extremes(gev_bayes, data.SeaLevel)\ntitle!(p2, \"Bayesian Inference\")\n\np3 = plot_rl_extremes(gev_pwm, data.SeaLevel)\ntitle!(p3, \"Probability-weighted moments\")\n\nplot(p1, p2, p3; link=:all)"
  },
  {
    "objectID": "slides/2023-11-06-pot-gev.html#limitations",
    "href": "slides/2023-11-06-pot-gev.html#limitations",
    "title": "Extreme Value Theory and Models",
    "section": "Limitations",
    "text": "Limitations\n\nNonstationarity\nSpatial dependence and multivariate extremes\nSmall samples\nUQ\nModel checking / validation"
  },
  {
    "objectID": "slides/2023-11-06-pot-gev.html#references",
    "href": "slides/2023-11-06-pot-gev.html#references",
    "title": "Extreme Value Theory and Models",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nColes, S. (2001). An introduction to statistical modeling of extreme values. London ; Springer.\n\n\nHosking, J. R. M. (1990). L-Moments: Analysis and Estimation of Distributions Using Linear Combinations of Order Statistics. Journal of the Royal Statistical Society. Series B (Methodological), 52(1), 105–124. Retrieved from https://www.jstor.org/stable/2345653\n\n\nHosking, J. R. M., Wallis, J. R., & Wood, E. F. (1985). Estimation of the Generalized Extreme-Value Distribution by the Method of Probability-Weighted Moments. Technometrics, 27(3), 251–261. https://doi.org/10.1080/00401706.1985.10488049"
  },
  {
    "objectID": "slides/2023-10-11-pca.html#dimension-reduction",
    "href": "slides/2023-10-11-pca.html#dimension-reduction",
    "title": "Principal Components Analysis (PCA)",
    "section": "Dimension Reduction",
    "text": "Dimension Reduction\n\n\nHigh-dimensional data is hard to visualize and interpret\nRedundant or irrelevant dimensions in analysis\nComputational challenges in high dimensions\nIdentify meaningful patterns in data"
  },
  {
    "objectID": "slides/2023-10-11-pca.html#climate-data",
    "href": "slides/2023-10-11-pca.html#climate-data",
    "title": "Principal Components Analysis (PCA)",
    "section": "Climate Data",
    "text": "Climate Data\n\n\nIndexed by location, time (and sometimes more)\nA common matrix representation:\n\nEach location (grid cell / point) is a column\nTime is a row\nOften very high dimensional\nStrong spatial correlation"
  },
  {
    "objectID": "slides/2023-10-11-pca.html#pca",
    "href": "slides/2023-10-11-pca.html#pca",
    "title": "Principal Components Analysis (PCA)",
    "section": "PCA",
    "text": "PCA\n\n\n\\(n\\) observations, \\(p\\) features: \\(X_1, X_2, \\ldots, X_p\\)\nfind a low-dimensional represnetation that represents as much variation as possible\nthe first principal component is the linear combination of features that maximizes the variance\n\n\\(Z_1 = \\phi_{11}X_1 + \\phi_{21}X_2 + \\ldots + \\phi_{p1}X_p\\)\nnormalized: \\(\\sum_{j=1}^p \\phi_{j1}^2 = 1\\)\n\nloading: \\(\\phi_1 = (\\phi_{11}, \\phi_{21}, \\ldots, \\phi_{p1})^T\\)"
  },
  {
    "objectID": "slides/2023-10-11-pca.html#geometric-interpretation",
    "href": "slides/2023-10-11-pca.html#geometric-interpretation",
    "title": "Principal Components Analysis (PCA)",
    "section": "Geometric interpretation",
    "text": "Geometric interpretation\nThe loading vector \\(\\phi_1\\) defines a direction in feature space along which the data vary the most\n\n\n\nPrincipal components analysis (PCA) scores and vectors for climate, soil, topography, and land cover variables. Sites are colored by estimated baseflow yield, and the percent of variance explained by each axis is indicated in the axis titles.\n\n\n\n\n\nDOI: 10.1016/j.ejrh.2015.04.008"
  },
  {
    "objectID": "slides/2023-10-11-pca.html#pca-as-optimization-i",
    "href": "slides/2023-10-11-pca.html#pca-as-optimization-i",
    "title": "Principal Components Analysis (PCA)",
    "section": "PCA as optimization I",
    "text": "PCA as optimization I\nConsider representing the data \\(X = (X_1, X_2, \\ldots, X_p)\\) as a linear model \\[\nf(Z) = \\mu + \\phi_q Z\n\\] where:\n\n\\(\\mu\\) is a location vector.\n\\(\\phi_q\\) is a \\(p \\times q\\) matrix with \\(q\\) orthogonal unit vectors as columns.\n\\(Z\\) is a \\(q\\)-dimensional vector of loadings (or coefficients)."
  },
  {
    "objectID": "slides/2023-10-11-pca.html#pca-as-optimization-ii",
    "href": "slides/2023-10-11-pca.html#pca-as-optimization-ii",
    "title": "Principal Components Analysis (PCA)",
    "section": "PCA as optimization II",
    "text": "PCA as optimization II\nMinimize the reconstruction error: \\[\n\\min \\sum_{i=1}^n \\| X_i - \\phi_q Z \\|_2^2\n\\] assuming \\(\\mu = 0\\) (centered data – more later)"
  },
  {
    "objectID": "slides/2023-10-11-pca.html#pca-as-svd",
    "href": "slides/2023-10-11-pca.html#pca-as-svd",
    "title": "Principal Components Analysis (PCA)",
    "section": "PCA as SVD",
    "text": "PCA as SVD\nWe can write the solution as a singular value decomposition (SVD) of the empirical covariance matrix. This reference explains things quite straightforwardly.\n\nSince we are using the covariance matrix, we are implicitly assuming that variance is a good way to measure variability\n\n\nWhen might this be a poor assumption?"
  },
  {
    "objectID": "slides/2023-10-11-pca.html#uniqueness",
    "href": "slides/2023-10-11-pca.html#uniqueness",
    "title": "Principal Components Analysis (PCA)",
    "section": "Uniqueness",
    "text": "Uniqueness\nEach principal component loading vector is unique, up to a sign flip."
  },
  {
    "objectID": "slides/2023-10-11-pca.html#interpretation",
    "href": "slides/2023-10-11-pca.html#interpretation",
    "title": "Principal Components Analysis (PCA)",
    "section": "Interpretation",
    "text": "Interpretation\nBecause we often use space-time data in climate science, we can interpret the principal components as spatial patterns and time series:\n\n\n\\(Z\\) are the “EOFs” or “principal components”\n\nDominant spatial patterns\n\n\\(\\phi\\): the loading\n\nTime series for each EOF\nReconstruct the data at time \\(t\\) from the EOFs"
  },
  {
    "objectID": "slides/2023-10-11-pca.html#preprocessing",
    "href": "slides/2023-10-11-pca.html#preprocessing",
    "title": "Principal Components Analysis (PCA)",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nVariables should have mean zero (more soon)\nOptional: standardize variables to have unit variance (more later)"
  },
  {
    "objectID": "slides/2023-10-11-pca.html#climate-anomalies",
    "href": "slides/2023-10-11-pca.html#climate-anomalies",
    "title": "Principal Components Analysis (PCA)",
    "section": "Climate anomalies",
    "text": "Climate anomalies\nIt is common in climate science to deconstruct a time series into a mean and anomalies: \\[\nx(t) = \\overline{x}(t) + x'(t)\n\\] where \\(\\overline{x}(t)\\) is the climatology and \\(x'(t)\\) is the anomaly. Typically, this is defined at each location separately."
  },
  {
    "objectID": "slides/2023-10-11-pca.html#computing-anomalies",
    "href": "slides/2023-10-11-pca.html#computing-anomalies",
    "title": "Principal Components Analysis (PCA)",
    "section": "Computing anomalies",
    "text": "Computing anomalies\nHow to define the climatology? Common approaches include:\n\n\nThe time-mean (over some reference period)\nThe time-mean, computed separately for each month or season (e.g. DJF, MAM, JJA, SON)\nA time-mean with a more complicated seasonal cycle removed (eg, sin and cos terms)"
  },
  {
    "objectID": "slides/2023-10-11-pca.html#how-many-pcs-to-use",
    "href": "slides/2023-10-11-pca.html#how-many-pcs-to-use",
    "title": "Principal Components Analysis (PCA)",
    "section": "How Many PCs to Use?",
    "text": "How Many PCs to Use?\n\n\n\n\nIdeally: use a scree plot (R) to find a natural break\nPractical considerations\nOther heuristics\n\n\n\n\n\n\nGraphPad"
  },
  {
    "objectID": "slides/2023-10-11-pca.html#pca-with-spatial-data",
    "href": "slides/2023-10-11-pca.html#pca-with-spatial-data",
    "title": "Principal Components Analysis (PCA)",
    "section": "PCA with Spatial Data",
    "text": "PCA with Spatial Data\n\n\nCentering: variance is the average squared deviation from the mean. Centered and non-centered data will have identical covariance matrices\nWeights: sometimes we have a reason to give one variable more weight than another\n\n\\(\\sqrt{\\cos(\\phi)}\\), where \\(\\phi\\) is latitude. Assigns a standard deviation proportional to the area, which scales with \\(\\cos \\phi\\).\nSometimes variables are weighted by the inverse of their variances. This is equivalent to standardizing each variable to have unit variance before applying PCA."
  },
  {
    "objectID": "slides/2023-10-11-pca.html#packages-for-pca-in-julia",
    "href": "slides/2023-10-11-pca.html#packages-for-pca-in-julia",
    "title": "Principal Components Analysis (PCA)",
    "section": "Packages for PCA in Julia",
    "text": "Packages for PCA in Julia\n\nEmpiricalOrthogonalFunctions.jl\n\nBased on EOFS in python\n\nMultivariateStats.jl"
  },
  {
    "objectID": "slides/2023-10-11-pca.html#examples",
    "href": "slides/2023-10-11-pca.html#examples",
    "title": "Principal Components Analysis (PCA)",
    "section": "Examples",
    "text": "Examples\n\nThis example uses the Python EOFs library\nHere’s another example of"
  },
  {
    "objectID": "slides/2023-10-11-pca.html#beyond-pca",
    "href": "slides/2023-10-11-pca.html#beyond-pca",
    "title": "Principal Components Analysis (PCA)",
    "section": "Beyond PCA",
    "text": "Beyond PCA\n\nProbabilistic PCA – going through this may help you understand PCA better\nRobust PCA\nSparse PCA\nCanonical Correlation Analysis"
  },
  {
    "objectID": "slides/2023-10-11-pca.html#summary",
    "href": "slides/2023-10-11-pca.html#summary",
    "title": "Principal Components Analysis (PCA)",
    "section": "Summary",
    "text": "Summary\nPCA is a versatile tool for dimensionality reduction, data visualization, and compression. By understanding its underlying principles and practical applications, we can effectively analyze and interpret complex datasets."
  },
  {
    "objectID": "slides/2023-10-11-pca.html#further-reading",
    "href": "slides/2023-10-11-pca.html#further-reading",
    "title": "Principal Components Analysis (PCA)",
    "section": "Further reading",
    "text": "Further reading\n\nSVD Mathematical Overview is a YouTube video by Steven Brunton that provides a conceptual overview of SVD, which is a linear algebra path to PCA. Note that he defines \\(X\\) as a matrix of column vectors, whereas we have defined it as a matrix of row vectors (ie, it’s transposed). You’ll want Video 2.\nMIT Computational Thinking Lecture\nChapter 10.2 of James et al. (2013)\nNCAR Quick Tutorial\nGeoStatsGuy Notebook"
  },
  {
    "objectID": "slides/2023-10-11-pca.html#references",
    "href": "slides/2023-10-11-pca.html#references",
    "title": "Principal Components Analysis (PCA)",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning (Vol. 103). New York, NY: Springer New York."
  },
  {
    "objectID": "slides/2023-09-11-uncertainty.html#recall-bayesian-decision-theory",
    "href": "slides/2023-09-11-uncertainty.html#recall-bayesian-decision-theory",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Recall: Bayesian Decision Theory",
    "text": "Recall: Bayesian Decision Theory\nRecall: \\[\n\\mathbb{E}\\left[L(a, \\theta) \\right] = \\int_\\theta L(a, \\theta) p(\\theta) d\\theta\n\\] Where \\(\\theta\\) is a vector of parameters, \\(a\\) is some action or decision, and \\(L\\) is the loss function.\n\n\n\n\n\n\nNote\n\n\nWe previously called \\(\\theta\\) a “state of the world” and \\(L\\) a “reward function”."
  },
  {
    "objectID": "slides/2023-09-11-uncertainty.html#problem-statement",
    "href": "slides/2023-09-11-uncertainty.html#problem-statement",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Problem statement",
    "text": "Problem statement\nYou have been comissioned by a client to assess their exposure to flooding. Specifically, they want to know the probability distribution of annual flood damages at their property if they do not elevate or floodproof their building.\n\n\n\\(p(h)\\): probability distribution of annual maximum flood heights at their property\n\\(d(h)\\): flood damages as a deterministic function of flood height\n\\(p(d) = \\int_h d(h) p(h) \\, dh\\)\n\n\n\nWith this information, they can compute metrics like the expected annual damage, the 99th percentile annual damage, and the probability of any flood occurring that will help them make a decision."
  },
  {
    "objectID": "slides/2023-09-11-uncertainty.html#data",
    "href": "slides/2023-09-11-uncertainty.html#data",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Data",
    "text": "Data\nWe fold this long code block to save space.\n\n\n3×2 DataFrame\n\n\n\nRow\nyear\nlsl\n\n\n\nInt64\nQuantity…\n\n\n\n\n1\n1928\n5.05343 ft\n\n\n2\n1929\n3.90728 ft\n\n\n3\n1930\n4.60245 ft\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecall the *= syntax: x *= 2 is equivalent ot x = x * 2. We use .*= to work element-wise on the vector.\nThis adds the points to the plot\nnormalize=:pdf normalizes the histogram so that the area under the curve is 1.\nThe \\(y\\) axis of the histogram matches that of the line plot, so we remove any y ticks from the histogram.\nWe can define very flexible layouts for combining multiple plots. See the docs.\nsuptitle adds a title to the entire figure.\n\nThis data comes from the NOAA Tides and Currents database, specifically a gauge at Sewell’s Point, VA, with sea level rise removed."
  },
  {
    "objectID": "slides/2023-09-11-uncertainty.html#flood-depths-model",
    "href": "slides/2023-09-11-uncertainty.html#flood-depths-model",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Flood depths model",
    "text": "Flood depths model\nWe want a probability distribution for flood depths \\(p(h)\\). We can work with the log of the flood depths and treat them as normally distributed: \\[\n\\log h_i \\sim \\mathcal{N}(\\mu, \\sigma^2)\n\\]\n\nWe call this a lognormal distribution: \\[\nh_i \\sim \\text{LN}(\\mu, \\sigma)\n\\]"
  },
  {
    "objectID": "slides/2023-09-11-uncertainty.html#distribution-and-histogram",
    "href": "slides/2023-09-11-uncertainty.html#distribution-and-histogram",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Distribution and histogram",
    "text": "Distribution and histogram\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnormalizes the histogram so that the area under the curve is 1.\nThe fit_mle function requires a vector of numbers, without units. We use ustrip to convert the units to a number, using feet (u\"ft\") as the reference unit.\nThis accesses the :lsl_ft column we created.\n\n\n\n\n\n\n\n\nNote\n\n\nWe’ll spend a whole module on extreme value distributions. They should do a better job of modeling annual maxima but require some subtelty."
  },
  {
    "objectID": "slides/2023-09-11-uncertainty.html#return-periods-and-levels",
    "href": "slides/2023-09-11-uncertainty.html#return-periods-and-levels",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Return periods and levels",
    "text": "Return periods and levels\nFlood probabilities are often plotted as return periods. This is just a visualization the CDF on a log (or log-log) scale.\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a trick to get the ticks on the axes right when we plot on a log scale.\n\n\n\n\n\n\n\nNote\n\n\nWork through this code and make sure you understand it"
  },
  {
    "objectID": "slides/2023-09-11-uncertainty.html#plot-position",
    "href": "slides/2023-09-11-uncertainty.html#plot-position",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Plot Position",
    "text": "Plot Position\nIt is common to add the data points as dots on the return period curve. This begs the question: what return period is assigned to each point? This is a subjective choice, but a common one is the Weibull plotting position:\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe are again using the vector that we converted to a scalar, in feet.\n\n\n\n\n\n\n\n🤔\n\n\nHow well does this fit the data?"
  },
  {
    "objectID": "slides/2023-09-11-uncertainty.html#depth-damage-model",
    "href": "slides/2023-09-11-uncertainty.html#depth-damage-model",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Depth-damage model",
    "text": "Depth-damage model\nA bounded logistic function provides a plausible depth-damage model for now: \\[\nd(h) = \\mathbb{I}\\left[x &gt; 0 \\right] \\frac{L}{1 + \\exp(-k(x - x_0))}\n\\] where \\(d\\) is damage as a percent of total value, \\(h\\) is water depth, \\(\\mathbb{I}\\) is the indicator function, \\(L\\) is the maximum loss, \\(k\\) is the slope, and \\(x_0\\) is the inflection point. We fix \\(L=1\\) (known upper bound: 100% damage) so"
  },
  {
    "objectID": "slides/2023-09-11-uncertainty.html#plotting-fit",
    "href": "slides/2023-09-11-uncertainty.html#plotting-fit",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Plotting fit",
    "text": "Plotting fit\nWe will use \\(x_0 = 4\\) and \\(k = 0.75\\).\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is another syntax for defining a function. It is equivalent to f(x) = blogistic(x, x0, k). It’s similar to lambda functions in python"
  },
  {
    "objectID": "slides/2023-09-11-uncertainty.html#analytic-approach",
    "href": "slides/2023-09-11-uncertainty.html#analytic-approach",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Analytic approach",
    "text": "Analytic approach\nPlugging in our bounded logistic model for \\(d(h)\\) and our lognormal model for \\(h\\): \\[\n\\begin{align}\np(d) &= \\int_h d(h) p(h) \\, dh \\\\\n&= \\int_{-\\infty}^\\infty \\mathbb{I}[h &gt; 0] \\text{logistic}(h) \\mathcal{N}(h | \\mu, \\sigma^2) \\, dh\\\\\n&= \\int_0^\\infty \\text{logistic}(h) \\mathcal{N}(\\mu, \\sigma^2) \\, dh\\\\\n&= \\int_0^\\infty \\frac{1}{1 + \\exp(-k * (x - x0))} \\frac {1}{\\sigma {\\sqrt {2\\pi }}} \\exp \\left\\{-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2} \\right\\} \\, dh\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/2023-09-11-uncertainty.html#limitations-analytic-appraoch",
    "href": "slides/2023-09-11-uncertainty.html#limitations-analytic-appraoch",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Limitations: analytic appraoch",
    "text": "Limitations: analytic appraoch\nWe might be able to solve this analytically (Wolfram Alpha can’t…). But…\n\n\nNumerous simplifying assumptions and approximations.\nWhat if we want to use a different distribution?\nA different damage model?"
  },
  {
    "objectID": "slides/2023-09-11-uncertainty.html#what-is-monte-carlo",
    "href": "slides/2023-09-11-uncertainty.html#what-is-monte-carlo",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "What is Monte Carlo?",
    "text": "What is Monte Carlo?\nMonte Carlo methods are a set of computational techniques for:\n\nGenerating samples from a target distribution\napproximating the expectations of some random quantities under this distribution."
  },
  {
    "objectID": "slides/2023-09-11-uncertainty.html#monte-carlo-theory",
    "href": "slides/2023-09-11-uncertainty.html#monte-carlo-theory",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Monte Carlo: Theory",
    "text": "Monte Carlo: Theory\nWe want to approximate the quantity \\[\n\\int_0^\\infty \\text{logistic}(h) \\mathcal{N}(\\mu, \\sigma^2) \\, dh\n\\]::::: {.incremental} :::: {.columns} ::: {.column width=“50%”} A deterministic strategy:\n\nsample \\(h^s = 0, \\Delta h, 2\\Delta h, \\ldots, (S-1)\\Delta h\\)\ncompute \\(\\text{logistic}(h^s) \\mathcal{N}(h^s | \\mu, \\sigma^2)\\) at each point and sum\ndrawbacks: we have to go to \\(\\infty\\) and select \\(\\Delta h\\).\n\n::: ::: {.column width=“50%”} A sampling strategy\n\nsample \\(h^1, h^2, \\ldots, h^S \\sim p(h)\\) – which we can do because we have a model for \\(p(h)\\)\nfor each value: compute \\(\\mathbb{I}(h^s &gt; 0) \\text{logistic}(h^s)\\) and take the average\nthis converges to the correct expectation!\n\n::: :::: :::::"
  },
  {
    "objectID": "slides/2023-09-11-uncertainty.html#more-formally",
    "href": "slides/2023-09-11-uncertainty.html#more-formally",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "More formally",
    "text": "More formally\nIf \\(\\theta^s \\sim p(\\theta)\\), then \\[\n\\mathbb{E}\\left[ f(\\theta) \\right] = \\int_\\theta f(\\theta) p(\\theta) d\\theta \\approx \\frac{1}{S} \\sum_{s=1}^S f(\\theta^s)\n\\]\n\n\n\n\n\n\n\nReferences\n\n\nSee chapter 10 of Gelman et al. (2014) for more details or section 5 of Betancourt (2018) for a more precise mathematical treatment."
  },
  {
    "objectID": "slides/2023-09-11-uncertainty.html#monte-carlo-implementation",
    "href": "slides/2023-09-11-uncertainty.html#monte-carlo-implementation",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Monte Carlo: Implementation",
    "text": "Monte Carlo: Implementation\nA deceptively simple idea:"
  },
  {
    "objectID": "slides/2023-09-11-uncertainty.html#monte-carlo-expectations",
    "href": "slides/2023-09-11-uncertainty.html#monte-carlo-expectations",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Monte Carlo Expectations",
    "text": "Monte Carlo Expectations\nGiven these samples, we can compute expectations of any function of the samples. For example, we can compute the mean damage, the 99th percentile of damage, and the probability of any damage occurring\n\n\n3-element Vector{Float64}:\n 0.00235\n 0.07008\n 0.03486"
  },
  {
    "objectID": "slides/2023-09-11-uncertainty.html#overview",
    "href": "slides/2023-09-11-uncertainty.html#overview",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Overview",
    "text": "Overview\nWe have been working with a single probability distribution for flood depths, which we computed by maximum likelihood.\n\nThese values are not precise. What happens if we consider the lognormal distribution with slightly different, but still plausible, parameters?\n\n\nWhat about the depth-damage parameters \\(x_0\\) and \\(k\\)?"
  },
  {
    "objectID": "slides/2023-09-11-uncertainty.html#parameter-uncertainty-flood-distribution",
    "href": "slides/2023-09-11-uncertainty.html#parameter-uncertainty-flood-distribution",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Parameter uncertainty: flood distribution",
    "text": "Parameter uncertainty: flood distribution"
  },
  {
    "objectID": "slides/2023-09-11-uncertainty.html#flood-distribution-damages",
    "href": "slides/2023-09-11-uncertainty.html#flood-distribution-damages",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Flood distribution ➡️ damages",
    "text": "Flood distribution ➡️ damages\n\n\n3×3 DataFrame\n\n\n\nRow\nparams\nMLE\nalt_dist\n\n\n\nString\nFloat64\nFloat64\n\n\n\n\n1\nAverage Annual Loss\n0.00235\n0.00975\n\n\n2\nQ99 Annual Loss\n0.07008\n0.16531\n\n\n3\nProbability of Loss\n0.03486\n0.10076"
  },
  {
    "objectID": "slides/2023-09-11-uncertainty.html#parameter-uncertainty-depth-damage-curve",
    "href": "slides/2023-09-11-uncertainty.html#parameter-uncertainty-depth-damage-curve",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Parameter uncertainty: depth-damage curve",
    "text": "Parameter uncertainty: depth-damage curve"
  },
  {
    "objectID": "slides/2023-09-11-uncertainty.html#depth-damage-curve-damages",
    "href": "slides/2023-09-11-uncertainty.html#depth-damage-curve-damages",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Depth-damage curve ➡️ damages",
    "text": "Depth-damage curve ➡️ damages\n\n\n3×5 DataFrame\n\n\n\nRow\nparams\nMLE\nalt_dist\nalt_curve\nalt_both\n\n\n\nString\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nAverage Annual Loss\n0.00235\n0.00975\n0.00407\n0.01529\n\n\n2\nQ99 Annual Loss\n0.07008\n0.16531\n0.12715\n0.24093\n\n\n3\nProbability of Loss\n0.03486\n0.10076\n0.03308\n0.09746"
  },
  {
    "objectID": "slides/2023-09-11-uncertainty.html#reflection",
    "href": "slides/2023-09-11-uncertainty.html#reflection",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Reflection",
    "text": "Reflection\n\n\nCommon problem: \\(\\mathbb{E} \\left[ f(x) \\right]\\) where \\(x \\sim p(x)\\)\nSolution: \\(\\mathbb{E} \\left[ f(x) \\right] = \\int_x p(x) f(x) \\, dx\\)\nMonte Carlo approach:\n\nSample \\(x^1, x^2, \\ldots, x^S \\sim p(x)\\)\nCompute \\(\\frac{1}{S} \\sum_{s=1}^S f(x^s)\\)\n\nUncertainties in our model parameters propagate to uncertainties in the things we care about."
  },
  {
    "objectID": "slides/2023-09-11-uncertainty.html#up-next",
    "href": "slides/2023-09-11-uncertainty.html#up-next",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "Up next",
    "text": "Up next\n\nTomrorow 9/12 at 10AM: Office hours (Ryon 215 or Zoom?)\nWednesday: intro to Bayesian inference\nFriday: Bayes lab"
  },
  {
    "objectID": "slides/2023-09-11-uncertainty.html#references-1",
    "href": "slides/2023-09-11-uncertainty.html#references-1",
    "title": "Parametric uncertainty and Monte Carlo",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nBetancourt, M. (2018, October). Probability Theory (For Scientists and Engineers). Retrieved from https://betanalpha.github.io/assets/case_studies/probability_theory.html#7_conclusion\n\n\nGelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (2014). Bayesian Data Analysis (3rd ed.). Chapman & Hall/CRC Boca Raton, FL, USA."
  },
  {
    "objectID": "slides/2023-08-23-climate.html#sizing-a-stormwater-pipe",
    "href": "slides/2023-08-23-climate.html#sizing-a-stormwater-pipe",
    "title": "What drives uncertain climate hazard?",
    "section": "Sizing a stormwater pipe",
    "text": "Sizing a stormwater pipe\n\n\n\n\nRainfall-runoff model\n\ne.g., peak flow from rational method: \\(Q = CiA\\)\n\\(i\\) is rainfall intensity, \\(A\\) is area, and \\(C\\) is runoff coefficient\n\nDesign rainfall based on return period \\(T\\)\n\n\\(p(i &gt; i^*) = 1/T\\)\n\nSize your culvert to handle \\(Q^* = Ci^*A\\)\nRequires knowing \\(p(i)\\)!\n\n\n\n\n\n\nDrainage installation"
  },
  {
    "objectID": "slides/2023-08-23-climate.html#floodplain-mapping-in-a-riverine-system",
    "href": "slides/2023-08-23-climate.html#floodplain-mapping-in-a-riverine-system",
    "title": "What drives uncertain climate hazard?",
    "section": "Floodplain mapping in a riverine system",
    "text": "Floodplain mapping in a riverine system\n\n\n\nThis is a moderately simplified workflow\n\n\n\nAnalyze historical streamflow data at a gauge\nTake the 99th percentile (100 year return level) of annual maximum streamflows\nUse a hydraulic model to model where the water goes\n\n\n\n\n\n\nFloodplain in Selinsgrove, PA"
  },
  {
    "objectID": "slides/2023-08-23-climate.html#reservoir-sizing-simplest-version",
    "href": "slides/2023-08-23-climate.html#reservoir-sizing-simplest-version",
    "title": "What drives uncertain climate hazard?",
    "section": "Reservoir sizing (simplest version)",
    "text": "Reservoir sizing (simplest version)\n\n\nConsider \\(N\\) years of inflows (and releases, evaporation, etc)\nCount number of times reservoir is empty (“failure”)\nRepeat experiment many different times with different inflows\nIf you are sampling this from \\(p(\\text{inflow})\\), you can estimate the reliability\n\nMonte Carlo method\nWhy not just use observed inflows?"
  },
  {
    "objectID": "slides/2023-08-23-climate.html#index-insurance-pricing",
    "href": "slides/2023-08-23-climate.html#index-insurance-pricing",
    "title": "What drives uncertain climate hazard?",
    "section": "Index insurance pricing",
    "text": "Index insurance pricing\n\n\nIndex insurance: if some index \\(I\\) is above a threshold \\(I^*\\), pay out \\(X\\)\n\nTotal rainfall in a season, area flooded, etc\n\nLet \\(p^* = p(I &gt; I^*)\\) is the probability of a payout\nNaive pricing: \\(R = p^* X\\)\nRisk premium: \\(R = X \\left( \\mathbb{E}[p^*] + \\lambda \\mathbb{V}^{1/2}[p^*] \\right)\\)"
  },
  {
    "objectID": "slides/2023-08-23-climate.html#other-examples",
    "href": "slides/2023-08-23-climate.html#other-examples",
    "title": "What drives uncertain climate hazard?",
    "section": "Other examples",
    "text": "Other examples\n\n\nSeasonal electricity resource adequacy (Doss-Gollin et al., 2021)\nLevee design (Garner & Keller, 2018)\nWater supply planning (Fletcher et al., 2019)\nMultihazard design (Bruneau et al., 2017)\netc…"
  },
  {
    "objectID": "slides/2023-08-23-climate.html#important-note",
    "href": "slides/2023-08-23-climate.html#important-note",
    "title": "What drives uncertain climate hazard?",
    "section": "Important note",
    "text": "Important note\nAll of these workflows are slightly simplified, but communicate the main idea. For each of these motivating problems, we need to know the probability distribution of some hazard – \\(p(\\bf{s})\\) to use our notation from last class"
  },
  {
    "objectID": "slides/2023-08-23-climate.html#storms-on-the-north-sea",
    "href": "slides/2023-08-23-climate.html#storms-on-the-north-sea",
    "title": "What drives uncertain climate hazard?",
    "section": "Storms on the North Sea",
    "text": "Storms on the North Sea\n\n\n\n\n\nWaves\n\n\n\n\n\n\nSynoptic Forecast"
  },
  {
    "objectID": "slides/2023-08-23-climate.html#storm-surge-in-houston",
    "href": "slides/2023-08-23-climate.html#storm-surge-in-houston",
    "title": "What drives uncertain climate hazard?",
    "section": "Storm surge in Houston",
    "text": "Storm surge in Houston\n\n\n\n\n\nIke flood depths\n\n\n\n\n\n\nIke path"
  },
  {
    "objectID": "slides/2023-08-23-climate.html#problem-statement",
    "href": "slides/2023-08-23-climate.html#problem-statement",
    "title": "What drives uncertain climate hazard?",
    "section": "Problem statement:",
    "text": "Problem statement:\n\nYou are designing a storm surge barrier on Galveston Bay. What is the probability distribution of storm surge at your location?\n\nThis knowledge will help you trade off the cost of the barrier against the residual risk of flooding."
  },
  {
    "objectID": "slides/2023-08-23-climate.html#what-do-we-need-to-know",
    "href": "slides/2023-08-23-climate.html#what-do-we-need-to-know",
    "title": "What drives uncertain climate hazard?",
    "section": "What do we need to know?",
    "text": "What do we need to know?\n\nTake a moment, write, and then share"
  },
  {
    "objectID": "slides/2023-08-23-climate.html#historical-data",
    "href": "slides/2023-08-23-climate.html#historical-data",
    "title": "What drives uncertain climate hazard?",
    "section": "Historical data",
    "text": "Historical data\n\n\n\n\n\nMaximum water levels at Galveston\n\n\n\n\n\n\nReturn levels"
  },
  {
    "objectID": "slides/2023-08-23-climate.html#tropical-cyclone-tracks-and-characteristics",
    "href": "slides/2023-08-23-climate.html#tropical-cyclone-tracks-and-characteristics",
    "title": "What drives uncertain climate hazard?",
    "section": "Tropical cyclone tracks and characteristics",
    "text": "Tropical cyclone tracks and characteristics\nCan we use models to create a longer “synthetic” record?\n\nBloemendaal et al. (2020)"
  },
  {
    "objectID": "slides/2023-08-23-climate.html#wind-and-rainfall-fields",
    "href": "slides/2023-08-23-climate.html#wind-and-rainfall-fields",
    "title": "What drives uncertain climate hazard?",
    "section": "Wind and rainfall fields",
    "text": "Wind and rainfall fields\nIf we’re going to generate synthetic storms, we need to model the wind and rainfall fields (and other boundary conditions) in order to model the storm surge (using Adcirc, GeoClaw, SFINCS, etc)\n\nKleiber et al. (2023)"
  },
  {
    "objectID": "slides/2023-08-23-climate.html#sea-level",
    "href": "slides/2023-08-23-climate.html#sea-level",
    "title": "What drives uncertain climate hazard?",
    "section": "Sea level",
    "text": "Sea level\n\n\n\n\n\nGalveston Relative Sea Level and Projections\n\n\n\nWhat separates the scenarios? To first order:\n\nHow much \\(CO_2\\) we emit\nHow much the climate system warms in response to \\(CO_2\\) (climate sensitivity)\nIce sheet response to temperatures"
  },
  {
    "objectID": "slides/2023-08-23-climate.html#limitations",
    "href": "slides/2023-08-23-climate.html#limitations",
    "title": "What drives uncertain climate hazard?",
    "section": "Limitations",
    "text": "Limitations\nSobel et al. (2023):\n\nModels are incorrectly simulating the equatorial Pacific response to greenhouse gas warming. This implies that projections of regional tropical cyclone activity may be incorrect as well"
  },
  {
    "objectID": "slides/2023-08-23-climate.html#lessons-learned",
    "href": "slides/2023-08-23-climate.html#lessons-learned",
    "title": "What drives uncertain climate hazard?",
    "section": "Lessons learned?",
    "text": "Lessons learned?\n\n\nHistorical data\n\nMeasures what we care about\nSampling uncertainty\nDoesn’t account for future conditions\n\nModel simulations\n\nCan account for future conditions\nMay be biased or inaccurate\nModel structure uncertainty"
  },
  {
    "objectID": "slides/2023-08-23-climate.html#some-terms-to-know",
    "href": "slides/2023-08-23-climate.html#some-terms-to-know",
    "title": "What drives uncertain climate hazard?",
    "section": "Some terms to know 😉",
    "text": "Some terms to know 😉\n\nReturn period\nReturn level\nMonte Carlo\nSynthetic record\nClimate sensitivity\n\nYou should also be able to reason about the merits and limitations of different methodologies for estimating the probability distribution of a hazard (more practice incoming!)"
  },
  {
    "objectID": "slides/2023-08-23-climate.html#questions",
    "href": "slides/2023-08-23-climate.html#questions",
    "title": "What drives uncertain climate hazard?",
    "section": "Questions?",
    "text": "Questions?\nFriday:\n\nBring your laptop, if you have one.\nCreate an account at https://github.com/\n\nI will be absent on Friday (visiting Harris County Flood Control District). Yuchen will lead lab 01."
  },
  {
    "objectID": "slides/2023-08-23-climate.html#references",
    "href": "slides/2023-08-23-climate.html#references",
    "title": "What drives uncertain climate hazard?",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nBloemendaal, N., Haigh, I. D., de Moel, H., Muis, S., Haarsma, R. J., & Aerts, J. C. J. H. (2020). Generation of a global synthetic tropical cyclone hazard dataset using STORM. Scientific Data, 7(1, 1), 40. https://doi.org/10.1038/s41597-020-0381-2\n\n\nBruneau, M., Barbato, M., Padgett, J. E., Zaghi, A. E., Mitrani-Reiser, J., & Li, Y. (2017). State of the art of multihazard design. Journal of Structural Engineering, 143(10), 03117002. https://doi.org/10.1061/(ASCE)ST.1943-541X.0001893\n\n\nDoss-Gollin, J., Farnham, D. J., Lall, U., & Modi, V. (2021). How unprecedented was the February 2021 Texas cold snap? Environmental Research Letters. https://doi.org/10.1088/1748-9326/ac0278\n\n\nFletcher, S., Lickley, M., & Strzepek, K. (2019). Learning about climate change uncertainty enables flexible water infrastructure planning. Nature Communications, 10(1), 1782. https://doi.org/10.1038/s41467-019-09677-x\n\n\nGarner, G. G., & Keller, K. (2018). Using direct policy search to identify robust strategies in adapting to uncertain sea-level rise and storm surge. Environmental Modelling & Software, 107, 96–104. https://doi.org/10.1016/j.envsoft.2018.05.006\n\n\nKleiber, W., Sain, S., Madaus, L., & Harr, P. (2023). Stochastic tropical cyclone precipitation field generation. Environmetrics, 34(1), e2766. https://doi.org/10.1002/env.2766\n\n\nSobel, A. H., Lee, C.-Y., Bowen, S. G., Camargo, S. J., Cane, M. A., Clement, A., et al. (2023). Near-term tropical cyclone risk and coupled Earth system model biases. Proceedings of the National Academy of Sciences, 120(33), e2209631120. https://doi.org/10.1073/pnas.2209631120"
  },
  {
    "objectID": "slides/2023-09-06-likelihood.html#motivation",
    "href": "slides/2023-09-06-likelihood.html#motivation",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Motivation",
    "text": "Motivation\nWe have some parametric statistical model with unknown parameters. We want to evaluate how consistent the data are with different values of the parameters, and to find the values of the parameters that are most consistent with the data."
  },
  {
    "objectID": "slides/2023-09-06-likelihood.html#today",
    "href": "slides/2023-09-06-likelihood.html#today",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Today",
    "text": "Today\nToday’s lecture contains a few key ideas and a lot of slides.\n\nLots of examples\nWe will move quickly through examples\nReview the examples yourself and ask questions on Canvas\nLab will build on the examples (use them as a reference!)"
  },
  {
    "objectID": "slides/2023-09-06-likelihood.html#definition",
    "href": "slides/2023-09-06-likelihood.html#definition",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Definition",
    "text": "Definition\nThe likelihood is the probability of the data given some parameters: \\[\np(y | \\theta)\n\\]\n\nOften, we want to study how the likelihood changes for different values of \\(\\theta\\), holding \\(y\\) fixed. This is just \\(p(y | \\theta)\\) for a range of \\(\\theta\\).\n\n\n\n\n\n\n\n\nNote\n\n\nYou will sometimes see this referred to as \\(\\mathcal{L}(\\theta)\\), which is a confusing notation…"
  },
  {
    "objectID": "slides/2023-09-06-likelihood.html#likelihood-example",
    "href": "slides/2023-09-06-likelihood.html#likelihood-example",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Likelihood example",
    "text": "Likelihood example\nWe can plot \\(p(y | \\theta)\\) for different values of \\(\\theta\\). To do that, we need a function for \\(p(y | \\theta)\\). We will consider \\(y \\sim \\mathcal{N}(\\mu, \\sigma)\\) so \\(\\theta = \\left\\{ \\mu, \\sigma \\right\\}\\).\n\nThe ::T are called “type annotations” and specify the type of variable that each argument can take. In this case, any Real (float or integer) will work. Read more in the docs.\nThis specifies the likelihood using the pdf function\n\n\nNext we plug in some values for \\(\\mu\\) and plot the likelihood for each. This is essentially plotting the likelihood as a function of \\(\\theta\\) for fixed \\(y\\).\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe vector notation lik. means to apply the function lik to each element of μ_try. [lik(xi, 1, 2) for xi in x] would do the same thing.\nNotice the likelihood function is maximized at \\(\\mu = y\\)."
  },
  {
    "objectID": "slides/2023-09-06-likelihood.html#iid-assumption",
    "href": "slides/2023-09-06-likelihood.html#iid-assumption",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "IID assumption",
    "text": "IID assumption\nIndependent and identically distributed (i.i.d.) assumption: \\[\n\\begin{align}\np(y_1, y_2, \\ldots, y_n) &= p(y_1) p(y_2) \\times \\ldots \\times p(y_n)\\\\\n&= \\prod_{i=1}^n p(y_i)\n\\end{align}\n\\]\n\nUsually we have more than one data point. Say we measure \\(y = y_1, y_2, \\ldots, y_n\\): \\[\np(y | \\theta) = \\prod_{i=1}^n p(y_i | \\theta)\n\\]"
  },
  {
    "objectID": "slides/2023-09-06-likelihood.html#log-trick",
    "href": "slides/2023-09-06-likelihood.html#log-trick",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Log trick",
    "text": "Log trick\nRecall: \\(\\log(AB) = \\log(A) + \\log(B)\\) or, more generally, \\[\n\\log \\left( \\prod_{i=1}^n f_i \\right) = \\sum_{i=1}^n \\log(f_i)\n\\]\n\nThus, we can work with the “log likelihood”: \\[\n\\log p(y | \\theta) =  \\log \\left( \\prod_{i=1}^n p(y_i | \\theta) \\right) = \\sum_{i=1}^n \\log \\left( p(y_i | \\theta) \\right)\n\\]\n\n\nAdding small numbers is more numerically stable than multiplying them"
  },
  {
    "objectID": "slides/2023-09-06-likelihood.html#numerical-example-multiple-data-points",
    "href": "slides/2023-09-06-likelihood.html#numerical-example-multiple-data-points",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Numerical example: multiple data points",
    "text": "Numerical example: multiple data points\nWe can extend our previous example to multiple data points. As before, we need a likelihood function.\n\nVector{&lt;:Real} means a vector of any subtype of Real. Julia uses “multiple dispatch” which means that we can have multiple functions with the same name but that do different things depending on what the type of the arguments is.\nlogpdf function (from Distributions) is the log of the pdf. Here log_liks will be a vector with the same length as y.\nAdd up all the log likelihoods then take the exponent – equvalent to the product of the likelihoods.\n\n\nAs before, we can plot the likelihood as a function of \\(\\mu\\) for fixed \\(y\\) and \\(\\sigma\\).\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this case both \\(\\mu\\) and \\(y\\) are vectors, with different lengths, so using the dot notation lik. won’t work – it doesn’t know which variable to vectorize over."
  },
  {
    "objectID": "slides/2023-09-06-likelihood.html#setup",
    "href": "slides/2023-09-06-likelihood.html#setup",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Setup",
    "text": "Setup\nWe collect \\(y_1, \\ldots, y_n\\) which are the number of tropical cyclones that make landfall in the continental United States in a given year. We decide to model them as a Poisson distribution with unknown rate \\(\\lambda\\): \\[\np(y_i | \\lambda) = \\frac{\\lambda^{y_i} e^{-\\lambda}}{y_i!}\n\\]## Total log likelihood\n\n\nTake the log \\[\n\\log p(y_i | \\lambda) = y_i \\log(\\lambda) - \\lambda - \\log(y_i!)\n\\]\nFor multiple data points \\[\n\\log p(y | \\lambda) = \\sum_{i=1}^n y_i \\log(\\lambda) - n \\lambda - \\sum_{i=1}^n \\log(y_i!)\n\\]\n\n\n\n\nI am deliberately showing different ways to implement the same thing in different examples. Here we use a list comprehension. We could instead use, for example, vector notation: logpdf.(Poisson(λ)). Performance differences are usually negligible, focus on readability."
  },
  {
    "objectID": "slides/2023-09-06-likelihood.html#plot",
    "href": "slides/2023-09-06-likelihood.html#plot",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Plot",
    "text": "Plot"
  },
  {
    "objectID": "slides/2023-09-06-likelihood.html#setup-1",
    "href": "slides/2023-09-06-likelihood.html#setup-1",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Setup",
    "text": "Setup\nLet’s say we don’t know for sure that \\(\\sigma = 1\\). In that case our mathmatical model for \\(p(y | \\mu, \\sigma)\\) is unchanged from the single-variable case.\nLet’s write a function for the log likelihood"
  },
  {
    "objectID": "slides/2023-09-06-likelihood.html#plotting",
    "href": "slides/2023-09-06-likelihood.html#plotting",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Plotting",
    "text": "Plotting\nWith two parameters, we need to plot a surface\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis syntax: z = [f(x, y) for xi in x, yi in y] will produce a matrix with z[i, j] = f(x[i], y[j]).\nDue to a quirk of syntax, we need to transpose the matrix lik_plot to get the correct orientation. Here, lik_plot' is equivalent to transpose(lik_plot).\nst=:heatmap tells Plots to plot a heatmap. We could also try :surface or :contourf.\n\nNotice that there is a very small region for which the likelihood is [relatively] high."
  },
  {
    "objectID": "slides/2023-09-06-likelihood.html#logic",
    "href": "slides/2023-09-06-likelihood.html#logic",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Logic",
    "text": "Logic\nCan we find the parameters \\(\\theta^*\\) that maximize the likelihood \\(p(y | \\theta)\\)?"
  },
  {
    "objectID": "slides/2023-09-06-likelihood.html#log-likelihood",
    "href": "slides/2023-09-06-likelihood.html#log-likelihood",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Log likelihood",
    "text": "Log likelihood\nWe can use the log likelihood \\(\\log p(y | \\theta)\\) instead of the likelihood \\(p(y | \\theta)\\).\nThe log likelihood is monotonic with the likelihood, so \\[\n\\arg \\max \\log p(y | \\theta) = \\arg \\max p(y | \\theta)\n\\]"
  },
  {
    "objectID": "slides/2023-09-06-likelihood.html#analytic-solution",
    "href": "slides/2023-09-06-likelihood.html#analytic-solution",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Analytic solution",
    "text": "Analytic solution\nSolving things analytically takes time up-front, but can be much faster to run because you can avoid the optimization step. Consider the (potentially multivariate) Gaussian example with known covariance matrix \\(\\Sigma\\). We want to maximize the likelihood \\[\n\\sum_{i=1}^n p(y_i | \\mu, \\Sigma)\n\\]\n\nTo maximize, we set its derivative with respect to \\(\\mu\\), which we’ll denote with \\(\\nabla_\\mu\\), to zero: \\[\n\\sum_{i=1}^n \\nabla_\\mu \\log p(y_i | \\mu, \\Sigma) = 0\n\\]\n\n\nSubstituting in the multivariate Gaussian likelihood we get: \\[\n\\begin{aligned}\n0 & =\\sum_{i=1}^n \\nabla_\\mu \\log \\frac{1}{\\sqrt{(2 \\pi)^d|\\Sigma|}} \\exp \\left(-\\frac{1}{2}\\left(x_i-\\mu\\right)^{\\top} \\Sigma^{-1}\\left(x_i-\\mu\\right)\\right) \\\\\n& =\\sum_{i=1}^n \\nabla_\\mu\\left(\\log \\left(\\frac{1}{\\sqrt{(2 \\pi)^d|\\Sigma|}}\\right)\\right)+\\log \\left(\\exp \\left(-\\frac{1}{2}\\left(x_i-\\mu\\right)^{\\top} \\Sigma^{-1}\\left(x_i-\\mu\\right)\\right)\\right) \\\\\n& =\\sum_{i=1}^n \\nabla_\\mu\\left(-\\frac{1}{2}\\left(x_i-\\mu\\right)^{\\top} \\Sigma^{-1}\\left(x_i-\\mu\\right)\\right)\\\\\n&=\\sum_{i=1}^n \\Sigma^{-1}\\left(x_i-\\mu\\right) \\\\\n0 &= \\sum_{i=1}^n (x_i - \\mu) \\\\\n\\mu &= \\frac{1}{n} \\sum_{i=1}^n x_i\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\nNote\n\n\nYou are not expected to remember the above equations and I won’t ask you to do this derivation in a time-constrained exam. You should understand the general procedure:\n\nwrite down log likelihood for all data points\n\nwrite down likelihood for one data point\nwrite down log likelihood for one data point\nsum over all data points\n\ntake \\(\\frac{d}{d\\theta}\\) and set equal to zero to maximize\nsolve for \\(\\theta^*\\)."
  },
  {
    "objectID": "slides/2023-09-06-likelihood.html#numerical-approach-i",
    "href": "slides/2023-09-06-likelihood.html#numerical-approach-i",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Numerical approach I",
    "text": "Numerical approach I\nWe can use the optimize function from the Optim.jl package to find the maximum likelihood estimate. First, we need to define the function to be optimized. optimize will minimize the function, so we need to define the negative log likelihood. We’ll call this the “loss” function.\n\nNote that this function takes in a single argument which is a vector of parameters. We’ll call this vector θ but it doesn’t matter what we call it."
  },
  {
    "objectID": "slides/2023-09-06-likelihood.html#numerical-approach-ii",
    "href": "slides/2023-09-06-likelihood.html#numerical-approach-ii",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Numerical approach II",
    "text": "Numerical approach II\nNow we can run the optimization. Since \\(\\sigma &gt; 0\\) always, we will pass along bounds. We could alternatively do something clever like work with \\(\\log \\sigma\\) instead of \\(\\sigma\\).\n\n\n2-element Vector{Float64}:\n 1.97000000014951\n 0.7590125165166877\n\n\n\nThe lower bound is actually zero, but we just set it to a “pretty small” number.\nThe upper bound is infinity, we can pass in Inf\nWe need to pass in a guess for the parameters. We’ll just use \\(\\mu = \\sigma = 1\\).\nThis will actually run the optimization\nThis will extract the parameters that minimize the loss function.\n\nWe could convert this to a Distributions object as\n\n\nNormal{Float64}(μ=1.97000000014951, σ=0.7590125165166877)"
  },
  {
    "objectID": "slides/2023-09-06-likelihood.html#overview",
    "href": "slides/2023-09-06-likelihood.html#overview",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Overview",
    "text": "Overview\nLet’s consider the generic regression probelem where we have paired observations \\(\\left\\{x_i, y_i\\right\\}_{i=1}^n\\). In general, we can write this regression as \\[\ny_i | \\alpha, \\beta, \\epsilon \\sim \\mathcal{N}(\\alpha + x_i \\beta, \\sigma^2)\n\\] where \\(x_i\\) and \\(\\beta\\) may be vectors.\n\nWe can create some raw data (click to “unfold” the code)"
  },
  {
    "objectID": "slides/2023-09-06-likelihood.html#anlytic-approach",
    "href": "slides/2023-09-06-likelihood.html#anlytic-approach",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Anlytic approach",
    "text": "Anlytic approach\nWe can use the same approach to derive the maximum likelihood estimate for linear regression:\n\nWrite the likelihood for one data point\nWrite the log likelihood for one data point\nWrite the log likelihood for all data points\nTake \\(\\frac{d}{d\\theta}\\) and set equal to zero to maximize\n\nIf you want a walkthrough, see Ryan Adams’s lecture notes starting at about equation 11."
  },
  {
    "objectID": "slides/2023-09-06-likelihood.html#numerical-optimization-i",
    "href": "slides/2023-09-06-likelihood.html#numerical-optimization-i",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Numerical optimization I",
    "text": "Numerical optimization I\nAs before, we need to write down a (log) likelihood function"
  },
  {
    "objectID": "slides/2023-09-06-likelihood.html#numerical-optimization-ii",
    "href": "slides/2023-09-06-likelihood.html#numerical-optimization-ii",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Numerical optimization II",
    "text": "Numerical optimization II\n\n\n3-element Vector{Float64}:\n -3.147\n  2.392\n  1.562\n\n\n\nThis is the loss function, which is the negative log likelihood. The first value of \\(\\theta\\) is the intercept, the second is the slope, and the third is the standard deviation.\nWe need to pass in bounds for the parameters. The standard deviation is always positive, so we set the lower bound to a small number.\nThis will extract the parameters that minimize the loss function and round to show three decimal places."
  },
  {
    "objectID": "slides/2023-09-06-likelihood.html#parallel-least-squares",
    "href": "slides/2023-09-06-likelihood.html#parallel-least-squares",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Parallel: least squares",
    "text": "Parallel: least squares\nIf we work through the math, we can show that the log likelihood for the linear regression problem is \\[\n\\log p(y | X, \\beta, \\sigma) = \\frac{N}{2} \\log (2 \\sigma^2 \\pi)  - \\frac{1}{2 \\sigma^2} \\left( X \\beta - y \\right)^T \\left( X \\beta - y \\right)\n\\]\n\n\n\n\n\n\nLinear algebra notation\n\n\nThere is no intercept here! This is a common notation and assumes that the first column of \\(X\\) is all ones. That is equivalent to writing down an intercept, but lets us use linear algebra notation and keep track of fewer variable names\n\n\n\n\nFrom this, we can show that terms drop out and \\[\n\\beta^\\text{MLE} = \\arg \\min_\\beta \\left( X \\beta - y \\right)^T \\left( X \\beta - y \\right)\n\\] which is exactly the least squares problem (minimize squared error): \\[\n\\min_{\\theta} \\sum_{i=1}^n (y_i - y_i^\\text{pred})^2\n\\]\n\n\n\n\n\n\n\n\nKey point\n\n\n“Least squares can be interpreted as assuming Gaussian noise, and particular choices of likelihood can be interpreted directly as (usually exponentiated) loss functions” –Adams\n\n\n\nIf we then want to estimate \\(\\sigma\\), we can estimate the standard deviation of the residuals."
  },
  {
    "objectID": "slides/2023-09-06-likelihood.html#dont-get-it-twisted",
    "href": "slides/2023-09-06-likelihood.html#dont-get-it-twisted",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Don’t get it twisted",
    "text": "Don’t get it twisted\n\nMany people get this backwards!\n\n\n\nThe likelihood is the probability of the data given the parameters: \\(p(y | \\theta)\\).\nWe often plot the likelihood for many different \\(\\theta\\)\n\n\\(p(y | \\theta)\\) for many different \\(\\theta\\)\n\nDon’t confuse this with the posterior, which is the probability of the parameters given the data: \\(p(\\theta | y)\\)"
  },
  {
    "objectID": "slides/2023-09-06-likelihood.html#logistics",
    "href": "slides/2023-09-06-likelihood.html#logistics",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "Logistics",
    "text": "Logistics\n\nFriday:\n\nLab 03 in class – look for GH Classroom link on Canvas\nLab 02 due\n\nNext week:\n\nBayesian inference"
  },
  {
    "objectID": "slides/2023-09-06-likelihood.html#references",
    "href": "slides/2023-09-06-likelihood.html#references",
    "title": "Likelihood and maximum likelihood estimation",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "slides/2023-08-25-lab.html#overview",
    "href": "slides/2023-08-25-lab.html#overview",
    "title": "Setting up Julia, GitHub, and Quarto",
    "section": "Overview",
    "text": "Overview\nLabs are in-class exercises intended to get practice with coding or analysis workflows.\n\nInstructions available on website\nDownload ahead of time by using link from Canvas\nYou will have your own repository (more in a minute)\nTry to finish in class, but due in 1 week"
  },
  {
    "objectID": "slides/2023-08-25-lab.html#tool-overview",
    "href": "slides/2023-08-25-lab.html#tool-overview",
    "title": "Setting up Julia, GitHub, and Quarto",
    "section": "Tool overview",
    "text": "Tool overview\nIn this class, we will use\n\n\nJulia\nGitHub\nQuarto\nVS Code (suggested)"
  },
  {
    "objectID": "slides/2023-08-25-lab.html#why-julia",
    "href": "slides/2023-08-25-lab.html#why-julia",
    "title": "Setting up Julia, GitHub, and Quarto",
    "section": "Why Julia?",
    "text": "Why Julia?\n\n\n\n\nSyntax\n\nReadable to computers and humans\nClosely parallels math notation\n\nDesigned for numerical and scientific computing\n\n\n\nFast!\n\n“Two language problem”\nAll you need is Julia\n\nOpen source"
  },
  {
    "objectID": "slides/2023-08-25-lab.html#julia-example",
    "href": "slides/2023-08-25-lab.html#julia-example",
    "title": "Setting up Julia, GitHub, and Quarto",
    "section": "Julia example",
    "text": "Julia example\nA (naive) implementation of the Fibonacci sequence:\n\nfunction fib(n)\n    if n &lt; 2\n        return n\n    else\n        return fib(n - 1) + fib(n - 2)\n    end\nend\nfib(10)\n\n55"
  },
  {
    "objectID": "slides/2023-08-25-lab.html#github",
    "href": "slides/2023-08-25-lab.html#github",
    "title": "Setting up Julia, GitHub, and Quarto",
    "section": "GitHub",
    "text": "GitHub\n\n\nYou need a GitHub account\nCode is stored in “repositories”\nclone a repository to your computer\nMake changes and commit them\npush your changes to GitHub\nUsing GitHub classroom, instructors can view your code"
  },
  {
    "objectID": "slides/2023-08-25-lab.html#quarto",
    "href": "slides/2023-08-25-lab.html#quarto",
    "title": "Setting up Julia, GitHub, and Quarto",
    "section": "Quarto",
    "text": "Quarto\nQuarto is a tool that allows you to combine text and code and create many types of output\n\nThis website is made with Quarto\nYou will use Quarto to create reports for labs\n\nEverything in one place\nNo running code, save a figure to Downloads, copy into Word, then update your code and try to remember where to paste it\n\nReproducible"
  },
  {
    "objectID": "slides/2023-08-25-lab.html#vs-code",
    "href": "slides/2023-08-25-lab.html#vs-code",
    "title": "Setting up Julia, GitHub, and Quarto",
    "section": "VS Code",
    "text": "VS Code\n\nVS Code is a text editor\n\nIf you are an advanced user of another text editor, you can use that instead, but I recommend VS Code\n\nVS Code can work as a Julia IDE"
  },
  {
    "objectID": "slides/2023-08-25-lab.html#detailed-instructions",
    "href": "slides/2023-08-25-lab.html#detailed-instructions",
    "title": "Setting up Julia, GitHub, and Quarto",
    "section": "Detailed instructions",
    "text": "Detailed instructions\nSee Setup"
  },
  {
    "objectID": "slides/2023-08-25-lab.html#lab-01-instructions",
    "href": "slides/2023-08-25-lab.html#lab-01-instructions",
    "title": "Setting up Julia, GitHub, and Quarto",
    "section": "Lab 01 Instructions",
    "text": "Lab 01 Instructions\n\nInstall software up following instructions on course website\nclone the repository for lab 01 (use the Github Classroom link from Canvas)\nEdit the solutions.qmd file to add your name and netID\ncommit and push your changes"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "This course covers the use of tools from data science (statistics, machine learning, and programming) to model climate hazards such as floods and droughts. Through hands-on programming assignments based on state-of-the-art published research, students will learn to apply methods to real-world problems with a strong emphasis on probabilistic methods and uncertainty quantification."
  },
  {
    "objectID": "syllabus.html#course-overview",
    "href": "syllabus.html#course-overview",
    "title": "Syllabus",
    "section": "",
    "text": "This course covers the use of tools from data science (statistics, machine learning, and programming) to model climate hazards such as floods and droughts. Through hands-on programming assignments based on state-of-the-art published research, students will learn to apply methods to real-world problems with a strong emphasis on probabilistic methods and uncertainty quantification."
  },
  {
    "objectID": "syllabus.html#course-information",
    "href": "syllabus.html#course-information",
    "title": "Syllabus",
    "section": "Course Information",
    "text": "Course Information\n\n\n\nInstructor\n\n James Doss-Gollin\n jdossgollin@rice.edu\n Ryon 215\n\n\n\n\nTA\n\n Yuchen Lu\n yl238@rice.edu\n Ryon B28\n\n\n\n\nMeetings\n\n MWF\n 11-11:50am\n Keck 107\n\n\n\n\n\nLearning Objectives\nAt the end of this class, students will:\n\nWrite down generative or statistical models for climate hazards;\nUse Bayesian and maximum likelihood methods to estimate the parameters of simple statistical models (“inverse modeling”);\nUse simulation models (“forward modeling”) to assess the logical implications of statistical models;\nUnderstand and apply extreme value theory to estimate the probability of rare climate hazards;\nCritically interpret statistical analyses of environmental data applied in academic journals, government, and industry; and\nUnderstand and communicate subjective modeling choices to technical (e.g., scientist) and non-technical (e.g., policy-maker) audiences.\n\n\n\nPrerequisites & Preparation\n\nLinear algebra (you should be comfortable with matrix notation and basic operations)\nA course in applied statistics (e.g., STAT 419/519)\nSome exposure to Python, Julia, Matlab, R, or another programming language\n\nIn addition, a course covering machine learning, Bayesian statistics, or applied statistics is encouraged but not required. If you are unsure whether your background gives you an adequate preparation for this course, please contact the instructor!\n\n\n\n\n\n\nWhat If My Skills Are Rusty?\n\n\n\nIf your programming, mathematics, or statistics skills are a little rusty, don’t worry! We will review concepts and build skills over the course of the semester.\n\n\n\n\nTopics\n\n\nThe course will build core skills in:\n\nStatistical inference\nMachine learning\nData exploration and visualization\nExtreme value statistics\nModel selection, validation, and comparison\n\n\nWe will apply these methods to a variety of case studies, including three project-based assignments that cover:\n\nPrecipitation frequency analysis\nFlood extent estimation\nStochastic streamflow generation\n\n\n\nFor a full list of topics to be covered, see the course schedule.\n\n\nRequired Materials\nNo textbook is required for this course. All materials will be posted as open source on the course website or as PDFs on Canvas."
  },
  {
    "objectID": "syllabus.html#a-community-of-learning",
    "href": "syllabus.html#a-community-of-learning",
    "title": "Syllabus",
    "section": "A Community of Learning",
    "text": "A Community of Learning\nRice’s core values are responsibility, integrity, community, and excellence. Our goal is to create a learning community aligned with these core values.\n\nCore Expectations\nCourse success involves a dual responsibility on the part of the instructor and the student.\n\n\nAs the instructor, my responsibility is to provide you with a structure and opportunity to learn. To this end, I commit to:\n\nprovide organized and focused lectures, in-class activities, and assignments;\nencourage students to regularly evaluate and provide feedback on the course;\nmanage the classroom atmosphere to promote learning;\nschedule sufficient out-of-class contact opportunities, such as office hours;\nallow adequate time for assignment completion;\nmake lecture materials, class policies, activities, and assignments accessible to students.\n\n\nStudents are responsible for their own learning in the course and should commit to:\n\nattending all lectures;\ndoing all required preparatory work before class;\nactively participating in online and in-class discussions;\nbeginning assignments and other work early; and\nattending office hours as needed.\n\n\n\n\n\n\n\n\n\nWhat If I’m Sick?\n\n\n\nPlease stay home if you’re feeling sick! This is beneficial for both for your own recovery and the health and safety of your classmates. We will also make any necessary arrangements for you to stay on top of the class material and if whatever is going on will negatively impact your grade, for example by causing you to be unable to submit an assignment on time.\n\n\n\n\n\n\n\n\nCanvas Discussions\n\n\n\n\nIf you wait until the day an assignment is due (or even late the previous night) to ask a question on Canvas, there is a strong chance that I will not see your post prior to the deadline.\nBut if you see unanswered questions and you have some insight, please answer! This class will work best when we all work together as a community.\n\n\n\n\n\nDiversity, Equity, and Inclusion\nRice is committed to building and maintaining an equitable and inclusive campus community. Diversity can refer to multiple ways that we identify ourselves, including but not limited to race, color, national origin, language, sex, disability, age, sexual orientation, gender identity, religion, creed, ancestry, belief, veteran status, or genetic information. Each of these diverse identities, along with many others not mentioned here, shape the perspectives our students, faculty, and staff bring to our campus. We, at Rice, will work to promote diversity, equity and inclusion not only because diversity fuels excellence and innovation, but because we want to pursue justice. We acknowledge our imperfections while we also fully commit to the work, inside and outside of our classrooms, of building and sustaining a campus community that increasingly embraces these core values.\nEach of us is responsible for creating a safer, more inclusive environment.y.\n\n\nAccommodation for Students with Disabilities\nIf you have a documented disability or other condition that may affect academic performance you should: 1) make sure this documentation is on file with the Disability Resource Center (Allen Center, Room 111 / adarice@rice.edu / x5841) to determine the accommodations you need; and 2) talk with me to discuss your accommodation needs.\n\n\nAccommodation for Scheduling Conflicts\nIf any of our class meetings conflict with your religious events, student athletics, or other non-negotiable scheduling conflict, please let me know ASAP so that we can make arrangements for you.\n\n\nMask Policies\nMasks are welcome but not required in the classroom. However, I strongly encourage compliance with requests to mask from students, faculty, and staff who are concerned about the risk of infection. Please be respectful of these concerns and requests and do not ask someone making such a request to disclose their underlying medical condition. If for some reason you need your instructor or classmates to wear a mask, please let me know and I will communicate this to the class without disclosing your identity.\nThese policies may change over the course of the semester as the situation evolves.\n\n\nPolicy on Web Posting of Course Materials\nUploading course materials to web sites is not an authorized use of the course material. Both the poster and the user are in violation of the university policy, which is actionable.\n\n\nAcademic Integrity\nThis class is designed to encourage collaboration, and students are encouraged to discuss their work with other students. Engineering as a profession relies upon the honesty and integrity of its practitioners (see e.g. the American Society for Civil Engineers’ Code of Ethics). All work submitted must represent the students’ own work and understanding, whether individually or as a group (depending on the particulars of the assignment). This includes analyses, code, software runs, and reports.\nMore specifically, all students will be held to the standards of the Rice Honor Code, a code that you pledged to honor when you matriculated at this institution. If you are unfamiliar with the details of this code and how it is administered, you should consult the Honor System Handbook at honor.rice.edu/honor-system-handbook/. This handbook outlines the University’s expectations for the integrity of your academic work, the procedures for resolving alleged violations of those expectations, and the rights and responsibilities of students and faculty members throughout the process.\nIf you are ever unclear about academic integrity, please ask! Additionally, always err on the side of providing more information.)\n\n\nAI/ML Resource Policy\nAI/ML tools, like ChatGPT, can be incredibly powerful aids in learning, and can help beginner programmers with syntax and code structure. However, the use of these tools poses risks including the potential for plagiarism and the potential for students to rely on these tools without developing their own understanding.\nAs noted, all work submitted for a grade in this course must reflect your own understanding. You are welcome to use AI/ML tools to help you learn, but you must describe how you used the tool when you turn in your assignment. Moreover, you should not submit code that you do not understand as you be held responsible for explaining any code you submit. For more, see our page on LLMs."
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\n\nLabs: 10%\nOn most Fridays we will use class time for hands-on programming exercises (“labs”) to give you guided practice applying the concepts and methods from class. These labs will be announced on the course website ahead of time so anyone who is able can bring a laptop to class. These labs can be done in groups; if you cannot bring a laptop to class for whatever reason, you will be able to (and are encouraged to) work with other students, though you must turn in your own assignment for grading.\nSome details on lab logistics:\n\nLabs will be designed to be completed in class, but you may occasionally require additional time to complete them.\nLabs will be graded on a 3-point scale: strong (3/3), acceptable (2/3), lacking (1/3), or missing (0/3).\nDetailed solutions will be provided and you will be responsible for reviewing them on your own. Material covered on labs may be covered in tests.\n\n\n\nTests: 40%\nIn-class written exams will be given for each of the four modules of the course, on the dates listed on the schedule. Tests will cover material from lectures and labs, and we will dedicate a class to review before each exam.\nBased on past experience, students enter the class with a wide range of backgrounds and experience. The tests are designed so that students who meet the pre-requisites, but do not have extensive additional experience, can do well. Students with backgrounds that exceed the minimum pre-requisites may find the tests relatively straightforward.\n\n\nProjects: 40%\nModules 2-4 will culminate with a project designed to apply the tools we learn in class to a real-world problem. These projects will be introduced at the start of each module, will motivate the material we cover in class, and give you an opportunity to apply the methods we learn to a problem of your choosing. Projects also offer an opportunity for students with more experience to dig deeper.\nSpecific instructions and rubrics will be provided for each project. You will submit your projects as a Quarto notebook (.qmd file) on Canvas using the provided GitHub classroom link.\n\n\nParticipation: 10%\nParticipating fully in the class allows you to gain more from the class and contribute more to the learning of your classmates. Some ways to participate include:\n\nAttending every class\nAsking questions in class\nAnswering questions on Canvas\nComing to office hours\n\nYou will be asked to evaluate your own participation over the course of the semester, and I will provide feedback on your participation as well\n\n\nLate Work Policy\n\nLate projects will be subjected to a 10% penalty per day, which can accumulate to 100% of the total grade.\nLate labs will not be accepted, because we will discuss solutions in class.\nSometimes things come up in life. Please reach out ahead of time if you have extenuating circumstances (including University-approved absences or illnesses) which would make it difficult for you to submit your work on time. Work which would be late for appropriate reasons will be given extensions and the late penalty will be waived."
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Setting Up Your Computer",
    "section": "",
    "text": "Getting set up for this course requires the following steps. If you are an experienced programmer, you are free to follow your own workflow to set up these tools. You will absolutely need Quarto, GitHub, and Julia. If you are not an experienced programmer, the following steps are not the only way to get these tools set up, but they are a very good way.\nIf you install course tools using steps other than the ones provided on this page, be aware that your instructors may be able to provide you with only limited support."
  },
  {
    "objectID": "setup.html#install-julia",
    "href": "setup.html#install-julia",
    "title": "Setting Up Your Computer",
    "section": "Install Julia",
    "text": "Install Julia\nI recommend installing Julia using the juliaup tool, which will let you easily manage versions in the future and works seamlessly with VS Code. The instructions can be found at the JuliaUp GitHub repository, but we will summarize them here.\n\nInstalling Juliaup on Windows\nIf your computer uses Windows, you can install Juliaup from the Windows Store.\n\n\nInstalling Juliaup on MacOS\nIf you have a Mac, open a terminal (such as the Terminal app) and enter:\ncurl -fsSL https://install.julialang.org | sh\n\n\nInstalling Julia 1.9\nOnce you install Juliaup, install Julia version 1.9 by opening a terminal (in MacOS or Linux) or the command line (in Windows) and entering:\njulia add 1.9\njulia default 1.9\nThis will install Julia 1.9 and make it the default version, which should maximize package compatibility throughout this course. Going forward, if you want to add new versions or change the default, you can follow the Juliaup instructions.\n\n\nMore Resources\nSee this tutorial for more information on using Julia."
  },
  {
    "objectID": "setup.html#install-and-set-up-vs-code",
    "href": "setup.html#install-and-set-up-vs-code",
    "title": "Setting Up Your Computer",
    "section": "Install and Set Up VS Code",
    "text": "Install and Set Up VS Code\nYou can skip this section if you already have an IDE you like; just set it up to work with Julia. Otherwise, VS Code is as close to an officially supported editor for Julia as you can get. We will follow this guide for setting up VS Code with Julia.\n\nInstalling VS Code\nYou can download it here; open the downloaded file to install. Make sure to select the correct version for your operating system. If you have a recent Apple mac, make sure to check whether you have an Intel or Apple chip.\n\n\nInstall the Julia Extension\n\nOpen VS Code.\nSelect View and click Extensions to open the Extension View.\nSearch for julia in the search box. Click the green install button.\nRestart VS Code once the installation is complete. It should automatically find your Julia installation; reach out if not.\n\nThe Julia VS Code extension offers you some nice features. You can start a REPL (an interactive Julia coding environment) by opening the “Command Palette” (View -&gt; Command Palette, or CTRL/CMD+SHIFT+P) and typing “REPL” to bring up “Julia: Start REPL”. You can also create .jl and .qmd files to write Julia code and execute line by line.\n\n\nMore Resources\nSee this tutorial for more information on using Julia."
  },
  {
    "objectID": "setup.html#set-up-github",
    "href": "setup.html#set-up-github",
    "title": "Setting Up Your Computer",
    "section": "Set Up GitHub",
    "text": "Set Up GitHub\n\nCreate GitHub Account\nIf you already have a GitHub account, you can use that for this course and do not need to create a new account. Otherwise, create an account. It doesn’t have to be linked to your Rice email or your NetID.\nFor labs and projects, you should use the GitHub Classroom link posted on Canvas to “accept” the assignment, which will give you your own GitHub repository for that assignment. The first time you click one of these links, you will need to link your place on the course roster with your GitHub account.\n\n\nGitHub Desktop (Optional)\nYou can do almost everything that you will need to do for this course with GitHub directly through VS Code. The GitHub desktop app is also great, or alternatively you may work directly through the terminal if you have prior experience.\n\n\nInstall Git\ngit is a version control software that powers GitHub under the hood (git is the version control software, GitHub is an online platform). Based on past experience with the course, you probably already have git installed. If you’re not sure if it’s installed, see instructions here.\n\n\nMore Resources\nSee GitHub official tutorials for more helpful resources and tutorials."
  },
  {
    "objectID": "setup.html#set-up-quarto",
    "href": "setup.html#set-up-quarto",
    "title": "Setting Up Your Computer",
    "section": "Set up Quarto",
    "text": "Set up Quarto\nQuarto combines the best of Jupyter notebooks and R Markdown to create a document format that is ideal for conducting and communicating data science. We will use Quarto to create and share our work in this course; this website is also built using Quarto.||\n\nInstall Quarto\nFollow the directions at https://quarto.org/docs/get-started/ to install Quarto. Be sure to ensure that you have the right version for your operating system.\n\n\nInstall the Quarto Extension for VS Code\nUnder “Step 2”, click on the VS Code icon.\n\n\nInstall Jupyter\nUnder the hood, Quarto uses Jupyter to run code. You don’t need to know how Jupyter works or worry about it, because it runs under the hood, but we will need to install it.\n\nIf you have Python installed\nIn your Terminal (open VS Code then open the terminal), run\npython3 -m pip install jupyter\nIf this throws an error, copy and paste the error onto Canvas\n\n\nIf you don’t have Python installed\n\nInstall Python\n\nWindows: see Microsoft instructions or Conda documentation\nMac/Linux: see Conda documentation\n\nFollow the instructions above\n\n\n\n\nMore Resources\nSee this tutorial for more information on using Quarto."
  },
  {
    "objectID": "setup.html#verify",
    "href": "setup.html#verify",
    "title": "Setting Up Your Computer",
    "section": "Verify",
    "text": "Verify\n\nLab 01 has a section that will help you verify that your setup is working.\nIf you have any trouble:\n\nOpen VS Code\nIn the Terminal, run quarto check\nTry to troubleshoot on your own; if you can’t, post the results of quarto check to Canvas"
  }
]