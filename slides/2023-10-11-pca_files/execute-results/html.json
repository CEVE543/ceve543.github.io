{
  "hash": "a11c5f291540f1dbe1e34edb87eb8ba2",
  "result": {
    "markdown": "---\ntitle: \"Principal Components Analysis (PCA)\"\nsubtitle: \"Lecture\"\ndate: 2023-10-11\n\n# metadata for the schedule page\nkind: \"Lecture\"\nModule: \"2\"\ncategories:\n    - \"Module 2\"\n    - \"Lectures\"\n\n# do not edit anything below this line\nformat: revealjs\nauthor: \"{{< var instructor.name >}}!\"\ncourse: \"{{< var course.number >}}, {{< var course.title >}}\"\ninstitution: \"{{< var course.institution >}}}\"\ntemplate-partials:\n    - title-slide.html\n---\n\n\n\n# Dimension Reduction\n\n## Motivation\n\n## Approaches\n\n# Linear algebra review\n\n## Rank of a matrix\n\n## Eigenvalues and eigenvectors\n\n# Principal Components Analysis\n\n## Goals\n\n## Algorithm\n\n## PCA as Optimization\n\nPCA can be viewed as an optimization problem where we maximize the variance of the projected data.\n\n## PCA as Projection\n\nPCA projects data onto a lower-dimensional subspace, ensuring that the variance of the projected data is maximized.\n\n# Example: MNIST Numbers Data\n\n## Overview\n\nThe MNIST dataset contains handwritten digits and is commonly used for training image processing systems.\n\n## Implementation\n\n## PCA as Compression\n\n## Implementation\n\n# Example: Climate Data\n\n# Practical Tips\n\n## How Many PCs to Use?\n\n- Use a scree plot to visualize the explained variance by each PC.\n- Typically, choose enough PCs to explain 95-99% of the total variance.\n\n## Checking for Numerical Errors\n\n- Ensure that the eigenvalues are non-negative.\n- Check the condition number of the covariance matrix.\n\n## Packages for PCA in Julia\n\n- MultivariateStats.jl: Provides functions for PCA and other multivariate analyses.\n- StatsBase.jl: Contains utility functions for statistical analyses.\n\n## PCA with Spatial Data in Julia\n\n- Ensure that spatial data is appropriately preprocessed.\n- Consider using specialized packages or tools that handle spatial data structures.\n\n# Wrapup\n\n## Summary\n\nPCA is a versatile tool for dimensionality reduction, data visualization, and compression. By understanding its underlying principles and practical applications, we can effectively analyze and interpret complex datasets.\n\n## Further reading\n\n- MIT Computational Thinking [Lecture](https://computationalthinking.mit.edu/Fall23/data_science/pca/)\n- Chapter 10.2 of @james_statlearn:2013\n- Chapter 14.5 of @Friedman:2001wp\n\n## References\n\n",
    "supporting": [
      "2023-10-11-pca_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {}
  }
}