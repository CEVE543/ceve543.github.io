---
title: "Intro to Machine Learning"
subtitle: "Lecture"
date: 2023-10-06

# metadata for the schedule page
kind: "Lecture"
Module: "2"
categories:
    - "Module 2"
    - "Lectures"

# do not edit anything below this line
format: revealjs
author: "{{< var instructor.name >}}!"
course: "{{< var course.number >}}, {{< var course.title >}}"
institution: "{{< var course.institution >}}}"
template-partials:
    - title-slide.html
---

```{julia}
#| output: false
#| echo: false
using Distributions
using LaTeXStrings
using NearestNeighbors
using Optim
using Plots
using Random
using StatsBase: mean

Plots.default(; margin=4Plots.mm, size=(700, 400), linewidth=2)
```

## Practice Problem {.smaller .scrollable}

We recorded the number of accidents and average vehicle speed in a city over several months:

| Month | Average Speed (km/h) | Number of Accidents |
|-------|----------------------|---------------------|
| Jan   | 45                   | 10                  |
| Feb   | 50                   | 8                   |
| Mar   | 55                   | 12                  |
| Apr   | 60                   | 15                  |

1. Identify a suitable distribution for modeling the number of accidents.
2. Write down the likelihood for the chosen distribution.
3. Suggest a link function to relate the average speed to the number of accidents.
4. Briefly explain why the chosen link function is appropriate.

## Logistics

- Turn in exam revisions now
- Revised [course schedule](../schedule.qmd)

# Motivation

## Example data

We want to make predictions about the value of $y$ given some function $x$.

. . .

The true function will be
$$
f(x) = 2x + x \sin(2 \pi x)
$$
but let's assume we don't know it.

## Viz {.scrollable}

```{julia}
#| code-fold: true
f(x) = 2x + x * sin(2pi * x)
N = 100
Random.seed!(1017)
x = rand(Uniform(0, 2), N)
y = f.(x) .+ rand(Normal(0, 1), N)
p_base = scatter(x, y; label="Obs", xlabel=L"$x$", ylabel=L"$y$")
p1 = plot(p_base)
plot!(p1, f; label=L"$f(x)$")
```

## Linear regression

$$
y | X \sim \mathcal{N}(X^T \beta, \sigma^2)
$$

::: {.callout-tip}
## Notation
We will frequently use this linear algebra notation, which is equivalent to writing $y_i | X_i \sim \mathcal{N}(\sum_{j=1}^J X_{ij} \beta+j, \sigma^2)$.
:::

## Linear regression MLE {.scrollable}

We can calcualte a **point estimate**, ie the maximum likelihood estimate, instead of sampling from the posterior.

```{julia}
#| code-fold: true
p2 = plot(p_base)
log_lik_regression(α, β, σ, x, y) = sum(logpdf.(Normal.(α .+ β .* x, σ), y))
loss_regression(θ) = -log_lik_regression(θ[1], θ[2], θ[3], x, y)
lower = [-Inf, -Inf, 0]
upper = [Inf, Inf, Inf]
guess = ones(3)

θ̂_regression = optimize(loss_regression, lower, upper, guess).minimizer
f̂_regression(x) = θ̂_regression[1] + θ̂_regression[2] * x
x_plot = range(minimum(x), maximum(x); length=250)
plot!(p2, x_plot, f̂_regression.(x_plot); label="Linear Regression")
```

## Critiquing the fit {.scrollable}

One way we can tell that our fit is terrible is by plotting the residuals.
We have assumed that the residuals are IID.
However, we can see that the residuals are correlated with our predictor!

```{julia}
ϵ_regression = y .- f̂_regression.(x)
p3 = scatter(x, ϵ_regression; label="Residuals", xlabel=L"$x$", ylabel=L"$\epsilon$")
```

## Curve fitting {.scrollable}

We don't have to fit a straight line.
Without modifying our generative model, we can write down a parametric function for $f$ and fit it using maximum likelihood (or sample from the posterior):
$$
\mu_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3
$$

```{julia}
#| code-fold: true
p4 = plot(p_base)
function log_lik_poly(β, σ, x, y)
    μ = [sum([βi * xi^(i - 1) for (i, βi) in enumerate(β)]) for xi in x]
    return sum(logpdf.(Normal.(μ, σ), y))
end
order = 7
loss_poly(θ) = -log_lik_poly(θ[1:(order + 1)], θ[order + 2], x, y)

lower = -Inf * ones(order + 2)
lower[end] = 0
upper = Inf * ones(order + 2)
guess = 0.5 * ones(order + 2)
res = optimize(loss_poly, lower, upper, guess)
θ̂_poly = res.minimizer
f̂_poly(x) = θ̂_poly[1] + sum(θ̂_poly[1 + k] * x^k for k in 1:order)
plot!(p4, x_plot, f̂_poly.(x_plot); label="Polynomial Fit K=$order")
```

::: {.fragment style="color: gray;"}
What are some problems with this polynomial?
:::

# Supervised learning

## Overview

We are interested generally in estimating some function $f$ that maps inputs $X$ to outputs $y$.
$$
y = f(X) + \epsilon
$$

## Nonparametric {.scrollable}

$K$ nearest neighbors (KNN): find the $K$ training examples that are closest to a given input and returns the average output.

::: {.callout-important}
## Note
Nonparametric does not mean that there are no parameters.
For example, $K$ is a parameter!
:::

```{julia}
#| code-fold: true

# data structure to make nearest neighbor search fast -- don't worry about this
kdtree = KDTree(transpose(x))

function knn_predict(xi, k)
    # find the k nearest neighbors
    idxs, _ = knn(kdtree, [xi], k, true)
    # return the average of the y values
    return mean(y[idxs])
end

ŷ_3 = knn_predict.(x_plot, 3)
ŷ_10 = knn_predict.(x_plot, 10)

p5 = plot(p_base)
plot!(p5, x_plot, ŷ_3; label="K=3")
plot!(p5, x_plot, ŷ_10; label="K=10")
```

::: {.fragment style="color: gray;"}
What are some problems with this $K$NN model?
:::

## Parametric function approximation {.scrollable}

The linear regression model is a parametric function approximation.

# Loss Functions

## Loss functions

We need to define what we mean by a "best" approximation.

::: {.incremental}
- **What?** Measures difference between predicted and actual values.
- **Why?** Guide optimization towards best model parameters.
- **Types**: Vary by algorithm and task (e.g., regression vs. classification).
- **Impact**: Choice can significantly affect model performance.
- **Key**: Align loss function with task objectives for best results.
:::

## Common Loss Functions

::: {.incremental}
1. **MSE (Mean Squared Error)**:  $L(y, \hat{y}) = (y - \hat{y})^2$. 
Emphasizes larger errors but is sensitive to outliers.
2. **MAE (Mean Absolute Error)**: $L(y, \hat{y}) = |y - \hat{y}|$. 
Less sensitive to outliers and non-differentiable at zero.
3. **Huber Loss**: $L_\delta(y, \hat{y}) = \begin{cases} 
\frac{1}{2}(y - \hat{y})^2 & \text{for } |y - \hat{y}| \leq \delta \\
\delta \( |y - \hat{y}| - \frac{1}{2}\delta^2 \) & \text{otherwise}
\end{cases}$
Combines MSE and MAE. Requires threshold parameter $\delta$.
4. **Quantile Loss**: $L_\tau(y, \hat{y}) = \tau(y - \hat{y})$ if $(y - \hat{y}) > 0$ else $(\tau - 1)(y - \hat{y})$. 
Tailored to specific quantiles ($\tau$). Useful for asymmetric errors.
:::

## Visualization

```{julia}
#| code-fold: true

# Sample residuals
residuals = -2:0.01:2

# Loss functions
mse_loss = residuals .^ 2
mae_loss = abs.(residuals)
huber_loss = [abs(r) <= 1 ? 0.5 * r^2 : abs(r) - 0.5 for r in residuals]
quantile_tau = 0.5  # example quantile value
quantile_loss = [r > 0 ? quantile_tau * r : (1 - quantile_tau) * r for r in residuals]

# Plot
plot(residuals, mse_loss; label="MSE", lw=2)
plot!(residuals, mae_loss; label="MAE", lw=2)
plot!(residuals, huber_loss; label="Huber", lw=2)
plot!(residuals, quantile_loss; label="Quantile (τ=0.5)", lw=2)
xlabel!(L"Residual ($y - \hat{y}$)")
ylabel!("Loss")
title!("Loss Functions Visualization")
```

# Bias-Variance Trade-Off

## Motivation

::: {.incremental}
1. **Model Performance**: Why does our model make errors? Can we reduce them?
2. **Overfitting vs Underfitting**: How do we balance fitting our data well and ensuring our model generalizes to new data?
3. **Model Complexity**: As we add more features or increase model complexity, how does it impact our model's errors?
4. **Optimal Model**: How do we find the sweet spot where our model has the best predictive performance?
5. **Interpretability**: Understanding bias and variance can help in making informed decisions about model selection and complexity.
:::

::: {.fragment .fade-in}
**Key Insight**: Every model error can be decomposed into bias, variance, and irreducible error. Balancing bias and variance is crucial for creating models that perform well on both training and unseen data.
:::

## Bias and variance {.smaller .scrollable}

::: {#fig-bias-variance layout-ncol=2}

![High bias, low variance](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a2/Truen_bad_prec_ok.png/240px-Truen_bad_prec_ok.png)

![High bias, high variance](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fd/Truen_bad_prec_bad.png/240px-Truen_bad_prec_bad.png)

![Low bias, low variance](https://upload.wikimedia.org/wikipedia/commons/thumb/2/2a/En_low_bias_low_variance.png/240px-En_low_bias_low_variance.png)

![Low bias, high variance](https://upload.wikimedia.org/wikipedia/commons/thumb/d/dc/Truen_ok_prec_bad.png/240px-Truen_ok_prec_bad.png)

Bias-variance illustration (Wikipedia)
:::

## Mean Squared Error (MSE) {.smaller .scrollable}

The expected prediction error for any machine learning algorithm can be broken down as:

$$
\text{MSE} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}
$$

::: {.incremental}
1. **Bias**: How much on average are the predicted values different from the actual values? Represents errors due to overly simplistic assumptions in the learning algorithm.
$$
\text{Bias}(\hat{f}(x)) = E[\hat{f}(x) - f(x)]
$$

2. **Variance**: How much does the prediction for a given point vary between different realizations of the model? Represents errors due to the model's sensitivity to small fluctuations in the training set.
$$
\text{Variance}(\hat{f}(x)) = E[\hat{f}(x)^2] - E[\hat{f}(x)]^2
$$

3. **Irreducible Error**: Noise inherent in any real-world data that we cannot remove, no matter how good our model is.
:::

## Bayesian methods and Bias-Variance

Prior information about the parameters of the model can be used to *reduce the variance of the model* while *adding some bias*.

## Ridge Regression

Ridge Regression, modifies linear regression to include a **regularization** term. 
The regularization term discourages overly complex models which can overfit the training data.

$$
L(\beta) = \| Y - X^T \beta \|_2^2 + \lambda \| \beta \|_2^2
$$
where $\lambda$ is the regularization parameter.

## Lasso Regression

Lasso Regression (Least Absolute Shrinkage and Selection Operator) includes an **L1** penalty.

::: {.incremental}
1. Stronger penalty near zero will set some coefficients to almost exactly zero
1. Smaller penalty far from zero
:::

. . .

$$
L(\beta) = \| Y - X^T \beta \|_2^2 + \lambda \| \beta \|_1
$$

# Wrapup

## Key ideas

::: {.incremental}
- Relative to statistical approaches, ML perspectives emphasize fitting complex models over quantifying all uncertainties. There isn't a clear demarcation!
- Measure quality of predictions using loss functions, which we can optimize
- Bias-Variance Trade-Off
- Regularization
:::

## Read more

Chapter 2 of @Friedman:2001wp elaborates on today's topics in more depth

## References

::: {.refs}
:::