---
title: "Maximum likelihood"
subtitle: "Lecture"
date: "September 6, 2023"

# don't adjust the below options, they should be the same for all slides
author: "{{< var instructor.name >}}!"
course: "{{< var course.number >}}, {{< var course.title >}}"
institution: "{{< var course.institution >}}}"
format:
    revealjs:
        slide-number: c/t
        show-slide-number: all
        center-title-slide: true
        width: 1280
        height: 720
        transition: none
        toc: true
        toc-depth: 1
        toc-title: "Overview"
        history: false
        link-external-newwindow: true
        theme: ../sass/slides.scss
        footer: "[{{< var course.number >}}, {{< var course.title >}}]({{< var course.url >}})"
        template-partials:
            - title-slide.html
        menu:
            numbers: true
        html-math-method: mathjax
        include-in-header: mathjax-config.html
        date-format: long
        code-annotations: below

execute:
    freeze: auto
    echo: true
---

# Likelihood

## Likelihood of one data point

In Bayes' rule the data $y$ affect the posterior $p(\theta | y)$ only through $p(y | \theta)$, called the "likelihood"
$$
p(y | \theta)
$$

## Likelihood example {.scrollable .smaller}

::: {.incremental}

- One data point: $y=2$.
- Normal distribution with $\sigma = 1$ and unknown $\mu$
- We can *plot* $p(\theta | y = 2, \sigma = 1)$ as a function of $\theta$

:::

. . .

```{julia}
using Distributions
using Plots
using LaTeXStrings

function lik(μ::Real, σ::Real, y::Real) # <1>
    dist = Normal(μ, σ)
    return pdf(dist, y) # <2>
end

μ = -5:0.02:7
y = 2
σ = 1
μ_lik = lik.(μ, σ, y) # <3>

plot(
    μ,
    μ_lik;
    label=L"$p(\theta | y=2,  \sigma=1)$",
    xlabel=L"$\theta$",
    ylabel="Likelihood",
    linewidth=2
)
```

1. These are called "type annotations" and specify the type of variable that each argument can take. In this case, any `Real` (float or integer) will work.
2. This specifies the likelihood using the `pdf` function
3. The vector notation `lik.` means to apply it to each element of `x`. `[lik(xi, 1, 2) for xi in x]` would do the same thing.

## Likelihood of multiple data points

Independent and identically distributed (i.i.d.) assumption:
$$
\begin{align}
p(y_1, y_2, \ldots, y_n) &= p(y_1) p(y_2) \times \ldots \times p(y_n)\\
 &= \prod_{i=1}^n p(y_i)
\end{align}
$$

. . .

Imagine we measure $y = y_1, y_2, \ldots, y_n$:
$$
p(y | \theta) = \prod_{i=1}^n p(y_i | \theta)
$$

## Log trick

Recall: $\log(AB) = \log(A) + \log(B)$ or, more generally,
$$
\log \left( \prod_{i=1}^n f_i \right) = \sum_{i=1}^n \log(f_i)
$$

. . .

It's often convenient to work with the "log likelihood":
$$
\log p(y | \theta) =  \log \left( \prod_{i=1}^n p(y_i | \theta) \right) = \sum_{i=1}^n \log \left( p(y_i | \theta) \right)
$$
for numerical reasons (multiplying tiny numbers ➡️ underflow)

## Likelihood example: multiple data points {.scrollable .smaller}

```{julia}
function lik(μ::Real, σ::Real, y::Vector{<:Real}) # <1>
    dist = Normal(μ, σ)
    log_liks = Distributions.logpdf.(dist, y) # <2>
    return exp(sum(log_liks)) # <3>
end

μ = -5:0.02:7
y = [2.1, 1.9, 2.2, 2.0, 2.1]
σ = 1
μ_lik = [lik(μi, σ, y) for μi in μ] # <4>
plot(
    μ,
    μ_lik;
    label=L"$p(\theta | y=y,  \sigma=1)$",
    xlabel=L"$\theta$",
    ylabel="Likelihood",
    linewidth=2
)
```

1. `Vector{<:Real}` means a vector of any subtype of `Real`. Julia uses "multiple dispatch" which means that we can have multiple functions with the same name but that do different things depending on what the type of the arguments is.
2. `Distributions.logpdf` is the log of the pdf. Here `log_liks` will be a vector with the same length as `y`.
3. Add up all the log likelihoods then take the exponent -- equvalent to the product of the likelihoods.
4. In this case both $\mu$ and $y$ are vectors, with different lengths, so using the dot notation `lik.` won't work -- it doesn't know which variable to vectorize over.

## Don't get it twisted

::: {.note}
Many people get this backwards!
:::

::: {.incremental}

- The likelihood is the probability of the data given the parameters: $p(y | \theta)$.
- We often plot the likelihood for many different $\theta$
    - $p(y | \theta)$ for many different $\theta$
    - Don't confuse this with the posterior, which is the probability of the parameters given the data: $p(\theta | y)$
- Again: likelihood is probability of *data* given *parameters*

:::

## References
